\documentclass[conference]{IEEEtran}
\pdfpagewidth=8.5in
\pdfpageheight=11in

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{microtype}
\usepackage{balance}
\usepackage{listings}
\usepackage{float}
\usepackage{color,graphicx}
\usepackage[figuresright]{rotating}
\usepackage[tableposition=top]{caption}
\usepackage{listings}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{color}

\usepackage{listings}
\lstset{keywordstyle=\bfseries, flexiblecolumns=true}
\lstloadlanguages{[ANSI]C++,HTML}
\lstdefinestyle{prg} {basicstyle=\small\sffamily, lineskip=-0.2ex, showspaces=false}

\newcommand{\progcpp}[3][tbp]{
 \begin{figure}[#1]
     \lstinputlisting[language=C++,style=prg]{fig/#2.h}
   \caption{#3\label{progcpp:#2}}
 \end{figure}
}

\newcommand{\progxml}[3][tbp]{
 \begin{figure}[#1]
     \lstinputlisting[language=XML,style=prg]{fig/#2.xml}
   \caption{#3\label{progxml:#2}}
 \end{figure}
}

\newcommand{\fig}[4][thb]{
  \begin{figure}[#1] {\centering{\includegraphics[#4]{fig/#2}}\par}
    \caption{#3\label{fig:#2}}
  \end{figure}
}

\begin{document}

\title{AEP - Automatic Exchange of Embedded System Software Parameters.}

\author{
	\IEEEauthorblockN{Rita de Cássia Cazu Soldi and Antônio Augusto Medeiros Fröhlich}
	\IEEEauthorblockA{Federal University of Santa Catarina (UFSC)\\
					  Laboratory for Software and Hardware Integration (LISHA)\\
					  Florianópolis, SC, Brazil\\
					  {rita, guto}@lisha.ufsc.br}
}
\maketitle

\begin{abstract}
The process of debugging  embedded system software is a nontrivial task that consumes a lot of time, once it needs a thorough inspection of the entire source code to make sure that there is no behavior beyond expectations.

Coding and testing embedded systems is even more defiant, since developers need to find out how to optimize the use of the scarce resources since the test itself will compete with the application under test by the scarce system resources. Also, both run in proper platforms, that depends on operating systems, architecture, vendors, debugging tool, etc. This makes embedded systems more susceptible to errors as well as specification failures.

This paper presents AEP, a tool to help developers in the process of debugging embedded systems. The main idea of this tool is emulating various possible system configuration to try to find errors in the application.  An XML file contains all required information to perform automated compilation, emulation and debugging , and there is no need of human interference.

The evaluation of AEP was in terms of memory consumption and time to perform debugging. The obtained results indicate that even with no previous information this tool can produce helpful data for developers to find and fix bugs.

\end{abstract}

\section{Introduction}
An embedded system can be presented as a combination of software and hardware designed to perform a specific task.  Applications involving environmental monitoring and analysis, intelligent cities, and precision farming are only a sample of a set of possible applications.

These systems were designed to monitor and process data related to the physical environment in which they were coupled. Then, the purpose of these systems was extrapolated to interact with the actors of the modification of this environment, humans. Now, embedded systems are widely attached to numerous electronic devices, and their activities are becoming more popular and intrinsic to human’s daily life~\cite{carro2003sistemas}.

When the interaction between embedded systems and humans was not the main focus, the price for a failure in these systems was more focused on the financial loss such as loss of market share, client information, people time, etc.~\cite{tassey2002economic}. Material losses are inconvenient, but endanger human life is an unacceptable risk. With a focus on direct interaction with humans, we must make sure that the behavior of the systems is in accordance with the specification.

Software testing is the process of analyzing a software item to detect the differences between existing and required conditions (that is, bugs) and to evaluate the features of the software item~\cite{standard:ieee1008:1987}. The test area has evolved considerably, but testing is still one of the most time-consuming development process. Mainly because, it requires a thorough inspection of the source code to find out if the software specification is fully satisfied, and this is a non-trivial process~\cite{parnin2011automated}.

Since the test is not part of the software behavior, it should never interfere in the flow of activities of the software under test. In general-purpose systems, it is usually possible to achieve this premise  without much effort, but some special computer systems have some restrictions, such as low memory, low processing power, limited battery time or a deadline to perform a certain activity. In this case, the developer will still need to find strategies to the test in itself does not compete for resources application.

%Most debug tools are partially automated, and demand interaction with the developer to make decisions during testing \cite{campos2012gzoltar, tracingDiagnose}. This type of tool saves some development time, but not as much when compared to total automation tools. The automation of the entire testing process without any human intervention is still a challenge to researchers, although there are some studies that can automate part of the process with data taken directly from the application \cite{Larson:2013:MDAT,JSWjsw0803603616}.

In this paper, we propose the automatic exchange of configuration parameters (AEP) as an automation of one part of the debugging process for embedded systems' application. The AEP contains a shell script responsible for exchanging configuration parameters according to an XML specification file, without any human interaction during debug processes. This proposal also presents an introduction to the problem of setting up a stable environment for testing embedded systems. 

%In summary, we make the following contributions:
%\begin{itemize}

%\item \textbf{Automated debug an embedded system application.} In this case study developers can run a script to automatic find errors and use a report to fix the code or find better parameter values.

%\item \textbf{Environment for debugging embedded applications, fully configurable according to specific hardware/software requirements.} It is shown how to create an environment for development and test embedded applications using GDB to cross debug the code and QEMU to simulate its execution.

%\end{itemize}

\section{Related Work}
The automated testing area has a vast literature that inspired this proposal. There are several approaches for debugging general purposing systems, and both parameters exchange script and  test environment for debugging embedded systems applications were designed based on them.

Seo et al.~\cite{seo} proposes an interface technique that identify and classify interfaces between embedded system layers. They created a model based tool that generates and executes test cases to analyze these interface layers. They also proposed the emulation test technique that integrates monitoring and debugging embedded systems. Despite the similarity, since we also emulate the target board environment and monitor the behavior of environment variables, Seo et al. focus on testing interfaces and layers, while AEP proposal addresses the testing of components and their integration.

ATEMES~\cite{atemes} is a tool for automatic random tests, including coverage testing, unit testing, performance testing and race condition testing. ATEMES supports instrumentation of source code, generation of tests cases and generation of primitive input data for multi-core embedded software. This system is similar to ours since we also automatically run random tests under cross-testing environment to support embedded software testing. However, the idea of this present work is to integrate the exchange parameters directly in the operating system, so it is possible not only test the application as well as optimize the choice of the configuration parameters.

Statistical Debugging techniques~\cite{zheng2006statistical,zhang2009capturing,parsa2011statistical} are capable of isolating a bug by automatic running an application several times and using generated statistical data to analyze these executions information. This statistical analysis can reduces the bug search area by pinpointing a suspiciousness ranking. This technique could not be incorporated into an embedded system, due to the need of large data set to accomplish this statistic and the necessity to have a big data storage to keep information of all executions. However, a ranking pointing out possible errors is a breakthrough in the developer's work. Therefore, the AEP uses a debugging environment was build on a machine with more storage and processing than the embedded system.

In program slicing~\cite{sasirekha2011program, Xu:2005:BSP:1050849.1050865,artho2011iterative}, the main idea is to divide the code into different parts, then test and remove paths that do not lead to errors. This technique has two approaches for reducing the path that lead to error: static slicing and dynamic slicing. Static slicing has faster reduction of application path, since the final set of paths leading to the error are an approximation of the real set. In dynamic slicing, the initial entry has a great influence on how to slice the code, allowing a greater precision for final errors path. This technique is interesting because it needs only one error path to simplify the group of inputs to be examined. AEP proposal intends to support both types of slicing through configuration files (traits) that can address the whole system or just a  part of the application.

In capture and replay~\cite{burger2008replaying,qi2011locating,orso2005selective} the program is executed until it reaches the end and all operations performed are stored in a log. Burger and Zeller developed a JINSI tool that can capture and replay interactions between inter/intra-components. So  all relevant operations are observed and run step by step, considering all communications between two components until find the bug. Besides being the most widely used, this technique needs to perform all possible paths from one object to another, making this technique time consuming. AEP script runs in a highly configurable operating system, on which  it is possible to plug a single component with different implementations. The design of AEP absorbs the idea of debug focusing on the components that compose the application.

\section{Embedded Systems Debugging Environment}
This section presents details of the debugging process, simulation and how to integrate both in order to create a better environment for developing and testing embedded applications.

Regardless of the technique to be used, debugging can be accomplished in two ways: locally and remotely. Local debugging is when the application runs on the same machine as the debugger. As a result, the latency of application and debugger communication is lower. However, the application interferes in the debug process, e.g. if the application under test crashes, the debugger will need to halt or restart to seek the cause.

 In remote debug,  this influence does not happen since application and debugger run over separate machines. The tests are performed into an isolated box over a network connection. Despite of having some latency issues, from the debugging point of view, the rest of the process can be viewed as a local debug with two screens connected in only one system.

In order to provide the most number of possibilities for the developer, the emulator used to debug applications must provide both ways to perform this activity. Also, for a useful debug, developers must consider others concepts involved in debugging, such as, how to configure the code execution mode, to observe the application outputs, watch some environment's variables, log the tasks performed and others configurations. This requires a good ally to become possible follow program steps, analyze executing state a moment before a crash or even to specify anything that might affect its behavior.

\subsection{Debugging with QEMU and GDB}
\label{sec:simulationEnv}
QEMU is a generic and open source machine emulator and virtualizer. When used as a machine emulator its possible to run applications made for one machine to another via dynamic translation. The decision to use QEMU emulator was based on active community, support of Linux as the host machine, a native set of target machines and the possibility to integrate a new machine.

Thus, besides having QEMU to emulate applications, we still need to examine the state and variables of the application. Using GDB - \textit{the GNU Project Debugger} it is possible to see inside the application while it executes \cite{gdb}. One important characteristic of GDB is to enable remote debug. Therefore, it is possible to run the program on a given embedded platform while we debug it with GDB running in separated machine. In remote debugging, GDB connects to a remote system over a network and then control the execution of the program and retrieve information about its state.

The integration of both is particular for each host/target machine; thus, some steps presented here must be tailored depending on your target architecture. Figure~\ref{fig:qemu_gdb_gray} presents the activities required to perform remote debugging using IA-32 architecture. These steps and additional explanation of which techniques and tools are used in this process are listed bellow:

\fig{qemu_gdb_gray}{Steps to integrate QEMU and GDB}{scale=.3}

\begin{enumerate}
\item \textbf{Compile with debug information} is the first and the most important step. The source code is the input and the output is the compiled application that has debug information. Using GCC (\textit{GNU project C and C++ compiler}) it can be performed by using \texttt{-g} option to compile.

\item \textbf{Emulate with QEMU} is a necessary step to execute the application in the correct target architecture. To perform this step, the developer must initiate QEMU with \textit{-s -S} options. The first option enables the GDB stub, in order to open communication between QEMU and GDB. The \textit{-S} option is used to force QEMU to wait GDB to connect after the system restart, e.g., if we compile an application with debug information (\textit{app.img}), that prints information in the screen (\textit{stdio}), QEMU call should look like \\
 \texttt{qemu -fda app.img -serial stdio -s -S}

\item \textbf{Connect with GDB} starts with a GDB session, that must be initialized in an separate window. Then, to connect GDB in QEMU the developer must explicitly specify that the target to be examined is remote and inform the host address and port of the target (in this case, QEMU). When host is in the same machine as GDB, its possible inform only the port, but the complete line must be similar to: \texttt{target remote [host]:[port]}


\item \textbf{Recovery debug information} is an important step to help developers to find errors, once its possible to use autocomplete to recovery the all name contained in the symbols table. The file used to keep debug information (as the path) must be informed to GDB using the command:
\texttt{file [path\_to\_the\_file]}

\item \textbf{Finding errors} is an activity that depends on the program to be debugged. From this step, the developer can set breakpoints, watchpoints, control the execution of the program and even enable logs. More information about command set can be found in GDB's page \footnote{http://www.gnu.org/s/gdb/}.
\end{enumerate}


\section{Automatic Exchange of Embedded System Software Parameters}
The algorithm of automatic exchange of parameters is independent of the operating system and platform. However it works on the premise that the system followed an  application oriented system design (AODS) and uses generic programming techniques. It is also desirable that each abstraction of the operating systems can be configured as desired using traits template parameter \cite{Stroustrup:c++}. 

The algorithm~\ref{algoritmo_AEP} shows the steps from the choice of a test application  until developer receives the report with all attempts. It flows in order to find the desired trait, exchange it for a predetermined amount (or random), run the new application and collect the feedback that the application returns.

\begin{algorithm}
\begin{algorithmic}
\REQUIRE file (Test configuration)
\ENSURE Report of tries
\STATE{traits$\Leftarrow$ GetTraitFile(file);}
\STATE{application$\Leftarrow$ GetApplicationFile(file);}
\IF{the file has any configuration value}
  \FORALL{configuration in file}
     \STATE{line$\Leftarrow$ GetTheConfiguration(configuration, traits);}

     \FORALL{value in configuration values}
        \STATE{newTrait$\Leftarrow$ ExchangeValue(line, traits);}
        \STATE{newApp$\Leftarrow$ Compile(application, newTrait);}
        \STATE{report$\Leftarrow$ report + Emulate(newApp);}
     \ENDFOR
   \ENDFOR
\ELSE
  \IF{the file has one maximum number of tries}
  	  \STATE{maxNumberTries$\Leftarrow$ GetMaxSize(file);}
  \ELSE
        \STATE{maxNumberTries$\Leftarrow$ GetRandomNumber();}
  \ENDIF
  
  \WHILE {tries < maxNumberTries}
	\STATE{line$\Leftarrow$ GetRandomNumber();}
    \STATE{newTrait$\Leftarrow$ ExchangeValue(line, traits);}
    \STATE{newApp$\Leftarrow$ Compile(application, newTrait);}
    \STATE{report$\Leftarrow$ report + Emulate(newApp);}
  \ENDWHILE
\ENDIF
\RETURN report;

\end{algorithmic}
\caption{Algorithm Exchange Configuration Parameters}\label{algoritmo_AEP}
\end{algorithm}

The configuration file (input) has the only two mandatory information for the beginning of the algorithm, the path to application and its trait. 

\subsection{Configuration file}
\label{sec:configuration_file}
To run the script with a specific configuration it was necessary to manually fill the information before each new round of tests. 

To improve the usability of the script, it is possible to define a configuration file with the information needed to run the test. We chose XML to set test settings because it can set all necessary rules to run the script in a way human-readable and furthermore is also easily interpreted by the computer.

Extensible Markup Language (XML)~\cite{bray1997extensible} describes the behavior of computer programs and it documents are made up of parsed (characters) or unparsed data (markup encodes). Figure~\ref{progxml:philosopher_xml} brings one example of the AEP configuration file for philosophers dinner application.

\progxml{philosopher_xml}{Example of AEP configuration file.}

The configuration will be set up only once, but even if it requires some manual changes it will not add difficulties, since this file can be read almost as a text: there is a \texttt{<Test>} of the \textit{philosopher\_dinner\_app} \texttt{<Application>} with two \texttt{<Configuration>}: one \texttt{<Trait>} and one \texttt{<Breakpoint>}. The \texttt{<Trait>} identified as \textit{ARCH} could be the \texttt{<Value>} \textit{IA32} or the \texttt{<Value>} \textit{AVR8}. It should use the \texttt{<Breakpoint>} file that can be found in "\textit{/home/breakpoint\_philosopher.txt}" \texttt{<path>}.

AEP current implementation works on the Embedded Parallel Operating System (EPOS) \cite{Froehlich:2001} since it adds a great deal of configurability of the system, which is very suitable for evaluating an exchange configuration script.For a better understanding of the implementation of AEP, we briefly explain how to configure abstractions on EPOS.

\subsection{EPOS}
EPOS is a component-based framework that provides all traditional abstractions of operating systems and services like memory management, communication and time management. Furthermore,  several academic research and industrial projects uses EPOS as base\footnote{http://www.lisha.ufsc.br/pub/index.php?key=EPOS}.

This operating system is instantiated only with the basic support for its dedicated application. It is important to highlight that an individual member of a trait is a characteristic of the system and all features of a component must be set appropriately for a better performance of the system. In this context, the automated exchange of these parameters can be used both to discovery a failure in the program by wrong characterization of components, or to improve the performance for the application by selecting a better configuration.

In EPOS, each application has its own trait to define their behavior. The figure~\ref{progcpp:trait_build} shows one excerpt of this configuration for an application that simulates the dining philosophers. This excerpt point how to build the application, which in this case the it should run in library mode, for an IA-32 (Intel Architecture, 32-bit), using a PC (Personal Computer) machine for generation of the system.
\progcpp{trait_build}{Excerpt of trait from philosophers dinner application.}

\section{AEP in a real-world application}
The Distributed Motion Estimation Component (DMEC) is a component to perform motion estimation by exploiting similarity between adjacent images in a video sequence. This estimation allows images to be coded differentially, increasing the compression ratio of the generated bitstream. Motion Estimation is a significant stage for H.264 encoding since it consumes around 90\% of the total time of the encoding process ~\cite{dmec}.

DMEC's test check the performance of motion estimation using a data partitioning strategy while \texttt{Workers} threads estimate and the \texttt{Coordinator} process results ~\cite{dmec}.

\fig{dmec}{Interaction between the \texttt{Coordinator} and \texttt{Workers} threads ~\cite{dmec}}{scale=.4}

Figure~\ref{fig:dmec} presents the interaction between the threads. The \texttt{Coordinator} is responsible for defining the partitioning of picture, provide the image to be processed, and return results generated to encoder while \texttt{Workers} must calculate motion cost and motion vectors.

The Distributed Motion Estimation Component was tested using the integrated environment demonstrated in the section~\ref{sec:simulationEnv}. The AEP script generated random values for \texttt{NUM\_WORKERS} configuration of DMEC, and since all exchanges were integer values, all of them were compiled and debugged in GDB, but only some could be emulated by QEMU.

This application was configured with breakpoints in all functions, specially the main function, see Figure~\ref{fig:gdb_dmec_60_workers}. The expected was to reach 5 breakpoints, but after the 3rd breakpoint, "continuing" is the last line that appears in the execution before it crashes.

\fig{gdb_dmec_60_workers}{DMEC debug with GDB execution with \texttt{NUM\_WORKERS} = 60.}{scale=.43}

All this steps are automated, and developer will receive a log with information about each configuration that could not even reach the main function. This log could tip the developer about this application fail with a large number of threads and, in this case, emulation and breakpoints were crucial to determine an error in DMEC.

\section{Evaluation}
\label{sec:evaluation}
The evaluation was performed with two applications: philosopher dinner and DMEC. Both were generated from a x86\_64 Linux system, with EPOS 1.1 and compiled with GNU 4.4.4 (IA32 architecture) or GNU 4.0.2 (AVR8 architecture). The integrated environment is composed by GDB 7.5 and QEMU 1.4.0.

Evaluation considered data from random and partially random tests. Random test is the one that has no prior information on the application. So any configuration within \texttt{traits} can change, including parameters that not influence the application. On the other hand, a partially random test has some tips about application, such as relevant settings and valid configurations. In other words, the script changes only parameters that directly influences the application.

For a better analysis of the effectiveness of the tool, test attempts were classified into: compiled, relevant and repeated.. Compilable are those that could be compiled without errors and generated a valid image for the system. The relevant beyond those that are compilable are useful for testing the system, i.e. a configuration modified to alter a system's behavior. Repeated attempts are retries of relevant test. With this classification, the effectiveness will be a compromise between the attempts and the number of relevant tests (minus repeated attempts).

\subsection{Philosopher dinner}
The dining philosophers application is the example used to explain the AEP implementation and setup. In this section, it will be the basis of the analysis of the effectiveness of changes made. Figure~\ref{fig:philosopher_results} presents the classification of data from random test and partial random test (configured as described in Section~\ref{sec:configuration_file}).

\fig{philosopher_results}{Philosopher dinner - data results}{scale=.4}

With this classification it is possible to note that the effectiveness of the random test (7\%) is lower than that of the partially random test (100\%). If only considered the effectiveness can be inferred that the greater the amount of useful tips that script receives, the greater chance to reach full productivity. Recalling that  the effectiveness of the algorithm is closely tied to the effectiveness of the configuration received.


\fig{philosopher_time_results}{Philosopher dinner - time results}{scale=.4}

\fig{philosopher_size_results}{Philosopher dinner - size results}{scale=.7}


\subsection{DMEC}

\fig{dmec_results}{DMEC - data results}{scale=.6}

\fig{dmec_time_results}{DMEC - time results}{scale=.45}

\fig{dmec_size_results}{DMEC - size results}{scale=.7}


\section{Conclusion}
In this paper, we introduce the automatic exchange of configuration parameters ans show how to set up a development environment for embedded applications based on specific hardware/software requirements.

The integrated development environment provides independence of the physical target platform for development and test. It is an important step since some embedded systems may not be able to store the extra data needed to support debug. The impact of enable debug information in code size and the execution time of the real-world application was more than 80\%. Also, developers no longer need to spend time understanding a new development platform whenever some characteristic of the embedded system changes.

The example application is naive and has only 30 parameters settings to make the exchange, but results show the a considerable difference between the two testing approaches and confirm that the effectiveness of the algorithm is closely tied to the effectiveness of the configuration received.

AEP was evaluated using two kinds of test. The fully automated test works with no prior information of the application, but it was possible to generate valid configurations that could be tested as alternative solutions. In partial automated test, all generated configurations were valid, and the report was useful to discovery that some parameter values were better than others.



\bibliographystyle{abbrv}
\bibliography{references}

\end{document}