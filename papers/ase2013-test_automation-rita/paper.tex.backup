\documentclass[conference]{IEEEtran}
\pdfpagewidth=8.5in
\pdfpageheight=11in

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{microtype}
\usepackage{balance}


\usepackage{float}
\usepackage{color,graphicx}
\usepackage[figuresright]{rotating}


\newcommand{\fig}[4][thb]{
  \begin{figure}[#1] {\centering{\includegraphics[#4]{fig/#2}}\par}
    \caption{#3\label{fig:#2}}
  \end{figure}
}

\begin{document}

\title{AEP - Automatic Exchange of Embedded System Software Parameters.}

\author{
	\IEEEauthorblockN{Rita de Cássia Cazu Soldi and Antônio Augusto Medeiros Fröhlich}
	\IEEEauthorblockA{Federal University of Santa Catarina (UFSC)\\
					  Laboratory for Software and Hardware Integration (LISHA)\\
					  Florianópolis, SC, Brazil\\
					  rita@lisha.ufsc.br\\
					  guto@lisha.ufsc.br
	}
}
\maketitle

\begin{abstract}
Embedded systems are present in most of humans' daily activities and they are so widespread that seem to be part of the natural process. Each of this system has a different task, but there are recent efforts to join them in a large and complex network in order to increase the welfare of the population.

To ensure that there will not be suggested harmful actions to humans, it requires a good check before deploying this complex network. However, testing embedded systems is a marathon, since the developer performs the process repeatedly on different platforms, which vary according to the architecture, languages and vendors.

This paper presents AEP, a tool for  Automatic exchange of embedded system software parameters. This tool can withdraw all configuration required to proceed test from a XML file, then it uses cross debug and emulation to automatic run tests and offer a full report of all tries to developers.  

AEP was evaluated using a real world problem in terms of valid information in report and memory consumption. The obtained results indicates that even with no previous information this tool can produce helpful answers for developers to find and fix bugs.

\end{abstract}



\section{Introduction}
An embedded system can be presented as a combination of software and hardware designed to perform a specific task. These systems are widely attached to numerous electronic devices, and their activities are becoming more popular and intrinsic to daily life \cite{carro2003sistemas}. These systems are a key technology in the path toward the smart world since they can collect data and perform computations about the environment in which they are inserted.

Applications involving environmental monitoring and analysis, intelligent cities, and precision farming are only a sample of a set of possible applications. But with the advent of smart things and internet of things, it became even more interesting to increase communications and the computing power in these systems, enabling  intelligent behavior to exchange data with other systems through a large network\cite{smartThings}. 

The motivation for build an intelligent world is  to produce a greater comfort to human beings, however, the realization of this idea should be based on the certainty that each component of this network is performing its activities properly. When a software deviates from the expected/specified behaviour it can result in disasters involving loss of market share, client information, people time, etc.\cite{tassey2002economic}. Therefore, it is essential that tests be thoroughly performed to ensure that these smart things are properly performing the activities assigned to them.

The debugging task consumes a considerable  time and brings many challenges since it requires a thorough inspection of the source code making it a non-trivial process  \cite{parnin2011automated}.Coding and testing embedded systems is very defiant, since developers need to find out how to optimize the use of the scarce resources and the platform depends on operating systems, architecture, vendors, debugging tool, etc\cite{schneider2004ten}. This makes embedded systems more susceptible to errors as well as specification failures.

There are several approaches for debugging applications and each one has a different degree of automation, nevertheless, the more automated is the process more information about the application it needs.

Most of the debug tools is partially automated and requires some interaction with the developer to make decisions during testing \cite{campos2012gzoltar,tracingDiagnose}. These tools save some development time, but not as much as total automation tools. The automation of the entire testing process without any human intervention is still a challenge to researchers, although there are some studies that can automate part of the process with data taken directly from the application \cite{Larson:2013:MDAT,JSWjsw0803603616}.

In this paper, we propose the automation of one part of the debugging process for embedded systems' application, the automatic exchange of configuration parameters (AEP). In this proposal,  there is a shell script responsible for exchanging configuration parameters according to an XML file. Also, a solution to the problem of setting up a stable environment for testing embedded systems is also part of the contribution.

The rest of this paper is organized as follows. Section \ref{sec:relatedWork} presents the related work. Section \ref{sec:simulationEnv} contain the details of the integrated GDB and QEMU environment for debugging embedded applications. Section \ref{sec:automatic} presents the solution of exchange parameter's configuration applied in a study case. Section \ref{sec:evaluation} shows the results and finally Section \ref{sec:Conclusion} concludes the paper.

\section{Related Work}
\label{sec:relatedWork}
The automated testing area has a vast literature that inspired this proposal. There are several approaches for debugging general purposing systems, and based on these works we created the parameters exchange script and the test environment for debugging embedded systems applications as a first step to achieve a fully automated tool.

The work presented by Seo et al. \cite{seo} proposes an interface technique that identify and classify interfaces between embedded system layers. They created a model based tool that generates and executes test cases to analyse these interface layers. Furthermore, they proposed the emulation test technique that integrates monitoring and debugging embedded systems. Despite the similarity, since we also emulate the target board environment and monitor the behaviour of environment variables, Seo et al. focus on testing interfaces and layers, while this proposal addresses the testing of components and their integration.

ATEMES \cite{atemes} is a tool for automatic random tests, that includes coverage testing, unit testing, performance testing and race condition testing. ATEMES supports instrumentation of source code, generation of tests cases and generation of primitive input data for multi-core embedded software. This system is similar to ours since we also automatically run random tests under cross-testing environment to support embedded software testing. However, the idea of this work is to integrate the exchange parameters directly in the operating system, so it is possible not only test the application as well as optimize the choice of the configuration parameters.

Statistical Debugging techniques \cite{zheng2006statistical,zhang2009capturing,parsa2011statistical} are capable of isolating a bug by automatic running an application several times and using generated statistical data to analyse these executions information. As a result of this analysis it is possible to pinpointing a suspiciousness ranking, that reduces the bugs' search area. This technique could be only  incorporated into an embedded system by using cross-debug  since it needs  large data set to accomplish this statistic and the necessity to save  information of all executions.  Although the ranking pointing out possible errors is a breakthrough in the developer's work, its incorporation  on a debugging environment requires a machine with more storage and processing than is not available on an embedded system.

In program slicing \cite{sasirekha2011program, Xu:2005:BSP:1050849.1050865,artho2011iterative} the main idea is to divide the code into different parts, testing and removing paths that do not lead to errors. This technique has two approaches for reducing the path that lead to error: static slicing and dynamic slicing. Static slicing has faster reduction of application path. However, it does not  consider the initial entry of the program and then the final set of paths leading to the error are an approximation of the real set. In dynamic slicing,  the initial entry has a great influence on how the slices are performed, allowing a greater precision for final errors path. This technique is interesting because it needs only one error path to simplify the set of inputs to be examined. This present proposal was designed to support both types of slicing through configuration files (traits) that can address the whole system or just a part of the application.

In capture and replay \cite{burger2008replaying,qi2011locating,orso2005selective} the program is executed until it reaches the end and all operations performed are stored in a log. Burger and Zeller developed a JINSI tool that can capture and replay interactions between inter/intracomponent. So all relevant operations are observed and run step by step, considering all communications between two components until find the bug. Besides being the most widely used, this technique is time consuming when it needs to perform all possible paths from one object to another. From this technique, our work absorbs the idea of debug focusing on the components that compose the application. We use a highly configurable operating system, on which can be plugged in a single component with different implementations, so a developer can verify the difference between them.

\section{Embedded Systems Debugging Environment}
\label{sec:simulationEnv}
This section presents details of the debugging process, simulation and how to integrate both in order to create a better environment for developing and testing embedded applications.

Regardless of the technique to be used, debugging can be achieved locally or remotely. Local debugging is when the application runs on the same machine as the debugger. As a result, application and debugger have a lower communication latency. However, the application interferes in the debug process, e.g. if the application under test crashes, the debugger will need to halt or restart to seek the cause.

This influence does not happen in remote debug, once application and debugger run in separate machines. The tests are performed into an isolated box over a network connection. Despite of having some latency issues, from the debugging point of view, the rest of the process can be viewed as a local debug with two screens connected in only one system.

In order to provide the most number of possibilities for the developer, the emulator used to debug applications must provide both ways to perform this activity. Also, for a useful debug, developers must consider others concepts involved in debugging, such as, how to configure the code execution mode, to observe the application outputs, watch some environment's variables, log the tasks performed and others configurations. This requires a good ally to follow program steps and analyse executing state a moment before a crash, or even to specify anything that might affect its behaviour.

\subsection{Debugging with QEMU and GDB}
QEMU is a generic and open source machine emulator and virtualizer. When used as a machine emulator,  it is possible to run applications made for one machine to another via dynamic translation. The decision to use QEMU emulator was based on active community, support of Linux as the host machine, a native set of target machines and the possibility to integrate a new machine.

Thus, besides having QEMU to emulate applications, we still need to examine the state and variables of the application. Using GDB - \textit{the GNU Project Debugger} - it is possible to see inside the application while it executes \cite{gdb}. One important characteristic of GDB is to enable remote debug. Therefore, it is possible to run the program on a given embedded platform while we debug it with GDB running in separated machine. In remote debugging, GDB connects to a remote system over a network and then control the execution of the program and retrieve information about its state.

The integration of both is particular for each host/target machine; thus, some steps presented here must be tailored depending on your target architecture. Figure~\ref{fig:qemu_gdb_gray} presents the activities required to perform remote debugging using IA-32 architecture. These steps and additional explanation of which techniques and tools are used in this process are listed bellow:

\fig{qemu_gdb_gray}{Steps to integrate QEMU and GDB}{scale=.25}

\begin{enumerate}

\item \textbf{Compile with debug information} is the first and the most important step. The source code is the input, and the output is the compiled application that has debug information. Using GCC (\textit{GNU project C and C++ compiler}) it can be performed by using \texttt{-g} option to compile.

\item \textbf{Emulate with QEMU} is a necessary step to execute the application in the correct target architecture. To perform this step, the developer must initiate QEMU with \textit{-s -S} options. The first option enables the GDB stub, in order to open communication between QEMU and GDB. The \textit{-S} option  forces QEMU to wait GDB to connect after the system restart, e.g., if we compile an application with debug information (\textit{app.img}) that prints information in the screen (\textit{stdio}), QEMU call should look like \\ \texttt{qemu -fda app.img -serial stdio -s -S}

\item \textbf{Connect with GDB} starts with a GDB session that must be initialized in a separate window. Then, to connect GDB in QEMU the developer must explicitly specify that the target to be examined is remote and inform the host address and port of the target (in this case, QEMU). When host is in the same machine as GDB, it is possible inform only the port, but the complete line must be similar to:\\ \texttt{target remote [host]:[port]}

\item \textbf{Recovery debug information} is an important step to help developers to find errors, once it is possible to autocomplete to recovery  all name contained in the symbols table. The file used to keep debug information (as the path) must be informed to GDB using the command: \\ \texttt{file [path\_to\_the\_file]}\\

\item \textbf{Finding errors} is an activity that depends on the program to be debugged. From this step, the developer can set breakpoints, watch points, control the execution of the program and even enable logs. More information about command set can be found in GDB's page \footnote{http://www.gnu.org/s/gdb/}. \end{enumerate}

\section{Automatic Exchange of Configuration Parameters}

\label{sec:automatic}

The automatic exchange of configuration parameters (AEP) is part of an effort to integrate a new debugging tool into operating system methods in order to reduce software development efforts.

The present work uses on the Embedded Parallel Operating System (EPOS) \cite{Froehlich:2001} since it adds a great deal of configurability of the system, which is very suitable for evaluating an exchange configuration script. Also, EPOS is a component-based framework that provides all traditional abstractions of operating systems and services like memory management, communication and time management. Furthermore, it was referenced in several \footnote{http://www.lisha.ufsc.br/pub/index.php?key=EPOS} academic and industrial projects.

Once EPOS uses generic programming techniques, each abstraction can be configured as desired using traits template parameter \cite{Stroustrup:c++}. Traits are parametrized classes that describe the properties of a given object/algorithm.

\fig{traits}{Set of definitions from a traits class in EPOS}{scale=.5}

Figure~\ref{fig:traits} shows a piece of traits classes used in EPOS to configure the main thread as a set of static members that describes some definitions used by this abstraction.

This operating system instantiates only with the basic support for its dedicated application. It is important to highlight that an individual member of a trait is a characteristic of the system and all features of a component must be set appropriately for a better performance of the system. In this context, the automated exchange of these parameters can be used both to discovery a failure in the program by wrong characterization of components, or to improve the performance for the application by selecting a better configuration.

Figure~\ref{fig:script_gray} is an overview of how the exchange of configurations parameters occurs. Basically, a trait information is selected, and its definition is changed according to the specification,  then  the application is recompiled. Traces generated by each version of application is compared and reported to the developer. Before a new cycle is complete, the application execution is verified by the integrated test environment. The performance is analysed and compared with other versions of the application, which also generates an execution report.

\fig{script_gray}{Overview of automatic exchange of configuration parameters}{scale=.26}

In the current version of the AEP shell script, the selection of  configuration and its parameter modification are random, but can also be supplemented with an artificial intelligence tool or some application-oriented system design tool to provide all information for the script.




\subsection{Real-World Application} The automatic exchange of parameters script was used to test the Distributed Motion Estimation Component (DMEC). This component performs a motion estimation that exploits the similarity between adjacent images in a video sequence, which allows images to be coded differentially, increasing the compression ratio of the generated bitstream. Motion Estimation is a significant stage for H.264 encoding since it consumes around 90\% of the total time of the encoding process \cite{dmec}.

DMEC's test check the performance of motion estimation using a data partitioning strategy while \texttt{Workers} threads estimate and the \texttt{Coordinator} processes results \cite{dmec}.

\fig{dmec}{Interaction between \texttt{Coordinator} and \texttt{Workers} threads \cite{dmec}}{scale=.3}

Figure~\ref{fig:dmec} presents the interaction between the threads. The \texttt{Coordinator} is responsible for defining the partitioning of picture, provide the image to be processed and return results generated to encoder, while \texttt{Workers} must calculate motion cost and motion vectors.

The Distributed Motion Estimation Component was tested using the integrated environment demonstrated in the section \ref{sec:simulationEnv}. Despite the first part of the script generates multiple configurations, only compile the code does not guarantee that the application is bug free. Figures \ref{fig:qemu_dmec_6_workers} and \ref{fig:qemu_dmec_60_workers} show the DMEC execution using values 6 and 60 for \texttt{NUM\_WORKERS} configuration.

\fig{qemu_dmec_6_workers}{DMEC emulated execution with \texttt{NUM\_WORKERS} = 6}{scale=.42} \fig{qemu_dmec_60_workers}{DMEC emulated execution with \texttt{NUM\_WORKERS} = 60}{scale=.42}

The difference between the two scenarios is that after retrieving information from the application, QEMU has a response only for the six \texttt{workers} configuration.

Part of the configurations changed by the script do not even compile. The AEP script uses GDB for debugging all configurations that could be compiled. This process was crucial to determine the error in DMEC's case. In this sense, some breakpoints were added to all functions, specially the main function, see Figure \ref{fig:gdb_dmec_60_workers}. It is possible to check "continuing" is the last line that appears in the execution. The execution failed because a "too high" value was defined for the number of threads.

\fig{gdb_dmec_60_workers}{DMEC debug with GDB execution with \texttt{NUM\_WORKERS} = 60}{scale=.43}

Through the second part of the script, i.e., the debugging process, it was possible to verify that the program not even reach the main function, which means that now the script must change configurations before calling the main function again.

\section{Evaluation}

\label{sec:evaluation}

Tests were performed with the chosen application, running under EPOS 1.1 and compiled with GNU 4.5.2 for IA32 architecture. The integrated environment is composed by GDB 7.2 and QEMU 0.14.0. Evaluation considered data from totally random and partially random tests usingof DMEC application.

Totally random test is the one that has no prior information on the application. In other words, any configuration within \texttt{traits} can change, including parameters that not influence the application. Figure~\ref{fig:comp_report_total} presents a piece of the report with some generated configuration, e.g., the size of the application stack if a thread should be busy waiting, the value of a quantum, how much cycles clock should consider, etc.

\fig{comp_report_total}{Totally random generated configurations}{scale=.36}

In total random execution case, test performed hundred tries, generating 85 different configurations in which only 23\% of them could be correctly compiled, but less than 5\% of configurations were relevant to DMEC. Figure~\ref{fig:total_random} presents the ratio of the different configurations generated, those that have been compiled and those that were actually relevant.

\fig{total_random}{Totally random configurations versus their relevance}{scale=.6}
 
The execution of these versions showed that the application did not change significantly, since most part of exchanged parameters not influence the application.

On the other hand, a partially random test has some tips about application, such as relevant settings and valid configurations. In other words, the script changes only parameters that directly influences the application. This second test was concentrated in only one configuration, the number of \texttt{Workers}. By focusing in only one parameter, it became possible to try  finding the best option for the application. Figure~\ref{fig:comp_report_partial} presents a report with all tries.

\fig{comp_report_partial}{Partially random generated configurations}{scale=.43}

The test got 69 different configurations (same number of tries) and all of them could be correctly compiled, and only 8 could be executed, in other words, less than 10\% of tries could be used as configuration. Figure~\ref{fig:partial_random} presents the ratio of the different configurations versus its relevance.

\fig{partial_random}{Partially random configurations versus their relevance}{scale=.6}

Also, to use the integrated debug environment it was necessary to build the code with a special set-up, in which it is possible to generate information about the application to be tested.

Without this information,  the original DMEC image consumes more than 50kB, but with the generation of debug symbols the new image consumes about 70 kB and increases in 80\% the cost in terms of memory to debug DMEC application.

\section{Conclusion}

\label{sec:Conclusion}

In this paper, we introduce the automatic exchange of configuration parameters ans show how to set up a development environment for embedded applications based on specific hardware/software requirements.

The integrated development environment provides independence of the physical target platform for development and test. It is an important step since some embedded systems may not be able to store the extra data needed to support debug. The impact of enable debug information in code size and the execution time of the real-world application was more than 80\%. Also, developers no longer need to spend time understanding a new development platform whenever some characteristic of the embedded system changes.

The automatic exchange was evaluated using two kinds of test. The fully automated test works with no prior information of the application, but it was possible to generate valid configurations that could be tested as alternative solutions. In partial automated test,  all generated configurations were valid, and the report was useful to discovery that some parameter values were better than others.

In this sense, it was possible to realize that even a small automation solution produce answers to help developers finding and fixing bugs. With only a hundred tries were possible to find error/restriction in the code.
\bibliographystyle{abbrv}
\bibliography{references}

\end{document}