\documentclass{ufscThesis}
\usepackage{graphicx}
\usepackage[labelsep=endash]{caption} % O separador de legenda é um -
\usepackage{color,graphicx}
\usepackage[lined,ruled,linesnumbered,portuguese]{algorithm2e}
\usepackage{listings}
\usepackage{verbatim}
\usepackage[printonlyused,smaller]{acronym}

\lstloadlanguages{C,HTML}
\lstdefinestyle{prg}{basicstyle=\scriptsize\sffamily, lineskip=-0.2ex, showspaces=false}

\definecolor{gray}{rgb}{0.4,0.4,0.4}
\definecolor{darkblue}{rgb}{0.0,0.0,0.6}
\definecolor{cyan}{rgb}{0.0,0.6,0.6}

\SetKwInput{KwData}{Dados}
\SetKwInput{KwResult}{Resultado}
\SetKwInput{KwIn}{Entrada}
\SetKwInput{KwOut}{Sa\'{i}da}
\SetKw{KwTo}{para}
\SetKw{KwRet}{retorne}
\SetKw{Return}{retorne}
\SetKwBlock{Begin}{in\'{i}cio}{fim}
\SetKwComment{tcc}{/*}{*/}
\SetKwComment{tcp}{//}{}
\SetKwIF{If}{ElseIf}{Else}{se}{ent\~{a}o}{sen\~{a} se}{sen\~{a}o}{fim}
\SetKwSwitch{Switch}{Case}{Other}{comute}{faça}{caso}{caso contr\'{a}rio}{fim da comutaç\~{a}o}
\SetKwFor{For}{para}{fa\c{c}a}{fim}
\SetKwFor{While}{enquanto}{fa\c{c}a}{fim}
\SetKwFor{ForEach}{para cada}{fa\c{c}a}{fim}
\SetKwRepeat{Repeat}{repita}{at\'{e}}

\titulo{AUTETESE: Uma abordagem de automação da execução de testes de software embarcado} % Titulo do trabalho
\autor{Rita de Cássia Cazu Soldi} % Nome do autor
\data{10}{dezembro}{2014} % Data da publicação do trabalho

\orientador{Prof. Dr. Antônio Augusto Medeiros Fröhlich} % Nome do orientador e (opcional)
%\departamento[a]{Faculdade de Ciências do Mar}
%\curso[a]{Atividade de Extensão em Corte e Costura}

\begin{acronym}
\acro{ADESD}{\textit{Application-Driven Embedded System Design}}
\acro{AUTETESE}{Automação da execução de testes de \textit{software} embarcado}
\acro{DD}{\textit{Delta Debugging}}
\acro{DMEC}{\textit{Distributed Motion Estimation Component}}
\acro{DID}{\textit{Discovery Interface Device}}
\acro{EPOS}{\textit{Embedded Parallel Operating System}}
\acro{GCC}{\textit{GNU Compiler Collection}}
\acro{GDB}{\textit{GNU Project Debugger}}
\acro{ICE}{\textit{In-circuit emulator}}
\acro{IDD}{\textit{Iterative Delta Debugging}}
\acro{IEEE}{\textit{Institute of Electrical and Electronics Engineers}}
\acro{JTAG}{\textit{Joint Test Access Group}}
\acro{MCO}{\textit{The Mars Climate Orbiter}}
\acro{MPL}{\textit{The Mars Polar Lander}}
\acro{QEMU}{\textit{Quick Emulator}}
\acro{RSSF}{Redes de Sensores Sem Fio}
\acro{TAP}{Troca Automática de Parâmetros de \textit{Software}}
\acro{TBT}{\textit{Target Based embedded system Testing}}
\acro{SUT}{\textit{Software Under Test}}
\acro{XML}{\textit{eXtensible Markup Language}}
\end{acronym}

\newcommand{\fig}[4][h!]{
  \begin{figure}[#1] {\centering{\includegraphics[#4]{fig/#2}} \par}
    \caption{#3\label{fig:#2}}
  \end{figure}
}


\newcommand{\progcpp}[3][h]{
 \begin{figure}[#1]
     \lstinputlisting[language=C,style=prg]{fig/#2.h}
   \caption{#3\label{progcpp:#2}}
 \end{figure}
}

\newcommand{\progxml}[3][hp]{
 \begin{figure}[#1]
     \lstinputlisting[language=XML,style=prg]{fig/#2.xml}
   \caption{#3\label{progxml:#2}}
 \end{figure}
}


%%% Sobre a Banca
\numerodemembrosnabanca{3} % Isso decide se haverá uma folha adicional
%\orientadornabanca{nao} % Se faz parte da banca definir como sim
%\bancaMembroA{Prof. Dr. Antônio Augusto Medeiros Fröhlich}[Orientador] %Nome do presidente da banca
\bancaMembroA{} % Nome do membro da Banca
\bancaMembroB{} % Nome do membro da Banca
\bancaMembroC{} % Nome do membro da Banca

\agradecimento{
Agradeço à Universidade Federal de Santa Catarina (UFSC) e ao Programa de Pós-Graduação em Ciência da Computação (PPGCC) pelo apoio institucional para a realização deste trabalho. Obrigada aos servidores e professores, em especial à Katiana de Castro Silva e ao Prof. Dr. Ronaldo dos Santos Mello, que me trataram com simpatia, respeito e solidariedade. Agradeço também:

Ao meu orientador, Prof. Dr. Antônio Augusto Medeiros Fröhlich (Guto). Muito obrigada pela oportunidade de fazer parte do Laboratório de Integração de Software e Hardware (LISHA) e por me guiar no desenvolvimento deste trabalho. As nossas discussões, trocas de e-mails e conhecimento foram imprescindíveis para o meu crescimento profissional e pessoal.

Aos meus colegas e amigos do LISHA, em especial o Mateus Krepsky Ludwich e o João Gabriel Reis. O apoio e a preocupação de vocês na realização deste trabalho foi fundamental, bem como as nossas discussões técnicas e científicas durante todos estes anos.

À minha família: meus amados pais Célia Regina Cazu Soldi e Luis Carlos Soldi, minha irmãzinha Luiza Helena Cazu Soldi e o meu companheiro para todas as horas, Marcelo Ribeiro Xavier da Silva. Muito obrigada pelo amor incondicional, paciência infinita e incansável dedicação ao meu sucesso e à minha felicidade durante todos os momentos de nossas vidas.
}

\epigrafe{How often have I said to you that when you have eliminated the impossible, whatever remains, however improbable, must be the truth?}{Sir Arthur Conan Doyle}

\textoResumo {O número de sistemas embarcados já supera a quantidade de habitantes do nosso planeta e este número continua crescendo em ritmo acelerado. Ademais, o projeto de \textit{hardware} e \textit{software} está cada vez mais sofisticado e com requisitos mais rígidos para atender o exigente mercado.

As consequências desta sofisticação afetam muito o desenvolvimento de \textit{software} embarcado. Mesmo representando uma parcela minoritária do sistema embarcado, o \textit{software} tornou-se responsável por cerca de 80\% dos erros encontrados nos sistemas~\cite{ebert2009embedded}.

Teste e depuração de \textit{software} não é trivial, uma vez que é necessária a inspeção de todo o código fonte para se certificar de que o comportamento não difere das expectativas. Realizar essas atividades em sistemas embarcados é ainda mais desafiador, uma vez que os desenvolvedores precisam descobrir como otimizar o uso dos recursos, pois o teste em si tende a competir com o aplicativo sob teste pelos escassos recursos do sistema.

Esta dissertação apresenta uma maneira de ajudar os desenvolvedores no processo de testar e depurar sistemas embarcados. Ela apresenta a Automação da execução  de testes de software embarcado ($AUTETESE$), um ambiente que executa os casos de teste e emula as possíveis configurações do sistema, a fim de tentar encontrar erros na aplicação. Uma vez detectado um comportamento não especificado, o ambiente automaticamente executa a compilação, a depuração e a emulação de acordo com um arquivo de especificação.

O $AUTETESE$ é avaliado de maneira quantitativa para os critérios de tentativas realizadas \textit{versus} a configuração de granularidade, o consumo de tempo para realizar o teste e depuração e o consumo de memória para suportar a execução de testes. Adicionalmente, o ambiente é  avaliado de maneira qualitativa em um comparativo com ferramentas e técnicas correlatas. Os resultados mostraram que a estratégia proposta resultou em um ambiente flexível e com grande cobertura dos desafios propostos para atingir a automação de testes de \textit{software}.}

\palavrasChave {Execução de testes automática, testes de \textit{software} embarcado, depuração de \textit{software} embarcado}

\textAbstract {The number of embedded systems already exceed the number of inhabitants of this planet and this number continues to grow. Moreover, the design of hardware and software are increasingly sophisticated and more stringent requirements to meet the demanding market.

The consequences of this sophistication greatly affect the development of the embedded software development. Even representing a minority of the embedded system, the software became responsible for about 80\% of the errors found in the systems~\cite{ebert2009embedded}.

Software testing and debugging is not trivial, once it needs a inspection of the entire source code to make sure that the behavior does not differ from expectations. Perform these activities in embedded systems is even more challenging, since developers need to figure out how to optimize the use of resources because the test itself tends to compete with the application under test for scarce system resources.

This work presents a way to help the developers in the process of testing and debugging embedded systems. It features the automatic execution of embedded software testing ($AUTETESE$), an environment that runs the test cases and emulates the possible system settings in order to try to find errors in the application. Once detected an unspecified performance, the environment automatically performs compilation, emulation and debugging accordingly to the specification file.

$AUTETESE$ is evaluated quantitatively for the criteria of attempts \textit{versus} granularity configuration, the time consumption for testing and debugging, and memory consumption to support the execution of tests. In addition, the environment is evaluated in a qualitative manner in comparison with related tools and techniques. Results show that the proposed strategy resulted in a flexible environment with high coverage to meet challenges posed by automation of software testing.}

\keywords {Automatic testing execution, embedded software testing, embedded system debugging}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Início do documento
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
%--------------------------------------------------------
% Elementos pré-textuais
\capa
%\folhaderosto[comficha] % Se nao quiser imprimir a ficha, é só não usar o parâmetro
%\folhaaprovacao
\paginadedicatoria
\paginaagradecimento
\paginaepigrafe
\paginaresumo
\paginaabstract
\listadefiguras
\listadetabelas
%\listadeabreviaturas
%\listadesimbolos
\sumario

\chapter{Introdução}
\label{cap:introducao}
Sistemas embarcados podem ser apresentados como uma combinação de \textit{hardware} e \textit{software} concebida para realizar uma tarefa específica. Estes sistemas estão amplamente acoplados a inúmeros dispositivos e suas funcionalidades estão cada vez mais intrínsecas no cotidiano das pessoas~\cite{carro2003sistemas}. Atualmente é uma tarefa usual a interação com mais de um sistema embarcado por dia, já que estes estão presentes em carro, trens, aviões, eletrodomésticos, etc. O número destes sistemas já supera o número de habitantes do planeta e este número continua crescendo em ritmo acelerado~\cite{Marcondes:MSC:2009}.

Como em qualquer sistema computacional, as soluções embarcadas também basearam-se na premissa de que cada componente utilizado está efetuando corretamente as suas atividades e que a integração entre os mesmos não se desvia do comportamento esperado. Caso contrário, o sistema pode apresentar erros. 

É comum que a consequência de um erro resulte em perdas financeiras, como por exemplo a perda de uma determinada quota de mercado, corrupção de dados do cliente, etc~\cite{tassey2002economic}. Os danos materiais são inconvenientes, mas colocar em perigo a vida humana é um risco inaceitável. Infelizmente não faltam exemplos de erros em sistemas que resultaram em risco de morte, como a falha do sistema de defesa contra mísseis (Patriot), a ruptura do oleoduto de Bellingham - Washington, ou nos casos de overdose de radiação no tratamento de câncer ocasionadas pelo Therac-25 ~\cite{zhivich2009real}. Também existem vários exemplos relacionados à erros em sistemas embarcados, como por exemplo as tragédias aeroespaciais de Ariane 501, \ac{MCO}, \ac{MPL}, entre outros ~\cite{leveson2009software}. 

Grande parte destas catástrofes possui um relatório descrevendo como ocorreu o acidente, os quais indicaram que a causa mais comum das falhas continua sendo subestimar a complexidade do sistema e superestimar a eficácia dos testes~\cite{leveson2009software, leveson2004role, frola1984system}. Mesmo depois de várias perdas relacionadas a erros em sistemas computacionais, ainda há muita complacência quando um sistema desvia do comportamento esperado, o que acaba subestimando a consequência deste erro. É importante, para garantir que os componentes vão agir de acordo com o requisitado, verificar cada detalhe do sistema sem subestimar sua complexidade.

Durante o planejamento e desenvolvimento dos testes, é necessário lembrar que o teste em si não é um comportamento do sistema e nunca deve interferir no fluxo de atividades que está sendo testado. Quando este teste tem como alvo um sistema embarcado é importante ressaltar que tanto a complexidade do sistema quanto a do teste são maiores, pois estes sistemas costumam ser mais restritos em termos de memória, capacidade processamento, tempo de bateria, prazos para executar uma determinada atividade, entre outros. 

Além da dificuldade da própria atividade de teste, os desenvolvedores ainda precisam se adaptar a uma grande variedade de plataformas, sistemas operacionais, arquitetura, fornecedores, ferramenta de depuração, etc~\cite{schneider2004ten}. 

Existem diversas ferramentas de apoio ao desenvolvimento de \textit{software} embarcado que tentam minimizar o impacto desta diversidade de opções. A escolha e integração destas ferramentas é uma etapa importante na construção de sistemas embarcados e deve ser feita de maneira criteriosa, uma vez que pode simplificar todo o processo de desenvolvimento.

\section{Motivação}
Além da quantidade, a complexidade dos sistemas embarcados é um fator que nunca para de crescer. O projeto de \textit{hardware} e \textit{software} está cada vez mais sofisticado e com requisitos mais rígidos. Para atender a esta demanda, o \textit{software} embarcado deixou de ser composto por um conjunto limitado de instruções \textit{assembly} e incorporou novas funcionalidades, que antes eram oferecidas apenas pelas linguagens de alto nível. 

Mesmo com este acúmulo de atividades, o \textit{software} embarcado ainda representa uma parcela minoritária do sistema~\cite{ebert2009embedded}. Esta crescente complexidade não é acompanhada pela geração de novas ferramentas para o auxílio ao desenvolvimento do \textit{software} embarcado.

Adicionalmente, constatou-se que grande parte das ferramentas disponíveis para automação da execução de testes não são de fácil compreensão/operação e poucas possuem resposta satisfatória para os casos em que os testes encontram erro no \textit{software}. Portanto, como melhora nestes sistemas, é possível adicionar (ou aprimorar o) suporte à depuração e aperfeiçoar os relatórios de falha de funcionamento do \textit{software}. Isto é, simplificar a investigação das causas de erros e \textit{bugs}.

Trabalhos recentes apontam que mais de 80\% dos erros de um sistema embarcado provém do \textit{software}, não do \textit{hardware}, e que tanto o teste quanto a depuração são de suma importância para a qualidade do projeto embarcado~\cite{torri2010evaluation}. Desta forma, a motivação deste trabalho está em diminuir a dificuldade para execução de testes em sistemas embarcados, para que seja possível aproveitar melhor as oportunidades que são oferecidas pela indústria e tecnologia.

\section{Objetivo}
O principal objetivo deste trabalho é propor um ambiente de execução automatizada de testes e depuração de \textit{software} embarcado. Este ambiente deve integrar-se automaticamente com um sistema de depuração de maneira simples e produzir informações que auxiliem na manutenção da qualidade do \textit{software}. O foco desta proposta é que este ambiente seja de simples configuração e que os relatórios gerados sejam de fácil leitura e compreensão.
 
É importante salientar que o presente trabalho não tem como objetivo a geração de casos de teste, limitando-se apenas em executá-los de maneira automatizada. Sendo assim, qualquer erro presente nos testes recebidos pelo protótipo para execução automática serão considerados como erro do próprio \textit{software}.

\subsection{Objetivos Específicos}
Para atender o objetivo geral, os seguintes objetivos específicos devem ser concluídos:
\begin{itemize}
 \item Realizar uma revisão sistemática dos trabalhos relacionados, formando uma base de conhecimento da área de teste e depuração de sistemas embarcados.

 \item Desenvolver um ambiente que realize a automação da execução de testes de maneira simples, sem que seja necessário configurar cada teste a ser executado.

 \item Garantir que este ambiente seja capaz de integrar a execução automática dos testes e a depuração do \textit{software}.

 \item Realizar uma avaliação qualitativa e quantitativa da arquitetura proposta, comparando o presente trabalho com os trabalhos relacionados.
 
 \item Apresentar o trabalho desenvolvido em forma de artigo científico em conferências e periódicos, para que especialistas da área de sistemas embarcados possam corroborar os resultados obtidos e a contribuição técnica/científica.

 \item Utilizar o ambiente proposto em disciplinas de ensino de desenvolvimento de \textit{software} embarcado, para certificar a simplicidade de utilização do ambiente e a efetividade dos relatórios obtidos.
\end{itemize}

\section{Organização do texto}
Esta dissertação está divida em 7 capítulos. A introdução foi apresentada neste capítulo, demonstrando a motivação, os objetivos e as limitações do trabalho proposto. O restante da escrita está organizado da seguinte maneira:
\begin{itemize}

 \item \textbf{Capítulo~\ref{cap:conceitosBasicos} - Conceitos básicos}: apresenta os conceitos fundamentais da área, compondo uma base teórica para auxiliar no desenvolvimento das ideias e na assimilação do ambiente proposto neste trabalho.
 
 \item \textbf{Capítulo~\ref{cap:trabalhosRelacionados} - Trabalhos relacionados}: são apresentados os trabalhos que representam o estado da arte da literatura e que servem de inspiração para o desenvolvimento desta dissertação. Todos os trabalhos aqui apresentados se relacionam diretamente com o tema abordado.
 
  \item \textbf{Capítulo~\ref{sec:tap} - Automação da execução de testes de \textit{software} embarcado}: apresenta os detalhes sobre o modelo de sistema, arquitetura, premissas básicas e
algoritmos propostos neste trabalho.

\item \textbf{Capítulo~\ref{sec:case_study} - Resultados experimentais e análise}: detalha os protótipos utilizados para a validação do ambiente proposto. São apresentados os cenários criados e os testes efetuados, bem como as análises dos dados obtidos.

\item \textbf{Capítulo~\ref{cap:analiseQualitativa} - Análise qualitativa das ferramentas de teste e depuração de \textit{software}}: são apresentadas as metas de desafios para a área de teste e depuração de sistemas embarcados e uma comparação qualitativa entre o ambiente proposto e outras abordagens na literatura.

\item \textbf{Capítulo~\ref{cap:conclusao} - Conclusões}: é o capítulo de desfecho da dissertação e apresenta as conclusões extraídas durante o desenvolvimento deste trabalho, além de alguns pontos de melhoria para trabalhos futuros.

\end{itemize}

\chapter{Conceitos básicos}
\label{cap:conceitosBasicos}
A evolução tecnológica e o crescente uso de sistemas computacionais culminou na produção de sistemas mais sofisticados e complexos, que exigem um projeto sistemático e rigoroso. Porém, o desenvolvimento de sistemas, por mais cauteloso que seja, está sujeito a erros. O teste de \textit{software} é aplicado para tentar eliminar os erros inseridos durante a elaboração de um sistema~\cite{pressman}.

\section{Testabilidade de um sistema}
A testabilidade de um sistema é o grau de facilidade para se estabelecer critérios de teste e para executar os testes, a fim de determinar se os critérios foram atendidos~\cite{ieee610.12:1990}. O conceito de testabilidade de um componente de \textit{software} foi primordialmente introduzido por Freedman~\cite{freedman1991testability} como uma combinação de controlabilidade e observabilidade. 

A controlabilidade e observabilidade interferem diretamente na viabilidade do projeto, da execução e da automação de testes. Além destas duas características iniciais, outras quatro foram elencadas como sugestões de como projetar um \textit{software} para se atingir um alto grau de testabilidade~\cite{bach2003heuristics, 4815358, pressman}:

\begin{description}
\item[Controlabilidade] - capacidade de controlar o processo, conduzir os testes e estados das variáveis. Um sistema é controlável quando é possível, à partir de uma determinada entrada, transferir o sistema de um estado inicial $X_{t_0}$ para um outro estado $X_{t_f}$ durante o intervalo finito de tempo ($t_f - t_0$). 

\item[Observabilidade] - examinar os registros dos estados do sistema, de visualizar todos os elementos que afetam os estados do sistema e de identificar saídas incorretas. Um sistema é observável quando uma determinada condição inicial do estado $X_{t_0}$ pode ter seu comportamento relatado durante o intervalo finito de tempo $t$, onde $t_0 \leq t \geq  t_f$.

\item[Disponibilidade] - facilidade de acesso ao sistema para poder aplicar os testes. Em um sistema disponível deve fornecer acesso ao código fonte e, principalmente, não conter defeitos impeditivos para a execução dos testes.

\item[Operabilidade] - execução do sistema deve ocorrer de maneira simples e clara. Um sistema operável é desenvolvido para atender os requisitos com a maior simplicidade possível, à partir de componentes coesas e com baixo acoplamento.

\item[Estabilidade] - resiliência à mudanças. Um sistema possui estabilidade quando as alterações não são frequentes, e as manutenções necessárias realizadas de maneira controlada e sem invalidar os testes já existentes.

\item[Compreensível] - facilidade de compreensão do sistema. Existem algumas características que podem deixar um sistema mais fácil de ser assimilado. Alguns exemplos são: a escolha de tecnologias adequadas para auxiliar no desenvolvimento do sistema, documentação acessível e detalhada, o uso de padrões de projeto e desenvolvimento, etc.
\end{description}

\section{Teste de \textit{software}}
A primeira conferência com foco em teste de \textit{software} ocorreu há 42 anos, em junho de 1972~\cite{Gelperin:1988}. Desde então há um esforço para definir melhor os conceitos relacionados à área e também para chegar-se a um consenso sobre pontos chave como: nomenclatura, documentação, especificação e execução de testes.

Esta seção traz os principais pontos de discussão e definições na área de teste de \textit{software}.

\subsection{Defeito, erro e falha}
Os termos defeito, erro e falha estão diretamente relacionados ao teste de \textit{software} e suas designações foram estabelecidas pela \ac{IEEE}~\cite{ieee}. Estes termos são semelhantes aos utilizados para designar erros de programação, entretanto, as expressões não são sinônimas entre si e são usadas para expressar conceitos distintos~\cite{koscianski2007qualidade}. Nos escopo deste trabalho os termos ficam estabelecidos - e ilustrados pela Figura~\ref{fig:defeito_erro_falha} - como:

\fig{defeito_erro_falha}{Defeito \textit{vs} erro \textit{vs} falha~\cite{neto2012introduccao}}{scale=.25}

\begin{itemize}
 \item \textbf{Defeito} é caracterizado como um passo, processo, ou definições que são realizadas por um indivíduo de maneira equivocada. O defeito é a causa de um erro, e normalmente ocorre em um nível base, no universo físico, ao abstrair informações, definir um requisito, ao inserir uma linha de código fonte.

 \item \textbf{Erro} é uma manifestação concreta de um defeito num artefato de \textit{software}. Eles ocorrem no universo da informação, e se sobressaem durante a execução do \textit{software}. Caracterizam-se pela divergência entre os resultados obtidos e os esperados de um \textit{software}, pois geram um resultado inesperado na execução de um \textit{software}.

 \item \textbf{Falha} é um desvio da especificação e surge quando existe um processamento incorreto que leva a um comportamento diferente do esperado pelo usuário. Uma falha pode ser considerada com uma deficiência do \textit{software} em cumprir determinados requisitos funcionais devido à ocorrência de um ou mais erros.
\end{itemize}

\subsection{Objetivos do teste de \textit{software}}
A definição com maior aceitação apresenta o teste de \textit{software} como o processo de análise de um item de \textit{software} para detectar as diferenças entre as condições existentes e exigidas (isto é, os erros), e avaliar as características do item de software~\cite{ieee87}. Sendo assim, esta atividade é amplamente utilizada para assegurar que o \textit{software} contempla as funcionalidades especificadas de maneira correta.

Segundo Myers~\cite{Myers:2004:AST:983238}, os objetivos do teste de \textit{software} podem ser expressos a partir da observação de três regras:
\begin{itemize}
 \item A atividade de teste é o processo de executar um programa com a intenção de descobrir um erro.
 \item Um bom caso de teste é aquele que apresenta uma elevada probabilidade de revelar um erro ainda não descoberto.
 \item Um teste bem sucedido é aquele que revela um erro ainda não descoberto.
\end{itemize}

Estas três regras ressaltam o objetivo do teste de \textit{software}, que visa salientar a presença dos erros e não sua ausência. Isto significa que o sucesso de um teste em encontrar um desvio de comportamento em um determinado \textit{software} não garante que todas as possíveis falhas tenham sido encontradas. O teste que não encontra erros é meramente inconclusivo, pois não se sabe se há ou não desvio no comportamento do \textit{software} analisado.

Segundo Pressman~\cite{pressman}, muitas estratégias de testes já foram propostas na literatura e todas fornecem um modelo para o teste com as seguintes características genéricas:
\begin{itemize}
 \item Para executar um teste de maneira eficaz, deve-se fazer revisões técnicas eficazes. Fazendo isso, muitos erros serão eliminados antes do começo do teste.
 \item O teste começa no nível de componente e progride em direção à integração do sistema computacional como um todo.
 \item Diferentes técnicas de teste são apropriadas para diferentes abordagens de engenharia de \textit{software} e em diferentes pontos no tempo.
 \item O teste é feito pelo desenvolvedor do \textit{software} e (para grandes projetos) por um grupo independente de teste.
 \item O teste e a depuração são atividades diferentes, mas depuração deve ser associada a alguma estratégia de teste.
\end{itemize}

Testar um \textit{software} é essencial para revelar o número máximo de erros a partir do menor esforço, ou seja, quanto antes forem detectadas divergências entre o que foi requisitado e o que foi entregue menor será o desperdício de tempo e esforço de retrabalho para adequar o \textit{software}. 

\subsection{Técnicas de teste de \textit{software}}
A escolha da abordagem e da seleção dos casos de testes de \textit{software} podem ser categorizados conforme a fonte de informações em que se baseiam~\cite{pezze2008teste}. Tomando por base estes critérios, os diferentes tipos de testes de \textit{software} podem ser classificados em dois grandes grupos: o dos testes funcionais e o dos testes estruturais~\cite{franzen2005arte}.

\subsubsection{Teste funcional}
O teste funcional, também conhecido como teste de caixa preta ou teste comportamental, é baseado em especificações do \textit{software} e suas entradas são selecionadas à partir de uma análise das funcionalidades do sistema.

Neste tipo de teste a estrutura de controle do sistema é propositalmente desconsiderada e concentra-se os esforços do teste no domínio da informação. Sendo assim, os casos de teste devem ser derivados dos requisitos funcionais do sistema.

Os dados de entrada para os casos de teste podem ser obtidos de maneira aleatória, através de especificações  ou através das funções do \textit{software}, gerando um conjunto de equivalências entre os valores de entrada e os valores de saída. Já a identificação dos erros é feita inserindo um determinado valor de entrada e contrastando o resultado esperado com o resultado obtido.

Existem alguns métodos de teste funcional e dentre os principais estão:
\begin{description}
\item[Método de teste com base em grafo] - consiste em abstrair o sistema como um grafo, onde os nodos são as possíveis entradas e saídas do sistema, e suas conexões expressam algum tipo de ação ou requisito para deslocar-se de um nodo para o outro. Este grafo será organizado em forma de tabelas de causa-efeito e utilizado como base para a geração dos casos de teste.

\item[Particionamento de equivalência] - classifica as entradas do sistema em grupos que tendem a utilizar uma determinada função do \textit{software} e a partir deste grupo são derivados os casos de teste. Estes casos de teste são elaborados para um representante de cada classe ao invés de realizar testes exaustivos com todos os membros. O foco é verificar o máximo de atributos da classe de equivalência ao mesmo tempo.

\item[Análise do valor limite] - investiga a capacidade do sistema em manipular os dados dentro de seu limite. Este método concentra-se em injetar valores que estão nas fronteiras do sistema, onde os erros ocorrem com maior frequência. Este método é bastante utilizado em conjunto com o particionamento de equivalência, extrapolando os valores limites de cada agrupamento.

\item[Matriz ortogonal] - tem como objetivo a construção de casos de teste utilizando-se de uma visualização geométrica do domínio de entradas de um \textit{software}. Com a matriz, as entradas ficam dispersas de maneira uniforme por todo o domínio do teste. Este modelo é utilizado quando a quantidade de dados de entradas, apesar de limitada, é muito grande para realizar testes exaustivos.
\end{description}

Para utilizar-se desta técnica, o ideal seria conter casos de teste que contemplem todas as entradas permitidas para o sistema, entretanto, na maioria dos sistemas isto pode não ser possível~\cite{Myers:2004:AST:983238}.

O teste funcional possui êxito em investigar alguns tipos de erros mas, como o modo de identificação do erro desconsidera o estado interno do sistema, os casos de teste podem não ser suficientes para identificar certos riscos num projeto de \textit{software}. Este tipo de técnica atinge erros nas seguintes categorias~\cite{pressman}:

\begin{itemize}
\item Funções incorretas ou defectivas
\item Erros de interface
\item Erros em estrutura de dados
\item Deficiência no acesso em bases de dados ou serviços externos
\item Comportamento anômalo do sistema
\item Problemas com desempenho
\item Erros na inicialização ou término do \textit{software}
\end{itemize}

\subsubsection{Teste estrutural}
O teste estrutural (ou teste de caixa branca) possui como estratégia a derivação dos dados de teste a partir de uma análise da lógica do \textit{software}~\cite{Myers:2004:AST:983238}. Ou seja, esta técnica baseia-se na estrutura interna do sistema, analisando os dados do \textit{software}, o estado interno dos módulos e o fluxo de controle.

O projeto de casos de teste é dependente da implementação do sistema e utiliza a estrutura de controle descritas como parte do projeto no nível de componentes. Com a técnica de testes estruturais os casos de testes focam nos seguintes aspectos~\cite{pressman}:

\begin{itemize}
\item Garantir que todos os caminhos independentes do sistema sejam exercitados pelo menos uma vez
\item Exercitar todas as decisões lógicas, investigando os estados verdadeiro e falso
\item Executar todos os ciclos dentro de seus limites e nas fronteiras operacionais
\item Analisar as estruturas de dados internos, a fim de assegurar a validade
\end{itemize}

Hetzel indica o teste caixa branca como "teste no pequeno", pois são tipicamente desenvolvidos para pequenos componentes do sistema~\cite{Hetzel:1988:CGS:42384, pressman}. Isto ocorre devido ao nível de detalhismo necessário para desenvolver os testes estruturais. Dentre os principais métodos utilizados para gerar casos de teste estão:

\begin{description}
\item[Caminho básico] - projeto de casos de teste para considerar que todas as instruções do \textit{software} são executadas pelo menos uma vez. O primeiro passo deste método é desenhar um grafo de fluxo representando o \textit{software}, onde os nós representam os processos e as arestas são o fluxo de controle. A partir deste grafo determina-se a complexidade ciclomática e os conjuntos de caminhos básicos, linearmente independentes. Os casos de teste são derivados do conjunto de caminhos básicos de acordo com um critério de cobertura que  irá assegurar que cada declaração foi exercitada, mas isso não significa que todos os resultados da decisão foram executados~\cite{farrell2008manage}.

\item[Estrutura de controle] - projeto de casos de teste derivado de características do fluxo de execução do \textit{software} como, por exemplo, laços, condições, comandos e desvios. Existem algumas variações de testes de estrutura de controle: (i) testes de condição, que verifica as condições lógicas contidas no código fonte do \textit{software}; (ii) teste de fluxo de dados, que elege os caminhos de teste de acordo com o uso de variáveis e dados internos; (iii) teste de laços, que valida a construção dos laços de repetição.
\end{description}

Independente da técnica utilizada, os testes devem ser adotados desde o início do projeto do \textit{software}, considerando a definição dos requisitos a serem testados, a política de testes, os critérios para a conclusão do teste, entre outros. 

Para que o processo de teste alcance seu objetivo é necessária uma estratégia de testes, suficientemente flexível para modelar-se às peculiaridades do \textit{software} e rígida o bastante para um acompanhamento satisfatório do projeto.

\section{Depuração de \textit{software} embarcado}
A depuração é o processo de diagnosticar erros em sistemas e determinar a melhor forma de corrigi-los~\cite{pressman}. Em geral, o ponto de partida para a depuração de um sistema é a ocorrência de algum erro, portanto, é muito comum que a depuração ocorra como consequência de um teste bem sucedido, isto é, de um teste que encontrou erros. 

Erros podem ser encontrados por qualquer pessoa e em qualquer etapa do ciclo de vida do \textit{software}. Isto implica que tanto o formato do relatório de um resultado inesperado, quanto as características dos erros podem variar de acordo com o conhecimento do autor do relato. Sendo assim, para realizar uma depuração eficaz deve-se ser capaz de identificar a técnica apropriada para analisar diferentes tipos de relatórios e obter as informações necessárias para eliminar o problema.

A depuração de \textit{software} embarcado é uma tarefa complexa, cuja dificuldade varia de acordo com o ambiente de desenvolvimento, linguagem de programação, tamanho do sistema e disponibilidade de ferramentas disponíveis para auxiliar o processo. No trabalho de Hopkins e McDonald-Maier~\cite{debugSuport} são definidas as seguintes diretrizes para suporte a uma depuração eficaz:

\begin{itemize}
	\item O suporte à depuração não deve interferir no comportamento do sistema.
	\item Deve existir uma infraestrutura para observar o estado do sistema e de possíveis pontos críticos.
	\item Garantia de acesso para controlar o estado interno do sistema e de seus recursos (incluindo periféricos).
	\item O sistema deve ser minimamente impactado, principalmente quando são necessárias mudanças no \textit{hardware}.
\end{itemize}

O apoio à depuração de \textit{software} normalmente ocorre a partir do monitoramento do \textit{software} compilado e instrumentado. A instrumentação pode ser aplicada e suportada de várias maneiras, dentre as mais frequentes estão a execução do \textit{software} em modo de depuração, o uso de tratadores de interrupções e o uso de instruções de suporte à depuração.

\subsection{Imprimir dados em dispositivos de saída}
Imprimir dados em dispositivos de saída pode ser considerada como uma técnica de instrumentação e depuração, principalmente quando é utilizada com o intuito de exibir/registrar variáveis, coletar informações sobre o estado do sistema ou até o progresso da execução do \textit{software} através do código do programa. 

Esta técnica de depuração é primitiva e intrusiva, pois normalmente está associada à mudanças no código fonte e recompilação do \textit{software}. Dependendo de como utilizada, pode-se retardar a execução do sistema, modificando inclusive requisitos do próprio \ac{SUT}. Portanto, para depurar utilizando a impressão de dados em dispositivos de saída, é necessário que o erro possa ser reproduzido e que ele não se altere com as impressões.

Esta maneira de depuração pode introduzir efeitos colaterais que mascarem o verdadeiro problema. Isto torna ainda mais difícil a identificação de erros relacionados à operações temporais, de paralelismo e de alocação de recursos.


\subsection{Emulador em circuito}
\ac{ICE} é um \textit{hardware} utilizado em depuração de projeto de sistemas embarcados, fornecendo recursos e conectividade que ajudam a superar as limitações de teste e depuração tanto do \textit{software} quanto do \textit{hardware}. 

Este equipamento é específico para cada microprocessador, por isso permite controle total do processador e viabiliza a observação das operações do sistema de maneira consistente e eficaz. A informação coletada por ele fica disponível em tempo de execução e, a princípio, o equipamento extra não exerce nenhum impacto sobre o comportamento do \ac{SUT}. 

As placas \ac{ICE} conseguem integrar o controle de execução do microprocessador, acesso à memória e monitoração em tempo real. Por isso é uma estratégia muito utilizada no início do desenvolvimento dos sistemas embarcados. Entretanto, o aumento da complexidade do sistema pode exigir adaptações no projeto de integração das \ac{ICE}s ao sistema. No caso de sistemas complexos o esforço e o custo para realizar tal adaptação pode ser tão grande que tornam a depuração proibitiva~\cite{debugSuport}.

\subsection{Depuradores}
Depuradores são ferramentas que auxiliam no monitoramento da execução de um programa, incluindo opções como: executar o programa passo a passo, pausar/reiniciar a execução e, em alguns casos, até voltar no tempo e desfazer a execução de uma determinada instrução.

A conexão entre o programa e o depurador pode ocorrer localmente ou remotamente. Ambas possuem vantagens e pontos de melhoria que devem ser considerados na construção de um ambiente de depuração estável.

No método local o programa é executado na mesma máquina que o depurador. Isto implica em um processo com latência menor e com grande influência entre ambos. Por exemplo, se um processo provoca um \textit{crash} no sistema, o depurador perde grande parte das informações de deputação, pois pertence ao mesmo ambiente que sofreu a parada e só poderia formar a hipótese de \textit{crash} porque ele mesmo realizou um comportamento inesperado (travar ou reiniciar).

Já na depuração remota não ocorre este tipo de interferência, uma vez que a aplicação sob depuração e o depurador encontram-se em máquinas separadas. Do ponto de vista do ambiente de depuração, a depuração remota é semelhante a uma depuração local com duas telas conectadas em um único sistema.

\section{Projeto de sistemas orientado a aplicação}
A metodologia de projeto de sistemas orientado a aplicação, a \ac{ADESD}, tem como objetivo principal a produção de sistemas para aplicações de computação dedicada e adaptados para atender exigências específicas da aplicação que irá utilizá-lo~\cite{Froehlich:2001}. 

Esta metodologia permite manter o foco nas aplicações que utilizarão o sistema desde o início do projeto. Desta forma, tanto a arquitetura quanto os componentes podem ser definidos a fim de maximizar a reutilização e a configurabilidade do sistema de acordo com as peculiaridades da sua aplicação. Os principais conceitos envolvidos em sua utilização são:
\begin{description}
	\item[Famílias de abstrações independentes de cenário]  \hfill \\
	Durante a decomposição de domínio as abstrações identificadas são agrupadas em famílias de abstrações e modeladas como membros desta família. As abstrações devem ser modeladas de forma totalmente independente de cenários, garantindo que sejam genéricas o suficiente para que sejam reutilizadas para compor qualquer sistema. Dependências ambientais observadas durante a decomposição de domínio devem ser separadamente modeladas como aspectos de cenário.
	
	\item[Adaptadores de cenário]  \hfill \\
	Utilizados para aplicar os aspectos de forma transparente às abstrações do sistema. Eles funcionam como uma membrana, que envolve uma determinada abstração e se torna um intermediário na comunicação dessa abstração, inserindo as adaptações necessárias para cada requisição que seja dependente de um cenário.
		
	\item[Características Configuráveis]  \hfill \\
	Utilizadas quando uma determinada característica pode ser aplicada a todos os membros de uma família, mas com valores diferentes. Então, ao invés de aumentar a família modelando novos membros, esta característica compartilhada é modelada como configurável. Desta forma é possível definir o comportamento desejado e aplicá-lo às abstrações de forma semelhante aos aspectos de cenário.
	
	\item[Interfaces infladas]  \hfill \\
	Resumem as características de todos os membros da família em um único componente de interface, permitindo que a implementação dos componentes de \textit{software} considerem sempre uma interface bem abrangente e, desta forma, postergando a decisão sobre qual membro da família utilizar. Esta decisão pode ser automatizada através de ferramentas que identificam quais características da família foram utilizadas e selecionam o membro dessa família que implementa o subconjunto da interface inflada.
\end{description}

\fig{aosd}{Visão geral da metodologia de projeto de sistemas orientado à aplicação~\cite{Froehlich:2001}}{scale=.6}

A Figura~\ref{fig:aosd} mostra uma visão geral da \ac{ADESD}, metodologia na qual é possível visualizar a decomposição do domínio em famílias de abstrações independentes de cenários e das dependências ambientais - separadamente modeladas como aspectos de cenário. 

A arquitetura do sistema é capturada pelos \textit{frameworks} dos componentes modelados a partir do domínio. Somente estes componentes serão reunidos para formar o sistema, pois somente eles são necessários para fornecer suporte à aplicação.

\chapter{Trabalhos relacionados}
\label{cap:trabalhosRelacionados}
Desde a formulação da área de testes de \textit{software}, muitos estudos surgiram com as mais variadas soluções para a automação deste processo. Apesar de existir uma vasta literatura de apoio, a maior parte dela relaciona-se à sistemas cujo o propósito geral e, portanto, apenas alguns conceitos podem ser aplicados à sistemas embarcados. 

Os trabalhos que focam na automação de teste de \textit{software} embarcado procuram soluções em que o teste não prejudique a execução do \ac{SUT} e ainda consiga contornar as restrições inerentes ao próprio sistema embarcado.

\section{Justitia}
Esta ferramenta de testes de \textit{software} embarcado é capaz de detectar automaticamente erros na interface, gerar casos de testes completos - com entradas, estados internos, valores de variáveis, saídas - e emular a execução do \textit{software} na arquitetura alvo~\cite{seo}.

$Justitia$ possui a hipótese de que a interface é um critério essencial para um teste de \textit{software} embarcado, e para defendê-la utiliza uma solução que une as seguintes técnicas:
\begin{itemize}
	\item{\textbf{Técnica de teste de interface}},  na qual os casos de teste são gerados através da análise do arquivo (imagem) executável do sistema. Como o próprio nome sugere, a técnica foca nas interfaces do sistema embarcado, em especial as referentes às camadas do sistema operacional e às camadas do \textit{hardware}. Quando teste é executado são extraídas as informações de depuração, tais como símbolos,  relações entre os componentes, informações do \textit{driver} de dispositivo, etc.
	\item{\textbf{Técnica de emulação dos casos de teste}}, uma combinação entre o monitoramento e a depuração do sistema. A ideia é definir \textit{breakpoints} para os casos de teste da interface e monitorar  as variáveis de interface para definir o sucesso ou falha do teste. As variáveis de interface desempenham o papel de indicador para o funcionamento do sistema embarcado,verificando se alguma camada está instável e encapsulando a influência do \textit{hardware} sobre falhas em \textit{software} embarcado
\end{itemize}

A solução foi analisada sob três pontos de vista: a densidade da interface, a complexidade do \textit{software} e a relação interface \textit{versus} falhas. Os pontos de análise surgiram de uma premissa que aponta o forte acoplamento entre \textit{hardware} e \textit{software} como a razão para aumento de dificuldades no teste de \textit{software} embarcado. 

Sendo assim, $Justitia$ apresenta uma análise da densidade da interface, na qual é estabelecida uma relação entre o grau de acoplamento \textit{software} e sua testabilidade. Já a complexidade do \textit{software} é atualmente uma das principais métricas para a previsão de falhas no sistema e no caso de $Justitia$ pode ser extrapolada para estimar a probabilidade de falhas na interface. O último ponto de vista aponta a análise da relação entre interface \textit{versus} falhas como base para os resultados encontrados pela ferramenta.

O primeiro protótipo apresentou resultados satisfatórios para as questões acima e chegou-se a conclusão de que a interface deve ser utilizada como um \textit{checkpoint} para encontrar falhas. A solução foi novamente corroborada por uma versão mais recente~\cite{KiSCL08}, agora aplicada ao teste de um \textit{driver} de um dispositivo embarcado e com a capacidade de suportar um número ainda maior de testes.

\begin{description}
\item[Conclusão]  \hfill \\
Este trabalho ganhou destaque ao apontar que a maior parte das falhas nos testes de \textit{software} embarcados são oriundos da integração de componentes heterogêneos e ao propor que a interface seja um ponto de intersecção entre os testes de \textit{software} e \textit{hardware}.

Outro diferencial encontra-se na infraestrutura de execução dos testes baseada em emulação, na qual tanto o resultado da execução do \ac{SUT} quanto o monitoramento da tabela de símbolos são utilizados para determinar o sucesso ou falha do teste.
\end{description}

\section{ATEMES}
$ATEMES$~\cite{atemes} é uma ferramenta para automação de teste para sistemas embarcados \textit{multicore}. Dentre os tipos de teste suportados estão os aleatórios, de unidade, cobertura, desempenho e condições de corrida. A ferramenta também prevê instrumentação do código, geração de casos de uso e de dados de entrada para sistemas de múltiplos núcleos. 

Para desempenhar todas estas funções $ATEMES$ conta com os seguintes componentes:
\begin{itemize}
	\item{\textbf{PRPM}} - módulo de pré-processamento - cujas atribuições consistem em a análise de código fonte, geração automática de casos de teste, geração automática dos dados de entrada de teste e instrumentação do código fonte. %Suas funções são automáticas, mas também é possível intervir manualmente nos testes através de uma interface.
	
	\item{\textbf{HSATM}} - módulo de teste automático do lado \textit{host} - é responsável por gerar automaticamente testes baseados em uma biblioteca pré-definida, compilar o código fonte para uma determinada arquitetura de \textit{hardware} e enviar a imagem executável para a plataforma alvo.
	
	\item{\textbf{TSATM}} - módulo de teste automático da plataforma de \textit{hardware} alvo - tem como principal função a execução da imagem recebida do $HSATM$, além de monitorar o andamento dos testes e enviar os dados desta execução. 
	
	\item{\textbf{POPM}} - módulo de pós-processamento - que analisa todos os resultados e dados coletados durante o teste.
\end{itemize}

A partir destes componentes é possível executar os testes em uma plataforma alvo a partir de uma estação de trabalho remota, diminuindo restrições quanto ao uso de recursos dos sistemas embarcados. Para tanto, o \textit{software} passa por uma compilação cruzada no lado \textit{host} (estação de trabalho) e é enviado automaticamente para a plataforma alvo onde é executado. 

As atividades são gravadas em um registro, dentre estes registros estão os dados de execução do sistema, o tempo de utilização de cada núcleo da CPU durante a execução dos testes, o resultado de saída, entre outros. O registro pode ser passado para o lado \textit{host} em tempo de execução, onde pode ser armazenado, processado e até apresentado de maneira visual através de uma interface.

\begin{description}
\item[Conclusão]  \hfill \\
Ao explorar sistematicamente a automação de testes de \textit{software} embarcado e componentizar cada operação necessária, $ATEMES$ apresenta uma solução de grande desempenho para sistemas multiprocessados. A ferramenta é tão robusta que é possível inclusive automatizar a geração de casos de testes à partir da análise do código fonte.

A abordagem de execução automática de testes  de $ATEMES$ é similar ao deste presente trabalho. Ambos utilizam depuração cruzada para compilar o código fonte para uma determinada arquitetura de \textit{hardware} e monitoram a execução desta imagem à partir de uma conexão remota. O registro dos dados de execução dos testes e a apresentação de um relatório para o usuário também é uma semelhança notória.
\end{description}

\section{Automação da execução de testes baseada na interface do sistema alvo}
Este trabalho apresenta uma abordagem o testes de sistemas embarcados heterogêneos e baseada nas características do \ac{SUT}~\cite{universal:2013}. 

Diferente da escolha tradicional, que normalmente é feita a partir de uma mistura de vários modelos para tentar alcançar o maior número possível de arquiteturas, a \ac{TBT} reforça que cada sistema embarcado possui características únicas e que devem ser consideradas na escolha do modelo de teste que será aplicado ao sistema.

A \textit{framework} criada utiliza-se de um módulo \ac{DID} que fica conectado diretamente com o sistema embarcado. Este módulo é o responsável por detectar o sistema, descobrir suas características (\textit{drivers} e interfaces) e apontar os testes que melhor se encaixam ao perfil do sistema conectado.  

Entretanto, para que seja possível a extração das informações do sistema alvo e realizar a execução automática dos testes o \ac{SUT} deve ter o projeto de testes baseado em interfaces, ou seja, cada módulo quem compõe o sistema deve implementar uma determinada interface de testes. 

Com estas informações, o mecanismo de teste da \textit{framework} consegue sugerir e executar três tipos de teste. Os testes gerais são sugeridos para sistemas que compõem o sistema embarcado como, por exemplo, capacitores, resistores, \textit{leds}, sensores, etc. Os testes internos verificam cada componente em separado e podem ser realizados com a ajuda de ferramentas que geram algoritmos para sistemas embarcados. Os testes externos verificam o sistema como um todo, contemplando também a integração do sistema.

Em uma nova versão da abordagem~\cite{priyaneural} o mecanismo de teste foi adaptado para considerar um oráculo, composto por uma rede neural de 2 camadas com retroalimentação. O treinamento desta rede neural ocorre a partir de execuções corretas do sistema embarcado até que o oráculo tenha um modelo de simulação do \textit{software} com a precisão desejada. 

Quando novas versões do sistema original forem lançadas, o \textit{software} é novamente executado e sua saída é incorporado através da retroalimentação. O resultado obtido pelo ensaio deve ser comparado com a saída do oráculo para determinar se está correto ou não.

\begin{description}
\item[Conclusão]  \hfill \\
O destaque desta abordagem está na escolha de técnicas para teste de sistemas heterogêneos. A \textit{framework} apresenta uma característica que pode facilitar muito a automação de testes para sistemas embarcados, pois desenvolver testes específicos para um sistema embarcado e que ainda sejam independentes do ambiente de implantação (\textit{deploy}) não é uma tarefa trivial. 
	
A adaptação do mecanismo de testes para o uso de um oráculo composto por uma rede neural é com certeza outro diferencial, tornando possível que a \textit{framework} aprenda com os próprios resultados e consiga generalizar ou especializar uma determinada configuração de testes.

No capítulo~\ref{sec:tap} será possível verificar uma grande semelhança no processo de avaliação dos testes para determinar seu sucesso/falha, principalmente quando utilizada a estratégia de comparação entre os resultados das execuções realizadas.
\end{description}

\section{Partição do \textit{software}}
O conceito de partição do \textit{software} surgiu da afirmação de que quando as pessoas estão depurando um \textit{software}, o processo mais comum é fazer abstrações mentais correspondentes a partes do todo. Desta forma desenvolve-se a hipótese de integrar particionadores de código fonte nos ambientes de depuração~\cite{Weiser:1981}.

Após a definição de Weiser, a partição foi utilizada em diversas áreas da computação, como paralelização, ajuste de níveis de compilação, engenharia reversa,  manutenção de \textit{software}, testes, entre outros. Na área de depuração, focou-se na partição sistemática do \textit{software}, investindo na remoção de estados ou caminhos que são irrelevantes para alcançar o objetivo selecionado, o que no caso da depuração é encontrar um erro~\cite{Xu:2005:BSP:1050849.1050865}. 

Cada trabalho apresenta um critério diferente para particionar, mas todos possuem o conceito de fatia (\textit{slice}) como um subconjunto de predicados do programa que afetam os valores computados, de acordo com o critério de controle. Uma distinção importante para particionar é o tipo de fatia que será realizada: estática, dinâmica ou uma variação delas~\cite{sasirekha2011program}.

A partição estática é o tipo mais rápido e, em contrapartida, aponta apenas uma aproximação do conjunto final de caminhos que podem levar ao erro. Isto ocorre porque o foco é simplificar ao máximo o \textit{software} em questão, reduzindo-o ao ponto de conter todos os estados que poderiam afetar o valor final, mas sem considerar o valor de entrada do \textit{software}. Este tipo de partição é mais utilizada para \textit{software} de pequeno porte e de pouca complexidade, em que o tamanho da partição permanece compatível com a sua simplicidade.

Na partição dinâmica, as informações do critério de corte tradicional não são suficientes, sendo necessária uma informação adicional sobre os valores de entrada do \textit{software}. A partição será realizada a partir destes dados e sequência de valores de entrada no qual o \textit{software} foi executado é determinante para o conjunto de saída. Esta partição é mais utilizada para \textit{software} complexo e de alto grau de acoplamento.

Além dos tipos base de partição, também existem modelos que são variações de um dos dois principais ou até gerados pela união de modelos~\cite{Venkatesh:1991,binkley2005minimal,Sridharan:2007:TS:1250734.1250748,Chebaro:2012}, proporcionando uma maior configurabilidade de depuração e atendendo melhor às características do \ac{SUT}.

\begin{description}
\item[Conclusão]  \hfill \\
A automação da partição do código deriva de uma abstração tradicionalmente utilizada por seres humanos durante a depuração. Desde que foi computacionalmente implementado, surgiram inúmeras variações no critério de partição, tamanho da fatia o programa e alternativas para o cálculo das fatias. A principal razão desta diversidade está na peculiaridade de cada \textit{software}, onde cada característica requer uma maneira diferenciada de realizar a partição.

Independente da abordagem selecionada, a partição do código é uma técnica versátil e muito utilizada, principalmente porque necessita apenas de uma execução com falhas para ser capaz de simplificar o \textit{software} em \textit{slices}, cujo grupo de entradas a serem examinadas é bem reduzido. Poder selecionar pequenos componentes do código fonte para seres testadas e depuradas em partes é uma das principais características da ferramenta apresentada no capítulo~\ref{sec:tap}.
\end{description}

\section{Depuração estatística}
Trabalhos baseados em depuração estatística utilizam-se de dados estatísticos relacionados a várias execuções do sistema para isolar um \textit{bug}. Esta análise reduz o espaço de busca utilizando-se de recursos estatisticamente relacionados ao fracasso, limitando assim o conjunto de dados até chegar a uma seleção em que o erro se faz presente. 

O primeiro passo deve ser a instrumentação do \textit{software} para coletar dados sobre os valores de determinados tipos de predicados em pontos específicos da execução do \textit{software}. Existem três categorias de predicados rastreados:
\begin{itemize}
\item \textbf{Ramificação} - para cada condicional encontrado são gerados dois predicados: um que indica o se o caminho escolhido foi o da condição verdadeira e outro que indica a escolha do caminho caso a condição seja falsa.
\item \textbf{Retorno de funções} - cada retorno de função com um valor escalar são gerados seis tipos de predicados para rastrear o valor: $>0$, $\geq0$, $=0$, $<0$, $\leq0$, $\neq0$.
\item \textbf{Atribuição} - em toda atribuição escalar também são gerados seis tipos de predicados para comparar os valores: $>$, $\geq$, $=$, $<$, $\leq$, $\neq$.
\end{itemize}

Essas informações são armazenadas em relatórios como um vetor de \textit{bits}, com dois \textit{bits} para cada predicado - um para o resultado observado e outro para o resultado verdadeiro e um bit final que representa o sucesso ou falha da execução. Em sequência, são atribuídas pontuações numéricas para identificar qual foi o predicado que melhor expressa o conjunto de predicados.

Como a análise é realizada a partir de uma depuração estática, estes modelos expõem as relações entre comportamentos do \textit{software} e seu eventual sucesso/fracasso de uma maneira estática. Então é possível fornecer uma identificação aproximada de qual parte do sistema gerou o erro.

\subsection{Depuração estatística baseada em amostras}
É uma das primeiras abordagens que propõe recolher os dados estatísticos através de declarações inseridas no código e, em tempo de execução, consegue coletar dicas sobre os dados que não estão relacionados com as falhas~\cite{zheng2003statistical}.

Um dos desafios desta proposta é coletar os dados necessários sem penalizar a execução do \textit{software} e garantindo a melhor utilização de todos os recursos do sistema. A solução encontrada foi inserir um evento aleatório junto às declarações inseridas no \textit{software}, possibilitando que apenas uma pequena parte delas seja executada para cada nova execução do sistema.

A partir desta probabilidade foi possível reduzir tempo gasto para coletar os dados, já que não é obrigatório executar todas as declarações em todas as execuções do sistema. Além disso, as amostras recebidas são agregadas sem a informação de cronologia e considerando apenas a quantidade de vezes em que o resultado das declarações permaneceu o mesmo, o que minimiza o espaço necessário para armazenar os dados.

\subsection{Depuração estatística baseada em predicados}
Este modelo tem o propósito de identificar a origem dos erros em um \textit{software} com uma modelagem probabilística de predicados, considerando inclusive que o \textit{software} pode conter mais de um erro simultaneamente~\cite{zheng2006statistical}.

A manipulação de predicados espelhados pelo código fonte não é um trabalho trivial, e isso faz com que muitos modelos não consigam selecionar padrões de erros úteis. Para mitigar este problema a depuração estatística baseada em predicados utilizada técnicas semelhantes às dos algoritmos \textit{bi-clusters},  que permite a extração de dados bidirecionais simultaneamente.

A implementação da extração de dados bidirecional executa um processo de votação coletiva iterativo, no qual todos os predicados tem um número que define a qualidade de representação. Este número pode ser modificado durante a execução do algoritmo através da distribuição de votos. Cada execução em que ocorre um erro tem direito a um voto para selecionar o predicado que melhor se encaixa na situação.

Esta solução é interessante porque reduz o problema de redundância, pois o processo de votação só acaba quando houver convergência e quanto maior o número de predicados competindo pelo votos, menor é a quantidade de votos que cada um deles pode ter. 

Depois de cada execução um predicado pode receber tanto um voto completo quando uma parcela do voto. No final, os predicados são classificados e selecionados considerando o número de votos que receberam.

%\subsection{Depuração estatística através do perfil da execução}
%\cite{chilimbi2009holmes}

\begin{description}
\item[Conclusão]  \hfill \\
Os trabalhos que utilizam-se de depuração estatística necessitam de um grande volume de dados para realizar uma análise confiável das informações e retornar resultados válidos. Este técnica é de difícil implementação em um sistema embarcado real por não estarem preparados para tal armazenamento. Entretanto, conforme será demonstrado nos capítulos seguintes, é possível coletar e armazenar estes dados no caso da depuração remota.

A depuração estatística baseada em amostras contribui com a discussão sobre a necessidade de ativar todos os possíveis estados do \textit{software} em todas as execuções do sistema para que seja possível descobrir a origem do erro. Sabe-se que ao inserir declarações aleatórias no \textit{software} é possível reduzir a quantidade de dados a serem analisados, visto que apenas uma parte é executada a cada rodada. 

A aleatoriedade pode ser um aliada na depuração do sistema, mas deve-se tomar cuidado com a quantidade de predicados e valores que os mesmos podem atingir. Para eliminar predicados pouco representativos, alternativas como \textit{ranking} e votação foram apresentadas. Levando em conta estes aspectos, o presente trabalho possui no Capítulo~\ref{sec:tap} uma discussão sobre a configuração da do sistema, para que a aleatoriedade não prejudique o resultado desejado.
\end{description}

\section{Depuração por delta}
A técnica de \ac{DD} estabelece uma hipótese sobre o motivo de um \textit{software} parar de funcionar corretamente e, dependendo dos resultados da execução dos testes, esta hipótese deve ser refinada ou rejeitada~\cite{zeller1999yesterday}. A ideia do \ac{DD} é chegar a um mínimo local, onde todos os eventos presentes interferem diretamente no comportamento incoerente do \textit{software}.

Assumindo-se que o erro é determinístico e que pode ser reproduzido automaticamente, é possível refinar a hipótese ao remover de forma iterativa os trechos do \textit{software} que não colaboram na ocorrência do erro. O mesmo ocorre com a rejeição da hipótese, quando um trecho é retirado e o \textit{software} não apresenta mais o erro. 

Para automatizar a escolha, refinamento e rejeição de hipóteses, a \ac{DD} trabalha com dois algoritmos:
\begin{itemize}
\item \textbf{Simplificação} - este algoritmo foca na simplificação da entrada que ocasionou a falha do teste. Para tanto, a entrada é analisada e tenta-se reduzir ao máximo sua complexidade, até que não seja mais possível simplificá-la sem eliminar o fracasso dos testes.
\item \textbf{Isolamento} - algoritmo para encontrar uma configuração na qual a adição ou remoção de um elemento tem influência direta no resultado dos testes. Ele é ideal para encontrar os subconjuntos de um caso de falha.
\end{itemize}

A automação dos testes e da depuração procura determinar as causas do comportamento inadequado a partir da observação as diferenças (deltas) entre versões. Ela pode ser utilizada para simplificar ou isolar causas da falha e pode ser aplicado a qualquer tipo de dado que afete de alguma forma a execução do \ac{SUT}.

Nesta abordagem é comum trabalhar com o teste e a depuração em conjunto, inclusive porque refazer a execução dos testes de um \textit{software} sob novas circunstâncias é uma técnica comum de depuração e, muitas vezes, é a única maneira de provar que as novas circunstâncias realmente podem causar a falha~\cite{zeller2002simplifying}.

\subsection{Depuração por delta com hierarquias}
A técnica genérica de \ac{DD} utiliza os algoritmos de simplificação e isolamento para fornecer um subconjunto onde encontra-se a mudança que possivelmente causou o erro. Porém, em casos mais complexos o \ac{DD} perde um pouco de eficácia e aponta para vários subconjuntos com falha nos testes e cujas entradas não são tão simplificadas.

O HDD - \textit{Hierarchical Delta Debugging}~\cite{Misherghi:2006} surgiu para melhorar o desempenho dos algoritmos do \ac{DD}, principalmente quando o código fonte analisado possui uma estrutura hierárquica e semântica, ou seja, o \ac{SUT} não é apenas uma estrutura de linhas desconexas e independentes. 

A ideia é aproveitar a estrutura hierárquica da entrada para reduzir o número de tentativas inválidas, pois explorando a estrutura dos dados de entrada é possível melhorar a convergência e chegar mais rápido à estrutura simplificada.

Para considera a hierarquia, o algoritmo de simplificação \ac{DD} foi modificado para operar em uma árvore de sintaxe abstrata e sua implementação foi aplicada para avaliar arquivos \ac{XML} e programas escritos em $C$. A versão \ac{XML} explora diretamente a estrutura hierárquica da entrada, extraindo subárvores, folhas, atributos e até caracteres. Para a versão $C$ foi necessário gerar um analisador do código fonte, essencial para manipular o \textit{software} de acordo com a árvore de sintaxe simplificada.

\subsection{Depuração por delta e iterativa}
Para identificar o mínimo local a \ac{DD} necessita de duas versões do \textit{software}: uma falha e uma correta. Contudo, quando um novo erro é descoberto, pode ser que a versão anterior também contenha o mesmo erro e será necessário procurar em versões ainda mais antigas, até encontrar uma versão correta para aplicar o delta. 

A escolha desta de versões foi abordada pelo \ac{IDD}~\cite{artho2011iterative}, o qual automatizou a busca dessa versão correta através de comparações iterativas. O conjuntos de testes também são executados de forma automática para cada iteração e o algoritmo prosseguem até que uma das três condições sejam atingidas:(i) a versão que passa pelos testes é encontrada, (ii) não haja mais versões antigas disponíveis no repositório, ou (iii) foi alcançado o tempo limite da busca.

Durante a comparação, sempre que a execução do \textit{software} não alcança o sucesso porque diverge do esperado (ex. erros de compilação, \textit{looping} infinito, mudanças sintáticas), os algoritmos de \ac{DD} fazem pequenos \textit{patch} para preservar o resultado anterior. No caso de não ser possível aplicar o \textit{patch}, o \ac{IDD} considera o resultado como fracasso.

Para garantir que os algoritmos do \ac{DD} não remova trechos de código que contribuem para o valor do resultado do teste, o \ac{IDD} também automatizou o monitoramento de dados importantes da execução, dos arquivos de log, consumo de memória e o tempo necessário para executar o \textit{software} tanto para ambas versões do delta.

\begin{description}
\item[Conclusão]  \hfill \\
A base da \ac{DD} está em analisar a diferença entre duas versões do sistema e isolar/analisar as possíveis causas do comportamento inesperado automaticamente quando alguma divergência é identificada na execução da nova versão.

Se o delta for bem aplicado, a estrutura que resta aponta apenas para os elementos relevantes no fracasso do teste do \textit{software}. Entretanto, no caso de grandes mudanças, a estrutura restante pode ser complexa e apresentar um grande número de dados para explorar. 

Alguns trabalhos subsequentes concentraram em maneiras mais assertivas de reduzir o conjunto de entradas e isolar o problema. Uma delas é a $HDD$, que utiliza-se de dados referentes à hierarquia do programa para agilizar o trabalho realizado pelo algoritmo de redução. Outra alternativa interessante é o \ac{IDD}, que procura o melhor delta entre várias versões disponíveis no repositório.

A comparação de versões no repositório é uma técnica interessante de depuração e pode ser aplicada inclusive para a geração de relatórios e gráficos sobre a consistência do \textit{software}. No capítulo~\ref{sec:tap} será apresentado um algoritmo quem também se utiliza do delta para determinar sucesso/falha dos testes do \ac{SUT}.
\end{description}

%\subsection{Capturing propagation of infected program states}
%\cite{zhang2009capturing}

\section{Captura e Reprodução}
A ideia da técnica de captura e reprodução consiste em selecionar um subsistema do \textit{software} como alvo do estudo, capturar todas as operações entre este subsistema e o resto do \textit{software} e depois reexecutar as operações isoladas do subsistema~\cite{orso2005selective}.

Este tipo de depuração permite que o desenvolvedor controle a nova execução do \textit{software} com execução de passos a frente, voltando passos na execução (contra o fluxo), examinando o contexto de alguma variável, verificando um determinado controle de fluxo, analisando fluxos alternativos, entre outras possibilidades.

A ferramenta $JINSI$ foi implementada para aplicar os conceitos de captura e reprodução para programas na linguagem Java~\cite{orso2006isolating}. Dado um sistema e um componente presente no sistema, $JINSI$ grava as interações entre esse componente e o resto do sistema. Neste contexto, o componente é considerado como um conjunto de classes com uma função definida.

$JINSI$ foi novamente abordada por Burger e Zeller~\cite{burger2008replaying}, adquirindo a capacidade de capturar e reproduzir as interações inter/intra componentes. Assim, todas as operações relevantes são observadas e executadas passo a passo, considerando-se todas as comunicações entre dois componentes até encontrar o \textit{bug}.

Um grande desafio nesta técnica é como se adaptar a interrupções, pois toda a estrutura de reprodução do \textit{software} se baseia no fluxo de controle e uma interrupção tem a capacidade de romper esta sequência. Afina, interrupções podem ocorrer a qualquer momento e provocar uma ruptura no fluxo de controle e, desta forma, barrar a execução do programa na instrução atual para continuar a partir da rotina de tratamento de interrupção.

Neste aspecto, a solução de tirar um \textit{snapshot} do contexto de execução quando ocorre uma interrupção~\cite{Sundmark} se mostra uma boa alternativa para que o desenvolvedor possa analisar erros oriundos de interrupções.

%\subsection{Locating failure-inducing environment changes}
%\cite{qi2011locating}

\begin{description}
\item[Conclusão]  \hfill \\
Captura e Reprodução sempre foi muito utilizada para testes de interface, em que a interação do usuário com o \textit{software} era armazenada para poder servir como caso de testes. Desta forma torna-se possível realizar testes de funcionalidade do sistema de acordo com o estímulo do usuário.

Já a depuração é um caso mais específico, pois nem todo erro pode ser reproduzido, mesmo que todas as ações sejam reexecutadas na mesma ordem. Contudo, para um erro determinístico em que se tem todas as operações que podem ter causado de um determinado erro, é possível realizar uma investigação baseada no fluxo de controle do \textit{software}.

A ferramenta $JINSI$ foi utilizada para corroborar a técnica, que além de capturar e reproduzir o \textit{software}, agora também incorporou os algoritmos de \ac{DD} para isolar o subconjunto de interações relevantes para simular o erro.
\end{description}

\chapter{Automação da execução de testes de \textit{software} embarcado}
\label{sec:tap}
%qual é a área
%qual é a micro área - emulação
%muitos trabalhos não olham para isso

A área de automação da execução de testes de \textit{software} embarcado tem sido muito pesquisada nas últimas décadas. Em sua maioria~\cite{burger2008replaying,artho2011iterative,universal:2013,priyaneural}, os trabalhos focam em simulação da execução de partes do sistema para atingir os resultados esperados. Contudo, executar os casos de testes simulando cada componente do sistema não é o suficiente para garantir o bom funcionamento do sistema completo, pois muitas vezes é preciso verificar a interação entre eles. 

Para resolver o problema de execução de teste do \textit{software} sem descartar as interações realizadas com o restante do sistema heterogêneo, existe a abordagem de emulação~\cite{atemes,seo}. A emulação completa do sistema oferece emulação de \textit{software}, de \textit{hardware} e de todos os periféricos necessários para a execução do sistema embarcado. Entretanto, muitas dentre as abordagens que utilizam a emulação possuem uma configuração complexa para os dados de entrada, além de operação de difícil compreensão e respostas pouco satisfatórias em relação aos resultados da execução dos casos de teste.

\ac{AUTETESE} utiliza emulação completa do sistema para a execução dos casos de teste, no entanto, foca em simplicidade para a configuração do ambiente, execução dos testes de forma transparente e legibilidade nos resultados obtidos. O ambiente apresentado neste trabalho também explora a emulação do \textit{hardware} para fornecer flexibilidade e portabilidade aos casos de teste, que podem ser executados em mais de uma plataforma alvo. Adicionalmente, integrou-se a emulação completa do sistema à uma ferramenta de depuração, fornecendo um ambiente mais completo para a execução dos testes e depuração de sistemas embarcados.

São detalhados na Seção~\ref{sec:modeloConceitual}  a arquitetura e o modelo conceitual do sistema, com as suposições necessárias para o correto uso e funcionamento do ambiente. Em seguida, na Seção~\ref{sec:implementacaoAutetese} é discutida a implementação do ambiente de \ac{AUTETESE}, considerando as razões para a escolha da abordagem, de cada tecnologia e ferramentas utilizadas.

\section{Modelo conceitual e arquitetura}
\label{sec:modeloConceitual}
\ac{AUTETESE} é um ambiente de execução automática de testes de \textit{software} embarcado. A execução dos testes é realizada com o intuito de verificar se o \textit{software} está em conformidade com sua especificação. Em função disto, assume-se que os dados utilizados como entrada para o ambiente estão corretos. A corretude do modelo de dados pode ser garantida através de mecanismos como, por exemplo, revisões ou inspeções~\cite{pressman}.

O \ac{AUTETESE} possui cinco fases conceituais: (i) importar informações do modelo e casos de teste; (ii) executar os testes de conformidade com a especificação através de testes caixa preta; (iii) executar os testes estruturais se foram encontrados erros na fase anterior; (iv) disponibilizar interação com o ambiente de depuração no caso de sucesso dos testes estruturais; (v) sintetizar os resultados dos testes em um relatório.

A primeira fase tem como objetivo extrair os dados necessários sobre os requisitos do sistema. Eles são utilizados para configurar a execução do \textit{software}, a emulação do \textit{hardware}, os casos de teste do sistema, entre outros. O usuário do ambiente é o responsável pelo preenchimento correto do arquivo de configuração de \ac{AUTETESE}, que deve conter requisitos do sistema e a variedade de valores que este requisito pode assumir. Adicionalmente, são definidas neste arquivo as customizações das funcionalidades do ambiente, como: quantos testes serão executados, se o ambiente executará os testes estruturais, se haverá interação com o usuário para depuração, entre outros.

Vale ressaltar que a variação de requisito no escopo de \ac{AUTETESE} é entendida como o intervalo de valores válidos para determinada configuração, por exemplo: a quantidade de número de núcleos de processamento, tamanho da memória, arquitetura alvo, tipo de escalonamento, ou qualquer particularidade do \textit{software} embarcado.

Durante a fase de execução dos testes de conformidade, é da competência do ambiente proposto executar os testes caixa preta e relatar para quais combinações de requisitos obtiveram êxito e para quais houve falha. Isto significa que se novas funcionalidades forem introduzidas ou se o sistema for alterado, o ambiente acusará se algum requisito deixou de ser atendido. Não está na competência do ambiente a integridade dos testes mas, sim, relatar se o \textit{software} mantém sua execução correta mesmo que haja variação dos requisitos, plataformas, arquiteturas, etc. 

As fases 3 e 4 são opcionais e devem ser customizadas pelo usuário durante a primeira fase. Se o ambiente estiver configurado para executar os testes funcionais (fase 3), o usuário obrigatoriamente deve disponibilizar um arquivo contendo os comandos necessários para a execução destes testes. Uma ferramenta, cujo objetivo original é depuração, teve seu uso adaptado para executar os testes estruturais. Portanto, cabe ao usuário traduzir os passos do teste estrutural para a linguagem do depurador escolhido.

A fase 4 foca em disponibilizar interação com uma ferramenta de depuração, para que seja possível investigar o \textit{software}. \ac{AUTETESE} não contempla a automação da depuração do \textit{software}, apenas sua execução automática. Cabe ao usuário informar no arquivo de configuração da fase 1 se deseja realizar a depuração de forma manual ou com execução automática dos comandos de depuração. Se configurado como intervenção manual, o ambiente disponibiliza acesso para a entrada de comandos de depuração por parte do usuário. Caso contrário, o usuário deve obrigatoriamente configurar o ambiente com um arquivo contendo os comandos necessários para a depuração. 

Em ambos os casos o usuário é o responsável pela geração dos comandos de depuração do \textit{software}, mas existem ferramentas que podem auxiliar na depuração, como a gerarão automática de \textit{breakpoints}~\cite{JSWjsw0803603616} e o controle do fluxo de execução \cite{Chern:2007}. 

A última etapa da exeução do ambiente é responsável por sintetizar as informações obtidas durante a execução das fases anteriores. As informações são entregues em um relatório que contém: resumo quantitativo dos testes executados (total, sucesso e erro), identificação dos testes que evidenciaram o erro, \textit{log} obtido através dos testes estruturais e \textit{trace} da execução do \textit{software}.

Em contraste com o modelo conceitual do ambiente, a arquitetura de \ac{AUTETESE} é mais enxuta e contém apenas quatro fases. A Figura~\ref{fig:autetese} ilustra uma visão macro da arquitetura, dos passos realizados e das saídas de cada fase presente no processo de automação da execução de testes de \textit{software} com \ac{AUTETESE}.

\fig{autetese}{Visão geral da arquitetura de $AUTETESE$}{scale=.09}

O processo é iniciado após o usuário disponibilizar um arquivo \ac{XML} contendo dados relevantes para a execução dos testes da aplicação. Neste arquivo de configuração deve estar discriminado em qual \textit{software} deseja-se realizar os testes, quais as funcionalidades a serem verificadas e os valores que podem assumir. A Seção~\ref{sec:exportarConfiguracoes} detalha melhor todas as questões de preenchimento correto do arquivo \ac{XML}.

O próximo passo ocorre dentro de \ac{AUTETESE}, no qual o ambiente importa os dados do \ac{XML} de configuração e traduz os casos de teste em \textit{scripts} de execução do \textit{software}. A Seção~\ref{sec:importarConfiguracoes} possui mais informações sobre a idealização e implementação do algoritmo de execução de testes.

Os testes funcionais, estruturais e a depuração do \textit{software} são executados em um ambiente composto por ferramentas cuja finalidade inicial é depuração. Estas ferramentas foram adaptadas para auxiliar na execução de testes, como pode ser visto na Seção~\ref{sec:executarTestes}.
A ordem de execução é: testes funcionais, testes estruturais e depuração. As duas últimas etapas são opcionais, de acordo com a configuração inicial. 

Por fim, os resultados da execução dos casos de testes são sintetizados em um relatório apresentado ao usuário. As informações sobre a disposição dos dados e os tipos de resultados apresentados pelo relatório são descritas na Seção~\ref{sec:relatorio}

\subsection{Exportar configurações}
\label{sec:exportarConfiguracoes}
A configuração de Automação da execução de testes de software embarcado \ac{AUTETESE} é feita através de um arquivo \ac{XML}. O uso da linguagem \ac{XML} se deu pela facilidade e legibilidade para definir todas as regras necessárias para executar os testes. O ajuste inicial é manual e simples, uma vez que este arquivo pode ser lido quase como um texto. 

A Figura~\ref{progxml:philosopherxml} traz um exemplo do arquivo de configuração para uma aplicação chamada "$exemplo$". Dentro do arquivo estão descritas duas configurações: a primeira é identificada como "$ARCH$" e pode assumir os valores "$IA32$" ou "$AVR8$", já a segunda propriedade está relacionada com a depuração e é um arquivo que contém \textit{breakpoints} e está no seguinte caminho:\\ "$/home/breakpoints.txt$".

\progxml{philosopherxml}{Exemplo do arquivo de configuração do teste para a $AUTETESE$.}

No arquivo de configuração de \ac{AUTETESE}, cada execução de teste deve iniciar com a \textit{tag} \texttt{<test>}, que identifica que um novo caso de teste será especificado. A partir desta \textit{tag} raiz, é possível acrescentar as opções de ajustes na execução do teste. Elas são:

\begin{description}
	\item[\textbf{$<$application$>$}] - esta \textit{tag}, juntamente com o descritor \texttt{name}, deve ser preenchida com o nome do \textit{software} no qual serão executados os testes. No caso da aplicação da Figura~\ref{progxml:philosopherxml}, a \textit{tag} é a \texttt{<application name=$"$exemplo$"$>}. É possível apontar mais de um \ac{SUT} em um mesmo arquivo, mas cada um deve possuir seus próprios parâmetros de execução. Ou seja, todas as definições de \texttt{<application>} devem ser filhas de uma \textit{tag} \texttt{<test>}.
	
	\item[\textbf{$<$configuration$>$}] - é uma \textit{tag} agrupadora para todas as configurações de um caso de teste. Com esta separação estrutural é possível especificar vários casos de teste para uma mesma aplicação.
	
	\item[\textbf{$<$trait$>$}] - é a \textit{tag} onde define-se o parâmetro do sistema que será analisado. Ela possui o descritor \texttt{id} para armazenar o nome da configuração e o descritor \texttt{scope} para os casos em que o parâmetro deve ser verificado dentro de um escopo específico da aplicação. Por exemplo, o valor de uma fatia de tempo (\texttt{QUANTUM}) pode ser aplicado em vários contextos: \textit{threads}, \textit{CPUs}, etc. Para considerar apenas o contexto de \textit{threads}, expressa-se da seguinte maneira: \texttt{<trait scope=$"$Thread$"$ id=$"$QUANTUM$"$>}
	
	\item[\textbf{$<$min$>$}, \textbf{$<$max$>$}, \textbf{$<$value$>$}] - são \textit{tags} filhas de \texttt{<trait>} e são utilizadas na definição dos valores dos parâmetros. As \textit{tags} \texttt{<min>} e \texttt{<max>} representam, respectivamente, o valor mínimo e máximo que a configuração pode atingir, ou seja, qualquer valor neste intervalo é considerado válido. Já a \textit{tag} \texttt{<value>} identifica um possível valor para a configuração. Caso o usuário deseje discriminar todos os possíveis valores para uma configuração, cada um deles deve ser apresentado em uma nova definição de \texttt{<value>}. Pode-se usar como exemplo a definição de valores para a configuração \texttt{ARCH} da Figura~\ref{progxml:philosopherxml}, descritas como \texttt{<value>IA32</value>} e \texttt{<value>AVR8</value>}.
	
	\item[\textbf{$<$structural$>$}, \textbf{$<$debug$>$}] - são as \textit{tags} que definem onde encontram-se, respectivamente, o arquivo com os testes caixa branca e o arquivo de depuração. O caminho até o arquivo é representado em \texttt{<path>} e deve ser definido com o caminho absoluto do arquivo, ou seja, inicia-se com a referência do diretório raiz ($/$) e a partir dele apresenta uma árvore de diretórios até encontrar o arquivo. No caso da \textit{tag} \texttt{<debug>}, o usuário pode optar pela forma manual, configurando a \textit{tag} sem um arquivo de depuração. A Figura~\ref{progxml:philosopherxml} apresenta um exemplo da \textit{tag} \texttt{<debug>}, com referência para o arquivo \texttt{breakpoints.txt}. 
\end{description}

\ac{AUTETESE} oferece três granularidades de configuração para o teste caixa preta: aleatória, parcialmente aleatória e determinada. Elas são selecionadas de acordo com a quantidade de informações fornecidas pelo usuário sobre os casos de teste e das características do \ac{SUT}. São elas:

\begin{description}
\item[\textbf{Execução aleatória}] - utilizada para testar valores fora do convencional, a fim de verificar a robustez da aplicação. Nela o usuário informa somente os requisitos do sistema através dos arquivo de \textit{traits}. Cabe então ao ambiente inferir os requisitos à partir do arquivo de \textit{traits} e gerar valores válidos, respeitando sua tipagem. A geração destes valores não segue o modelo do \textit{software}, apenas gera valores arbitrários para requisitos aleatórios. \\
Este teste foi desenvolvido como um pior caso para comparações estatísticas, pois o ambiente pode gerar  testes repetidos e fora do limite de modelagem do \textit{software} ocasionando distorções nos resultados obtidos. \\
A execução aleatória possui utilidade quando sabe-se que o \textit{software} possui erros e não existem indícios para descobrir a origem deste erro ao iniciar a depuração. Através deste tipo de execução pode-se encontrar valores errados de configuração e auxiliar os desenvolvedores com pouco conhecimento dos requisitos do \textit{software} a depurar pequenas aplicações.

\item[\textbf{Execução parcialmente aleatória}] - utilizada para executar testes em que apenas algumas informações são fornecidas à priori e para verificar os requisitos do sistema que não possuem um valor determinado, ou seja, mais de um valor pode ser considerado correto. Neste caso, o usuário informa um intervalo de valores válidos para os casos de teste na configuração. \\
Por exemplo, sabe-se que o requisito $X$ pode assumir valores de $1$ a $10$, sendo assim, o ambiente seria configurado da seguinte maneira:
\lstset{language=XML}
\begin{lstlisting}
<trait id="X">	
	<min>1</min>
	<max>10</max>
</trait>
\end{lstlisting}

Se apenas o requisito for informado, cabe ao ambiente inferir os valores dos requisitos à partir do arquivo de \textit{traits}, assim como na execução aleatória. Desta forma, o ambiente não garante que os valores gerados sejam distintos uns dos outros e existe a possibilidade da execução de testes apresentar resultados com falsos negativos.

\item[\textbf{Execução determinada}] - utilizada quando os casos de teste estão bem definidos e todos os requisitos podem ser traduzidos no arquivo de configuração. Se algum caso de teste na execução determinada obtiver valores diferente dos descritos, \ac{AUTETESE} considerará esta execução falha. \\
Por exemplo, para que o requisito $X$ assuma apenas os valores $1$ ou $10$ o ambiente deve ser configurado da seguinte maneira:
\lstset{language=XML}
\begin{lstlisting}
<trait id="X">	
	<value>1</value>
	<value>10</value>
</trait>
\end{lstlisting}

O modo determinado também é interessante quando se deseja otimizar uma configuração, pois uma vez que o comportamento da aplicação e todas suas configurações sejam conhecidas, a única variável do sistema afetará o resultado final. \\
\end{description}


A diferença fundamental entre os três modos de execução dos testes funcionais é a quantidade de regras que são inseridas no arquivo de configuração: o modo aleatório não possui regras, o parcialmente aleatório contém definição de valor ou propriedade a ser considerada, e o modo determinado possui tanto a propriedade quanto os valores a serem trocados.

\ac{AUTETESE} também suporta a execução de testes estruturais e depuração de maneira primária. Os testes de caixa branca são importados através do arquivo de configuração, que contém uma sequência de comandos que representam os casos de teste. Para a depuração, o usuário pode optar pela forma manual ou execução automatizada. Na forma manual, \ac{AUTETESE} fornece ao usuário a interação com o depurador, na qual será possível inserir os comandos desejados. Já na depuração automatizada, o sistema é configurado com um arquivo que contém uma sequência de passos de depuração. Um exemplo deste tipo de arquivo utilizando a ferramenta \ac{GDB} é ilustado na Figura~\ref{fig:breakpoint}.

\fig{breakpoint}{Exemplo do arquivo de breakpoint.}{scale=.3}

A sequência de passos inicia-se ao ativar o \textit{log} do \ac{GDB} ($set$ $logging$ $on$) para poder salvar todos os passos da depuração e poder analisá-los posteriormente, caso necessário. Com esta opção ativa no \textit{script} de depuração os passos executados no \ac{GDB} também serão anexados ao relatório final de \ac{AUTETESE}. O comando $break$ adiciona \textit{breakpoints} nas funções desejadas, neste exemplo é adicionado na função $main$. O comando $continue$ é um sinal de liberação da execução do sistema para iniciar a depuração.

\subsection{Interpretar dados de entrada}
\label{sec:importarConfiguracoes}
Na fase de interpretar os dados de entrada ocorre o processamento das informações e a transformação dos dados em ações a serem executadas de maneira automática. Para realizar este processo \ac{AUTETESE} utiliza-se de um algoritmo que extrai as informações necessárias para a realização dos testes e realiza a troca de valores de configuração.
 
O algoritmo de troca de parâmetros, conforme ilustrado na Figura~\ref{fig:script}, verifica as configurações disponíveis no arquivo \ac{XML} e seleciona as trocas a serem realizadas, de acordo com a especificação. O valor da propriedade é modificado diretamente no arquivo de configuração da aplicação (\textit{traits}) e é compilado e executado em cada nova troca de parâmetros.

\fig{script}{Algoritmo para troca de valores de configuração do \textit{software}}{scale=.15}

A troca de parâmetros é realizada a partir de uma combinação entre as propriedades e seus possíveis valores, de acordo com a fórmula de combinação matemática: $C^n_s={n\choose s} =\frac{n!}{s!\cdot\left(n-s\right)!}\,\!$. Sendo $n$ a quantidade de formas distintas em que é possível escolher os elementos de um conjunto de entrada, e $n$ é o tamanho do conjunto de entradas. 

No caso de Automação da execução  de testes de software embarcado ($AUTETESE$), o tamanho do conjunto de entradas varia de acordo com o arquivo de configuração do ambiente. Por exemplo, quando a configuração aponta para uma execução determinada, o número de entradas é igual à quantidade de \textit{tags} $<value>$ da configuração.

O tipo de execução de testes presente no \ac{XML} possui grande influência na troca dos parâmetros. No caso de uma execução aleatória, em que apenas o arquivo \textit{traits} é informado, o algoritmo deve selecionar tanto uma configuração quanto um valor para realizar a troca. Já em casos de execução determinada, o algoritmo limita-se a realizar as trocas propostas. Na execução parcialmente aleatória ambas as condições estão presentes, sendo assim, este tipo de execução será utilizada para exemplificar a troca de parâmetros.

Suponha a execução de testes para a aplicação exemplo que contém as propriedades: (i) \texttt{Escalonador}, que assume valores fixos; e (ii) \texttt{NumeroThreads}, cujo valor é variável. A Figura~\ref{fig:resultado_script} apresenta um exemplo do arquivo de configuração \ac{XML} e das aplicações originadas à partir da troca de parâmetros.

\fig{resultado_script}{Configuração de $AUTETESE$ e os resultados das trocas de valores de configuração da aplicação \texttt{Exemplo}}{scale=.095}

Para a aplicação \texttt{Exemplo}, a combinação realizada foi:\\
 $C^2_1 \cdot C^4_1 =\frac{2!}{1!\cdot\left(2-1\right)!}\,\! \cdot \frac{4!}{1!\cdot\left(4-1\right)!}\,\! = 8$
 
O conjunto de resultados obtidos é referente à combinação dos dois possíveis valores para \texttt{Escalonador} com a combinação dos quatro valores para propriedade \texttt{NumeroThreads}, resultando em $8$ variações de configuração, que serão compiladas e executadas posteriormente. 

\subsection{Executar testes e depuração}
\label{sec:executarTestes}
\ac{AUTETESE} possui suporte para executar tanto os testes funcionais, quanto os estruturais. Adicionalmente, se ambos os testes apresentarem falha na execução, inicia-se a execução da depuração. A Figura~\ref{fig:fluxo_execucao_testes_depuracao_svg} ilustra o fluxo das atividades de teste. 

\fig{fluxo_execucao_testes_depuracao_svg}{Fluxo de execução dos testes e da depuração da aplicação com $AUTETESE$}{scale=.3}

A execução dos testes funcionais é realizada automaticamente após obter as aplicações compiladas pelo algoritmo de troca de parâmetros. Já a execução dos testes estruturais ocorre somente em caso de falha na execução dos testes funcionais e se houver sido explicitada a 	configuração no arquivo \ac{XML} do ambiente. 

A depuração da aplicação ocorre após as execuções de ambos os teste e, somente se, resultarem em falha. Este processo é opcional e pode ser configurado no \ac{XML} como manual ou automático. Na depuração manual, o ambiente fornece uma entrada de dados para que o usuário possa depurar a aplicação. Para a depuração automática, o arquivo \ac{XML} deve conter o caminho para um outro arquivo que contém um roteiro da depuração e \ac{AUTETESE} apenas executará os comandos que estiverem neste roteiro.

O projeto do ambiente para que testes e depuração compartilhem uma infraestrutura iniciou-se a partir da observação e estudo de vários ambientes de execução de ambas atividades em separado. O desenvolvimento de \ac{AUTETESE} focou no suporte para sistemas embarcados, pois estes sistemas geralmente possuem apenas os recursos essenciais para desempenhar sua função e as atividades de teste e depuração compartilham os escassos recursos do \ac{SUT}.

A execução dos testes e depuração em \ac{AUTETESE} é realizada utilizando-se um emulador e um depurador. A emulação do sistema possibilita a visualização do estado do \textit{software} e proporciona maior liberdade para controlar a execução da aplicação. Sua função principal para o ambiente é auxiliar o teste do \textit{software} embarcado sem o \textit{hardware} físico. 

A função do depurador é de auxiliar em encontrar os erros do \textit{software}, facilitando sua correção. Eventualmente, o ambiente de depuração também será utilizado como ferramenta auxiliar para os testes de caixa branca, visto que sua conexão com o emulador fornece dados atualizados sobre o sistema em execução.

\subsection{Sintetizar os dados}
\label{sec:relatorio}
A síntese dos dados e emissão do relatório para o usuário ocorre após a execução dos casos de teste e da depuração do sistema. Este documento apresenta diferentes configurações que dependem do tipo de execução dos testes. A Figura~\ref{fig:tipos_de_relatorio} apresenta um exemplo dos tipos de relatórios que podem ser gerados por \ac{AUTETESE}.

\fig{tipos_de_relatorio}{Exemplo de apresentação do relatório conforme tipo de execução dos testes.}{scale=.2}

No caso de uma execução determinada, o usuário sabe quais configurações foram trocadas e por quais valores. Desta forma, o relatório não precisa explicitar todas as trocas realizadas, apresentando apenas um resumo da execução. 

Para a execução parcialmente aleatória sabe-se a configuração que será trocada, entretanto, não conhece os valores, uma vez que estes foram gerados por \ac{AUTETESE}. Sendo assim, no relatório são informados todos os valores utilizados nas trocas. 

No caso do relatório para uma execução aleatória, devem ser descritas todas as configurações que foram trocadas pelo ambiente, juntamente com seus respectivos valores e o resultado da execução.

Todos os relatórios possuem um resumo da quantidade de testes executados, quantos obtiveram sucesso e quantos falharam. Um arquivo de \textit{log} de execução é armazenado para cada falha da aplicação na execução dos testes. Este \textit{log} fica anexado ao relatório e contém os dados específicos sobre a compilação, execução dos testes (funcionais e estruturais) e depuração.
 
\section{Implementação do ambiente AUTETESE}
\label{sec:implementacaoAutetese}
O ambiente \ac{AUTETESE} realiza a automação da execução dos casos de teste através do algoritmo de \ac{TAP}, que define diversas configurações do sistema. Estas configurações são compiladas e executadas em um ambiente específico para testes e depuração. O resultado das execuções é disponibilizado em forma de relatório.

Para viabilizar o ambiente compartilhado \ac{AUTETESE} explora as funcionalidades de ferramentas já consolidadas com foco em emulação e depuração. A contribuição de \ac{AUTETESE} está justamente em integrar estas ferramentas, configurando apenas um ambiente para executar as atividades de teste funcional, teste estrutural e depuração, sem ter que reestruturar o ambiente sempre que um novo requisito é adicionado ao \ac{SUT}. Foi adicionado a este ambiente um comportamento cujo intuito é traduzir os dados de saída das ferramentas em um relatório com informações relevantes para investigação de erros. 

Além de utilizar ferramentas já consolidadas, também foi desenvolvida \ac{TAP}, uma nova ferramenta que executa a troca de parâmetros do sistema. Sua principal contribuição é a possibilidade de reutilizar casos de teste. Uma das funções da reutilização de testes ocorre quando os sistemas embarcados possuem um núcleo e cada novo \textit{software} é uma extensão deste núcleo. Ou seja, o mesmo caso de teste pode ser utilizado em mais de um \textit{software}, diferenciando apenas as propriedades para as novas extensões.

O algoritmo de \ac{TAP} foi implementado utilizando-se o sistema operacional \textit{Linux}. As trocas são realizadas através de um \textit{script} que contém comandos presentes no sistema operacional como, por exemplo: \texttt{make},\texttt{grep}, \texttt{sed}, \texttt{cp} \texttt{screen}, etc. Todas as trocas consideram o arquivo de configuração do ambiente (\ac{XML}) e os \textit{traits} da aplicação.

Nas subseções que se seguem, será explicado em detalhes como estas ferramentas se integram, como o algoritmo foi implementado e o porquê das decisões da abordagem e ferramentas escolhidas. 

\subsection{Ambiente compartilhado de testes e depuração}
\label{sec:ambienteCompartilhado}
Existem inúmeras alternativas para realizar a atividade de teste de \textit{software} embarcado, como por exemplo: executar o \textit{software} em um simulador, testar o sistema com um protótipo limitado do \textit{hardware}, emular o sistema, ou até utilizando o próprio ambiente de desenvolvimento~\cite{progressBeforeHardware}.

Neste trabalho foi escolhida a abordagem de emulação do sistema, pois ela obtém sucesso em executar os testes e a depuração, mesmo em casos nos quais existe dificuldade de acesso ao \textit{hardware} e limitação de recursos. Um exemplo deste tipo de sistema é a \ac{RSSF}, que é composta por vários sensores  geograficamente espalhados por uma grande área de monitoramento e, geralmente, possuem pouco poder de processamento e apenas alguns \textit{kilobytes} de armazenamento~\cite{pottie2000wireless, margi2009segurancca, dantas2010desro}.

No caso de muitas \ac{RSSF} é impraticável coletar todos os sensores para realizar testes, pois além da dificuldade de acesso às áreas em que os sensores atuam, as atividades de monitoramento teriam que sofrer uma pausa até o término do teste. A solução mais comum nestes casos é realizar os testes de \textit{software} e \textit{hardware} separadamente, utilizando-se simulação.

Contudo, simular separadamente as componentes não garante o funcionamento de sua integração. Em sistemas embarcados é comum que \textit{software} e \textit{hardware} trabalhem em conjunto para desempenhar uma tarefa. Aliás, grande parte das falhas nos testes de um \textit{software} embarcado são oriundos da integração de componentes heterogêneos~\cite{seo}. Isto não acontece na emulação do sistema, pois ela possibilita a visualização do estado do \textit{software} e permite o controle de sua execução~\cite{KiSCL08}. Desta maneira, é possível analisar a intersecção entre componentes e monitorar a execução dos testes através da tabela de símbolos.

O processo de teste para sistemas heterogêneos é uma tarefa trabalhosa, principalmente quando não se sabe qual ambiente de implantação será utilizado~\cite{universal:2013}. Existem várias alternativas para ajudar na tarefa e aprimorar a qualidade dos testes, como utilizar oráculo como um repositório de testes~\cite{priyaneural}, selecionar \textit{checkpoints} baseados na interfaces dos componentes~\cite{KiSCL08}, módulos de pré-processamento com análise de código fonte~\cite{atemes}, entre outros.

Sabe-se que o processo de depuração também precisa de ferramentas específicas para auxiliar no monitoramento da execução de um programa. O sucesso das técnicas que focam em automação da depuração não dependem apenas dos algoritmos ou dados coletados, mas também da estabilidade fornecida pelo ambiente para desempenhar tais atividades. Por exemplo, nas ferramentas de captura e reprodução~\cite{orso2005selective, orso2006isolating, qi2011locating} é essencial que a etapa de captura grave todas as operações que levam ao erro.

\subsubsection{Detalhes da implementação}
\label{ref:configAmbiente}
As ferramentas utilizadas para a construção deste ambiente remoto podem ser substituídas por qualquer outra equivalente, contudo, a configuração completa será tecnicamente apresentada para que seja possível a reprodução do experimento.

O ambiente compartilhado proposto utiliza o \ac{QEMU} para emular a máquina com a aplicação alvo a partir de outra máquina, usando tradução dinâmica. Desta forma torna-se possível utilizar-se um computador pessoal para testar aplicações compiladas para algum outro sistema embarcado. O \ac{GDB} encaixou-se no papel de depurador, pois nele é possível especificar qualquer regra que possa afetar o comportamento do \ac{SUT} de maneira estática.

A integração da execução dos testes e da depuração é particular para cada máquina hospedeira e alvo. Portanto, talvez alguns passos aqui apresentados devam ser adaptados para a arquitetura alvo. 

A Figura~\ref{fig:emulator_debugger_gray} apresenta as atividades necessárias para executar a depuração remota em conjunto com a emulação dos testes.

\fig{emulator_debugger_gray}{Atividades de integração entre emulador e depurador}{scale=.25}

Uma explicação adicional das técnicas e ferramentas utilizadas neste processo estão listados abaixo:

\begin{description}
\item[1. Compilar com informações de depuração] \hfill \\
Este passo realiza uma compilação simbólica e sem otimizações. A otimização correta é importante para a execução do \textit{software}, mas quando o foco é a depuração o ideal é que não haja muita diferença entre o código fonte e as instruções geradas.

Como entrada é necessário o código fonte do \textit{software} e como saída esperada encontra-se o código compilado com informações de depuração. O \ac{GCC} realiza esta atividade quando acrescentada a opção $-g$ para compilar. 
	
\item [2. Executar o sistema utilizando um emulador] \hfill \\
É um passo necessário para executar a \textit{software} na arquitetura alvo correta. É importante ressaltar que o \textit{software} a ser emulado não deve iniciar a execução antes que o depurador esteja conectado. Caso isso ocorra, não será possível inspecionar todos os detalhes da execução.

Para executar este passo utilizamos o \ac{QEMU}, que deve ser inicializado com os argumentos $-s -S$. A primeira opção ativa o \textit{stub} para conectar um depurador, a fim de abrir a comunicação entre o emulador e o depurador. A opção $-S$ é utilizada para forçar que o \ac{QEMU} espere a conexão do depurador antes da inicialização do sistema.
Por exemplo, se uma aplicação compilada com informações de depuração ($app.img$) imprime algo na tela ($stdio$), a chamada do \ac{QEMU} deve ser semelhante à: $qemu-[arch]$ $-serial$ $stdio$ $-fda$ $app.img$ $-s$ $-S$
	
\item [3. Conectar-se com o depurador] \hfill \\
Esta conexão é importante para fornecer ações ao usuário, tais como: executar o programa passo a passo, pausar/reiniciar a execução, entre outras. Para fornecer a estabilidade necessária para a infraestrutura gerada, a conexão do depurador será realizada de maneira remota.

Para que a depuração seja remota a sessão do depurador deve ser inicializada em um ambiente separado. Então, para se conectar ao depurador ao \ac{QEMU} o desenvolvedor deve explicitar que o alvo a ser examinado é remoto e informar o endereço da máquina alvo e porta em que se encontra o alvo a ser examinado. Utilizando-se o \ac{GDB}, a tela será iniciada a partir do comando: $target$ $remote$ $[endereço\_alvo]:[porta\_alvo]$

\item [4. Recuperar as informações de depuração] \hfill \\
O arquivo gerado no primeiro passo   contém todas as informações que podem ser retiradas da compilação, como por exemplo o endereço de uma variável, ou os nomes contidos na tabela de símbolos. Então este passo é importante para ajudar os desenvolvedores a encontrarem erros.

O arquivo usado para manter as informações de depuração deverá ser informado para o \ac{GDB} usando o comando: $file$ $[caminho\_do\_arquivo]$
	
\item [5. Encontrar origem dos erros] \hfill \\
Encontrar e corrigir erros é uma atividade depende do programa a ser depurado. A partir desta etapa, o desenvolvedor pode definir \textit{breakpoints}, \textit{watchpoints}, controlar a execução do programa e até mesmo permitir \textit{logs}. 

Existem vários trabalhos com o foco em ajudar a encontrar os erros através da automação de alguns pontos, como a gerarão automática de \textit{breakpoints}~\cite{JSWjsw0803603616} e o controle do fluxo de execução \cite{Chern:2007}.
\end{description}

Após a definição da infraestrutura compartilhada para testes e depuração também é necessário dividir as funções para cada uma das atividades. A ideia é utilizar o emulador para a automação da execução do \textit{software} e de seus casos de teste. A depuração será iniciadas somente no caso de sucesso em um teste, ou seja, quando um erro é detectado.

\subsubsection{Considerações sobre o uso do ambiente compartilhado}
Muitas vezes as configurações do \ac{SUT} devem ser alteradas para poder executar um tipo específico de teste ou melhorar a eficácia da depuração. Estas adaptações podem influenciar no funcionamento normal do \ac{SUT} e ocultar determinados tipos de erros.

A presente proposta de ambiente compartilhado não é uma exceção e, inclusive, um dos primeiros passos para utilizar o ambiente de maneira adequada está em executar uma compilação simbólica do \textit{software} e sem otimizações.

Para um compilador, a diferença entre utilizar ou não alguma otimização ativa é o foco da compilação do \textit{software}. Ao utilizar uma compilação sem otimização, o objetivo do compilador será de reduzir o custo da compilação e da depuração a fim de sempre produzir os resultados esperados. Ao ativar alguma otimização sua função será compilar com o intuito de melhorar o desempenho e/ou o tamanho do código à custa do tempo de compilação e, possivelmente, da capacidade para depurar o programa~\cite{gccOptimization}.

Uma compilação sem otimizações deixa o \textit{software} mais preparado para extrair informações valiosas na depuração. Contudo, além de aumentar a quantidade de memória necessária, o fato de retirar as otimizações da compilação pode ocultar erros e distorcer as condições de corrida de um \textit{software}.

Estas alterações podem, em alguns casos, gerar erros que provavelmente não existiriam em uma compilação com as otimizações. Isto implica em falsos positivos que podem a vir ser apontados por \ac{TAP}. Porém, os relatórios gerados por \ac{TAP}, independente de falsos positivos, necessitam de interpretação. Isto é, o ambiente não substitui o desenvolvedor, apenas auxilia no desenvolvimento, e cabe ao desenvolvedor considerar ou não mudanças de acordo com o relatório. Em suma, é uma questão de hermenêutica.

As otimizações utilizadas por um \ac{SUT} podem definir o sucesso do teste e da depuração e devem ser muito bem avaliadas para não gerarem resultados falsos. Para corroborar este fato, existem trabalhos que discutem como as otimizações podem influenciar \textit{softwares} com múltiplas \textit{threads}~\cite{jia2013compiler} e sugestões~\cite{effinger2012ifrit, 5544315, 6821096} para o teste e depuração de \textit{software} com o mínimo de influência no sistema, com foco em não ocultar erros de simultaneidade.

\subsection{Troca automática de parâmetros de \textit{software}}
\label{sec:algoritmoTrocaParametros}
Existe uma demanda crescente de desenvolvimento de novos sistemas ou adaptações em \textit{softwares} já existentes. Novos requisitos de \textit{software} e \textit{hardware} surgem diariamente e estão cada vez mais complexos, fazendo com que os sistemas tenham que ser adaptados para que atendam as necessidades dos usuários~\cite{pressman}.

Para que o esforço de desenvolver (ou manter) um \textit{software} seja o mínimo possível, é essencial que seu código fonte seja bem projetado e estruturado, facilitando a adição e adaptação de funcionalidades~\cite{kerievsky2008refatoraccao}. 

No desenvolvimento para sistemas embarcados é comum que uma especificação seja reutilizada, diferenciando apenas alguns requisitos não funcionais. Esta estratégia visa utilizar a propriedade intelectual já adquirida para reduzir o tempo para que um novo produto entre no mercado.%~\cite{gupta1997introducing}.

Normalmente o núcleo do sistema é formado por componentes previamente concebidos e verificados, que posteriormente são conectados à extensões~\cite{743146}. Estas extensões são as adaptações e novas implementações de componentes de \textit{software} ou \textit{hardware} que atendem a um determinado requisito. Para cada nova extensão, um novo conjunto de testes é gerado para garantir o novo sistema.

Além do reuso, o desenvolvimento deste tipo de sistemas costuma ter o baixo consumo de recursos como um requisito essencial. Logo, meios que possibilitam tanto o reuso de código quanto a menor sobrecarga possível sobre o sistema, são desejáveis~\cite{1704131}. O teste de um sistema embarcado deve seguir a mesma linha de raciocínio. Para tanto, \ac{AUTETESE} procura atender estes requisitos ao extrapolar os conceitos da metodologia de projeto orientado aplicação e da técnica de abstração de dados para o universo dos testes. 

Em um projeto orientado à aplicação o sistema é desenvolvido a partir de componentes especificamente adaptados e configurados de acordo com os requisitos da aplicação alvo. Ou seja, o próprio projeto do sistema oferece a opção de conter somente os componentes essenciais ao funcionamento da aplicação.

Os conceitos de \ac{ADESD} na concepção do \textit{software} podem reduzir o tempo gasto com o teste e a depuração do sistema, pois a partir deste tipo de projeto podemos considerar que a entrada do sistema já está simplificada, sendo equivalente à técnicas de depuração delta. Desta forma, não é necessário utilizar técnicas como, por exemplo, partições do código para diminuir a complexidade do sistema. 

Ademais, no desenvolvimento de \textit{software} é importante que cada funcionalidade significativa seja implementada em apenas um lugar no código fonte. Caso existam funções semelhantes espalhadas pelo código, é interessante e benéfico combiná-las em uma única abstração e derivar suas peculiaridades em funções diferentes~\cite{abelson1996structure}. 

O reuso do projeto do sistema embarcado permite que todo o sistema seja reaproveitado, exigindo apenas um conjunto de testes complementar para cada extensão anexada. A abstração de casos de teste deve ser composta por testes da própria abstração e testes relacionados ao comportamento específico de cada requisito não funcional. 

O algoritmo de troca de parâmetros de configuração do sistema foi desenvolvido para aproveitar estas vantagens. Assim, cada \ac{SUT} que possua um conjunto de requisitos que possam ser refinadas de acordo com o propósito da aplicação pode utilizar \ac{TAP} para trocar automaticamente as configurações por uma abstração equivalente.

\subsubsection{Algoritmo de troca de parâmetros}
\ac{TAP} foi idealizado de acordo com os conceitos de abstrações de dados e com foco no desenvolvimento e teste de sistemas embarcados.

A ideia é poder aplicar um único conjunto de testes para todas as implementações que compartilham uma mesma especificação base, especializando apenas a configuração desejada. Desta forma, é possível executar o mesmo conjunto de testes para atender uma maior variabilidade de configurações de um componente de \textit{software} ou \textit{hardware}.

No Algoritmo~\ref{algoritmo_AEP} são apresentados os passos realizados a partir do momento em que se tem um \ac{SUT} até o retorno do relatório para o usuário. A entrada do algoritmo é o arquivo de configuração e a partir das especificações contidas nele, o algoritmo flui no sentido de tentar encontrar a característica desejada, trocá-la por um valor predeterminado, executar a nova aplicação e recolher o retorno da aplicação.

\begin{algorithm}
\caption{Algoritmo de troca dos parâmetros de configuração}\label{algoritmo_AEP}
\KwIn{arquivo de configuração do ambiente}
\KwOut{relat\'{o}rio com resultado da execução dos testes funcionais}

propriedades $\Leftarrow$ pegarPropriedadeDoArquivo(arquivo)\;
\eIf{ o arquivo possui valor para configura\c{c}\~{a}o }{
  \ForEach{configura\c{c}\~{a}o no arquivo}{
             linha $\Leftarrow$ pegarConfiguracao(configura\c{c}\~{a}o, propriedades)\;
     \ForEach{valor para configura\c{c}\~{a}o}{
        novaPropriedade$\Leftarrow$ modificarValorPropriedade(linha, propriedades) \;
        novaAplicacao$\Leftarrow$ compilar(aplica\c{c}\~{a}o, novaPropriedade) \;
        relat\'{o}rio$\Leftarrow$ relat\'{o}rio + emular(novaAplicacao) \;
     }
   }
}{
\eIf{ o arquivo possui n\'{u}mero m\'{a}ximo de tentativas}{
        numMaxTentativas$\Leftarrow$ pegarTamanhoMaximo(arquivo)\;
}{
        numMaxTentativas$\Leftarrow$ gerarValorAleatorio()\;
  }
\While{tentativas $<$ numMaxTentativas}{
    linha$\Leftarrow$ gerarValorAleatorio()\;
	novaPropriedade$\Leftarrow$ modificarValorPropriedade(linha, propriedades)\;
    novaAplicacao$\Leftarrow$ compilar(aplica\c{c}\~{a}o, novaPropriedade)\;
    relat\'{o}rio$\Leftarrow$ relat\'{o}rio + emular(novaAplicacao)\;
  }
}
\Return relat\'{o}rio;
\end{algorithm}

Vale lembrar que o algoritmo não leva em conta a semântica da aplicação nas trocas realizadas. Se o aquivo de configuração não trouxer sugestões de propriedades a serem modificadas e valores cuja semântica é adequada, a ferramenta apenas selecionará valores estruturalmente válidos.

\subsubsection{Detalhes da implementação}
\label{sec:implementacaoTAP}
A implementação do algoritmo utiliza como base um sistema operacional com uma grande capacidade de configuração do sistema. O \ac{EPOS} é um \textit{framework} baseado em componentes que fornece todas as abstrações tradicionais de sistemas operacionais e serviços como: gerenciamento de memória, comunicação e gestão do tempo~\cite{Froehlich:2001}. Além disso, possui vários projetos industriais e acadêmicos que o utilizam como base~\cite{lisha}.

Este sistema operacional foi concebido com \ac{ADESD} e é instanciado apenas com o suporte básico para sua aplicação dedicada. É importante salientar que todas as características dos componentes também são características da aplicação, desta maneira, a escolha dos valores destas propriedades tem influência direta no comportamento final da aplicação. 

Cada aplicação gerada possui um arquivo próprio de configuração de abstrações para definir o seu comportamento. A Figura~\ref{progcpp:traitbuild} mostra um trecho desta configuração, que neste caso foi configurada para executar no modo biblioteca para a arquitetura $IA-32$ (\textit{Intel Architecture, 32-bit}), através de um $PC$ (\textit{Personal Computer}).

\progcpp{traitbuild}{Trecho do \textit{trait} da aplicação $DMEC$.}

Como a execução do algoritmo de troca de parâmetros ocorre dentro do ambiente de \ac{AUTETESE}, os \textit{traits} desta aplicação devem ser adaptados para o arquivo de cofiguração de \ac{XML} do ambiente. Neste contexto, a troca automatizada destes parâmetros pode ser utilizada tanto para a descoberta de um \textit{bug} no programa quanto para melhorar o desempenho para a aplicação através da seleção de uma melhor configuração.

\chapter{Resultados experimentais e análise}
\label{sec:case_study}
\ac{AUTETESE} é um ambiente desenvolvido para a execução dos testes funcionais, dos testes estruturais e da depuração de sistemas embarcados. Para validar o ambiente proposto, os experimentos foram elaborados de forma a salientar os pontos positivos da proposta e também seus pontos de melhoria. 

O primeiro experimento teve como objetivo averiguar se o algoritmo era robusto o suficiente para contribuir no ciclo de desenvolvimento de ferramentas complexas. Para tal, foi testada através de \ac{AUTETESE} um componente cuja função é fazer a estimativa de movimento em vídeos que seguem o padrão de compressão H.264.

Já o segundo experimento foi aplicado a códigos fonte gerados por desenvolvedores com pouca experiência em implementação e manutenção de \textit{software} embarcado. Seu foco está na viabilização do ambiente como ferramenta auxiliar no aprendizado.

\section{Experimento 1 - Ciclo de desenvolvimento}
Este experimento foi desenvolvido com o intuito de verificar o comportamento de \ac{AUTETESE} durante o ciclo de desenvolvimento e manutenção de \textit{software}. 

A aplicação escolhida para este experimento foi a \ac{DMEC}~\cite{DMEC}, que executa uma estimativa de movimento explorando a semelhança entre imagens adjacentes numa sequência de vídeo. Esta aplicação permite que as imagens sejam codificadas diferencialmente, aumentando a taxa de compressão da sequência de \textit{bits} gerada. A estimativa de movimento é uma fase importante para codificação H.264, já que consome cerca de 90\% do tempo total do processo de codificação.

A Figura~\ref{fig:dmec} apresenta a interação que ocorre na aplicação: o coordenador é responsável por definir o particionamento de imagem, fornecer a imagem a ser processada e retornar resultados gerados para o codificador, enquanto cada trabalhador deve calcular o custo de movimento e os vetores de movimento.

\fig{dmec}{Interação entre o coordenador e os trabalhadores na aplicação teste do $DMEC$~\cite{DMEC}.}{scale=.3}
 
\subsection{Execução dos testes e depuração}
A aplicação de teste de \ac{DMEC} verifica o desempenho de estimativa de movimento usando uma estratégia de particionamento de dados, enquanto os trabalhadores ($Workers$) realizam a estimativa e o coordenador ($Coordinator$) processa os resultados.

Um dos requisitos desta aplicação é a produção de estimativas consumindo o menor tempo possível. Desta forma, \ac{AUTETESE} foi configurado para trocar a configuração do número de trabalhadores (\texttt{NUM\_WORKERS}), a fim de paralelizar o trabalho da estimativa. A Figura~\ref{progxml:dmecApp} ilustra o arquivo de configuração para a troca de parâmetros do número de \textit{threads} trabalhadoras.

\progxml{dmecApp}{Configuração de $AUTETESE$ para as aplicações $DMEC$.}

\ac{AUTETESE} gerou todas as configurações de \texttt{NUM\_WORKERS} de 1 até 60. Em nenhuma destas configurações houve erro de compilação, mas algumas diferenças foram encontradas durante a execução das aplicações. Para exemplificar a diferença, são apresentadas as Figuras~\ref{fig:qemu_dmec_6_workers} e \ref{fig:qemu_dmec_60_workers} que ilustram, respectivamente, a execução do caso de teste do limite inferior e do limite superior.

\fig{qemu_dmec_6_workers}{Teste do $DMEC$ com configuração \texttt{NUM\_WORKERS} = 1}{scale=.4} \fig{qemu_dmec_60_workers}{Teste do $DMEC$ com configuração \texttt{NUM\_WORKERS} = 60}{scale=.4}

A diferença entre as execuções apresentadas pelas Figuras~\ref{fig:qemu_dmec_6_workers} e \ref{fig:qemu_dmec_60_workers} está na informação impressa pela aplicação no dispositivo de saída. Para a a configuração \texttt{NUM\_WORKERS} igual a 1, a aplicação imprimiu os detalhes da execução, como: \textit{numPartitions}, \textit{partitionModel}, \textit{macroblocks}, etc. Porém, na Figura~\ref{fig:qemu_dmec_60_workers} não é evidenciada a interação da aplicação com o dispositivo de saída, ou seja, comparando as duas aplicações é possível ter indícios que de uma das configurações não está correta.

Para os casos nos quais a execução da aplicação não foi bem sucedida, \ac{AUTETESE} automaticamente aciona o depurador para que se possa descobrir o porquê deste comportamento. Sendo assim, foi necessário incluir na configuração do ambiente um arquivo com \textit{breakpoints} para poder verificar-se o problema. Foram adicionados pontos de interrupção em cada uma das funções da aplicação, inclusive na função principal. A execução do \ac{GDB} com os \textit{breakpoints} é demonstrada na Figura~\ref{fig:gdb_dmec_60_workers}.

\fig{gdb_dmec_60_workers}{Depuração do $DMEC$ com a configuração \texttt{NUM\_WORKERS} = 60}{scale=.4}

Com esta configuração apenas execuções que chamaram ao menos um vez cada uma das funções foram consideradas execuções corretas da aplicação. Note-se que após o comando \texttt{continue} não houve mais nenhuma parada em qualquer uma das funções. Isto significa que para a configuração \texttt{NUM\_WORKERS} com o valor 60 não houve nenhuma resposta da aplicação e que, inclusive, não conseguiu nem atingir a função principal.

Após a execução dos casos de teste e análise do relatório, foi possível determinar que sempre que a configuração \texttt{NUM\_WORKERS} apresentava um número maior que 6 a aplicação se comportava de maneira anômala, surgindo a hipótese de que existia um limite máximo de trabalhadores. 

Descobriu-se que, embora o particionamento da imagem seja estático, o tamanho das partições é compatível com a dimensão dos macroblocos, ou seja, o número de \textit{threads} influencia no particionamento das imagens. A Figura~\ref{fig:tiposParticao} apresenta os tipos de partição implementados para o \ac{DMEC}.

\fig{tiposParticao}{Tipos de particionamento das imagens em $DMEC$}{scale=.15}

A imagem é dividida de acordo com os tipos de particionamento para evitar que blocos adjacentes da imagem fiquem divididos entre duas partições adjacentes. Como o número máximo de fatias é alcançado com a configuração $2x3$, consequentemente, o número de \textit{threads} trabalhadoras também está limitado por este valor. Ou seja, para um número de \textit{threads} maior que 6 havia uma falha na aplicação devido a um erro cometido pela \textit{thread} coordenadora e que impossibilitava que o \ac{SUT} atingisse a função \texttt{main}.

A partir desta descoberta foi possível maximizar o paralelismo das estimativas sem que houvesse efeitos colaterais no funcionamento da aplicação. Esta informação ajudou no desenvolvimento do componente, uma vez que descobriu-se que 6 \textit{threads} é o limite máximo de \textit{threads} suportado por este tipo de particionamento, e que para suportar um número maior que este seria necessário criar novas formas de particionamento.

Apesar do ambiente ter fornecido dados importantes para verificar uma falha na execução do \ac{SUT}, vale lembrar que a qualidade da informação de retorno é inerente à qualidade de informação à priori que é repassada no arquivo \ac{XML} de configuração de \ac{AUTETESE}. A Figura~\ref{fig:comp_report_partial} apresenta um trecho de relatório com algumas configurações geradas.

\fig{comp_report_partial}{Trecho do relatório com a troca da propriedade \texttt{NUM\_WORKERS} por valores gerados aleatoriamente.}{scale = 0.5}

Em casos como o do teste aleatório qualquer propriedade pode ter seu valor modificado como, por exemplo, o tamanho da pilha de aplicativos, o valor de um \textit{quantum}, a quantidade de ciclos de relógio, etc. Relatórios gerados com mais dados tendem a ser mais concisos e menos repetitivos, já os oriundos de testes aleatórios tendem a uma menor organização e maior redundância nas informações.

Outro ponto de interesse científico encontra-se na razão entre o consumo de tempo para executar os testes versus a qualidade da informação que pode ser extraída. As Figuras \ref{fig:dmec_results} e \ref{fig:dmec_time_results} apresentam, respectivamente, os resultados dos experimentos relacionados à qualidade da informação devolvida para o usuário e ao consumo de tempo.

\fig{dmec_results}{Classificação das tentativas realizadas versus a configuração da granularidade.}{scale = 0.65}
\fig{dmec_time_results}{Classificação das tentativas realizadas versus o consumo de tempo.}{scale = 0.4}

Neste experimento foram realizadas 50 tentativas para cada tipo de granularidade. Para o teste parcialmente aleatório, foi modificada a propriedade \texttt{NUM\_WORKERS} com valores em aberto e para o teste determinado foi alterada esta mesma propriedade com valores de 1 a 60.

A diferença entre as tentativas totalmente aleatórias e as outras duas granularidades foi grande. Este resultado já era esperado, visto que a depuração de uma aplicação sem informação nenhuma à priori tem a sua efetividade ligada à probabilidade de encontrar tanto a falha quanto a sua causa.

Não houve, entretanto, muita alteração entre os tipos determinado e parcialmente aleatório. Isto ocorreu devido à limitação na quantidade de propriedades e de seus possíveis valores de troca da aplicação, ou seja, com tal restrição as trocas com sucesso foram semelhantes nas duas configurações.

\fig{dmec_size_results}{Consumo de memória extra para armazenar as informações de depuração.}{scale = 0.65}

Conforme a Figura~\ref{fig:dmec_size_results} ilustra, a aplicação não tem uma imagem grande, mas quando adicionamos a informação extra em tempo de compilação, o consumo de memória foi aumentado em cerca de 200\%. Em um sistema embarcado real, o tamanho desta nova imagem seria proibitivo.

\section{Experimento 2 - Utilização em disciplina e efetividade dos resultados}
Este experimento foi elaborado para verificar a utilização do ambiente como artifício para estudo de disciplinas que envolvem codificação de \textit{software} embarcado. Além disto, este experimento tem como objetivo secundário a avaliação de \ac{AUTETESE} e da efetividade dos seus resultados. 

Para verificar a viabilidade do uso em disciplinas o experimento foi aplicado em códigos fonte de desenvolvedores inexperientes na produção de \textit{software} embarcado. A ideia é entender como  \ac{AUTETESE} pode ser introduzido em disciplinas de desenvolvimento de \textit{software} considerando a facilidade de uso, abrangência dos testes e ajuste fino do relatório gerado.

\ac{AUTETESE} foi avaliado através do código fonte produzido por estudantes do curso de bacharelado em ciências da computação e que estavam cursando a disciplina de \textit{Sistemas Operacionais II}. A escolha da turma se deu devido ao fato de ser uma disciplina com muitos exercícios práticos, desenvolvidos no sistema operacional \ac{EPOS}, cuja disponibilidade é controlada e sabe-se de antemão quais estudantes já tiveram contato com o sistema.

No início dessa disciplina os alunos têm acesso a um \ac{EPOS} incompleto e cada exercício realizado tem o intuito de implementar uma parte deste \textit{software}. Todos os exercícios têm um escopo fechado e sabe-se quais testes devem ser executados corretamente para cada uma das fases. 

O \ac{EPOS} possui um arquivo de \textit{traits} para a configuração de abstrações e propriedades de cada aplicação. Dentre estas configurações encontram-se definições que podem afetar um determinado comportamento da aplicação sem modificar seu propósito, como, por exemplo, o tipo de escalonamento. 

Se existem várias configurações válidas de uma mesma aplicação, todas elas devem ser atendidas pelas soluções dos exercícios. Entretanto, devido à inexperiência, alguns alunos não utilizam esta troca de parâmetros como ferramenta de apoio.


\subsection{Configuração do experimento e execução dos testes}
%A turma foi dividida em 12 grupos para a execução de exercícios práticos e de um projeto final. Os exercícios práticos servem para que os alunos consigam atingir a maturidade necessária para realizar um projeto final.

Para fornecer sugestões sobre as configurações mais importantes, \ac{AUTETESE} foi configurado para realizar a troca das seguintes configurações: tipo de escalonamento de processos, o quantum de tempo e tamanhos da \textit{stack} e \textit{heap}. As propriedades do arquivo de configuração desenvolvido para a disciplina podem ser observadas na Tabela~\ref{ref:tabConfiguracaoSo2}.

%\progxml{so2}{Configuração de \ac{TAP} para as aplicações do \ac{EPOS} da disciplina.}

\begin{table*}[h]
\begin{center}
\caption{Configuração de $AUTETESE$ para as aplicações do $EPOS$ da disciplina.}
\begin{tabular}{|l|c|c|c|}
\hline 
& \multicolumn{3}{c|}{\textbf{Namespace}} \\ 
\hline 
\textbf{Propriedades} & \textbf{Aplicação} & \textbf{Sistema} & \textbf{Thread} \\ 
\hline 
\textbf{STACK\_SIZE} & 512MB & 512MB & - \\ 
& 1GB & 1GB & - \\ 

\hline 
\textbf{HEAP\_SIZE} & 512MB & 512MB & - \\ 
& 1GB & 1GB & - \\ 

\hline 
\textbf{QUANTUM} & - & 10ms & 10ms \\ 
& - & 20ms & 20ms \\ 
& - & 50ms & 50ms \\ 
& - & 100ms & 100ms \\ 
& - & 200ms & 200ms \\ 
& - & 500ms & 500ms \\ 

\hline 
\textbf{Scheduling\_Criteria} & - & - & RM \\ 
& - & - & EDF \\ 
\hline 
\end{tabular} 
\label{ref:tabConfiguracaoSo2} 
\end{center}
\end{table*}

A configuração do ambiente para a disciplina focou em apenas algumas propriedades, mas com alta variabilidade de valores candidatos para a substituição. \ac{AUTETESE} foi configurada desta forma para que seja possível apresentar aos alunos a quantidade de aplicações que devem funcionar corretamente após a implementação de um determinado exercício.

Quando o foco é o aprendizado, é justo que esta configuração seja bem abrangente, visto que esta variedade pode ampliar a visão para cenários que talvez sejam importantes e que podem ter sido negligenciados durante a fase de concepção, incluindo a idealização dos requisitos não funcionais.

Inicialmente o \ac{EPOS} conta com $18$ aplicações de teste e em cada uma foram realizadas $648$ trocas, ou seja, \ac{AUTETESE} trocou as configurações destas aplicações e gerou um total de $11.664$ variações, as quais foram automaticamente executadas e depuradas. Para cada aplicação (e suas variações) foram necessárias cerca de $20$ horas de processamento.

Esta abrangência no resultado gera uma grande quantidade de dados, portanto, para simplificar o estudo da execução dos testes, o relatório fornecido apresenta informações resumidas, com a opção de verificar os arquivos de \textit{log} das execuções falhas. Na Figura~\ref{progxml:report_so2} encontra-se um trecho de relatório gerado por \ac{AUTETESE}.

\progxml{report_so2}{Trecho de relatório gerado por \ac{AUTETESE} para as aplicações do $EPOS$ da disciplina.}

Os arquivos com as execuções falhas contém informações desde o momento de compilação até o relatório final da depuração com o \ac{GDB}. Cada execução tem seu próprio arquivo, identificado pelo próprio nome concatenado com as configurações que foram trocadas. 

Utilizando como exemplo a Figura~\ref{progxml:report_so2}, nota-se que a aplicação $thread\_test$ contém execuções falhas, sendo assim, dentre os anexos devemos encontrar $1152$ arquivos cujos nomes iniciam-se com esta aplicação. Por exemplo, o arquivo  $thread\_test\_Scheduler\_EDF.log$ é um relatório de falha da execução dos testes da aplicação $trhead\_test$ quando a configuração $Scheduler$ foi modificada para $EDF$.

%A Figura~\ref{fig:so2_falha} apresenta um resumo das execuções de falha dos exercícios $E1$ a $E7$ através da solução dos grupos ($G1$ a $G12$). Nela é possível notar que o número de execuções com falha começa a diminuir conforme são entregues os exercícios.

\subsection{Identificando os defeitos}
Conforme apresentado no relatório de \ac{AUTETESE}, a aplicação $thread\_test$ falhou em todas execuções, independente das configurações apresentadas. Desta forma, a entrega do trabalho não é satisfatória e o grupo responsável deve descobrir o defeito inserido e corrigir a implementação. 

A aplicação de testes das \textit{threads} foi desenvolvida para verificar se as operações comuns a este tipo de abstração estão corretamente implementadas. A Figura~\ref{fig:thread_test_app} ilustra o trecho de código da aplicação, a qual inicia-se com a \textit{thread} $m$ criando mais duas \textit{threads} ($a$ e $b$), depois $m$ espera que ambas finalizem e exclui todas as \textit{threads}, realizando inclusive uma auto deleção.

\fig{thread_test_app}{Trecho de código da aplicação \textit{thread\_test}}{scale = 0.4}

Na Figura~\ref{fig:thread_test_errado} encontra-se o resultado da execução da aplicação, que continuou mesmo depois da \textit{thread} $m$ executar o comando de auto deleção. \ac{AUTETESE} revelou que esta aplicação possui o processamento incorreto, pois no arquivo de \textit{log} da execução não poderia aparecer a frase \textit{"It should not be shown on the display!"}. 

\fig{thread_test_errado}{Resultado da execução falha da aplicação \textit{thread\_test}.}{scale = 0.4}

Quando a aplicação é reprovada, uma instância do \ac{GDB} é automaticamente é disponibilizada para que o usuário possa encontrar o erro. Para esta aplicação foram inseridos \textit{breakpoints} e \textit{watchpoints} conforme a Figura~\ref{fig:thread_test_breakpoints}. 

\fig{thread_test_breakpoints}{Depuração da aplicação \textit{thread\_test}.}{scale = 0.5}

A depuração ocorreu no modo \textit{step} e foi possível verificar que chamada do método de destruição das \textit{threads} foi executado para todas as \textit{threads}, entretanto, apenas $a$ e $b$ foram realmente deletadas. Analisando o código fonte, notou-se que a \textit{thread} $m$ não entrou em nenhuma das condições para a deleção. Analisando a implementação do destrutor da \textit{thread}, descobriu-se que o erro ocorreu no tratamento do término da última \textit{thread}, que não considerou o caso de haver uma \textit{thread} esperando na fila de um sincronizador.

Para resolver este problema, os alunos responsáveis optaram dor desenvolver uma lista de recursos que cada abstração do sistema deve gerenciar. Sendo assim, quando a \textit{thread} for deletada é necessário liberar todos os sincronizadores em que ela possa estar. Além disso, agora a \textit{thread} não pode de auto deletar. O novo trecho de código fonte está ilustrado na Figura~\ref{fig:thread_test_codigo_corrigido}.

\fig{thread_test_codigo_corrigido}{Código fonte do destrutor da \textit{thread} após correção.}{scale = 0.4}

Após a correção do código fonte, a aplicação \textit{thread\_test} foi novamente executada por \ac{AUTETESE} e, desta vez, foi aprovada. Este resultado não significa que o código não possui erros e, inclusive, a correção não realiza o tratamento da auto destruição da \textit{thread}. Uma solução mais completa depende de outros aspectos que não foram completamente abordados na aplicação de teste como, por exemplo, perda do contexto de execução e a não liberação da pilha. Este defeito na auto deleção não foi exclusivo de um grupo, pois mais de 25\% da turma não se atentou ao tratamento desta situação e não realizou a entrega corretamente.
%4 grupos não implementaram, 5 grupos não entregaram e 3 entregaram ok
 
\subsection{Distribuição dos exercícios com falhas}
Além de identificar comportamentos inesperados do código fonte dos alunos com \ac{AUTETESE}, também é possível retirar dados para compor indicadores de desempenho. Estes indicadores podem ser direcionados a vários aspectos da disciplina, como auxiliar o monitoramento da evolução das entregas, servir de referência para tomada de decisões, identificar pontos de melhoria, entre outros. Um exemplo de monitoramento da evolução das entregas encontra-se na Figura~\ref{fig:dist_falha}.

\fig{dist_falha}{Distribuição percentual das falhas.}{scale = 0.5}

Na distribuição percentual das falhas verifica-se que no ínicio da disciplina existe uma redução na quantidade de falhas nas entregas, e que este declínio se sustenta até a entrega do exercício $E3$. Ao monitorar o desempenho da turma, identifica-se que no $E4$ o percentual de falhas se manteve. Isto ocorreu porque os exercícios de $E1$ até $E3$ relacionam-se diretamente com o assunto \textit{threads}, criando uma zona de estabilidade. Já o $E4$ envolve o conceitos como o de eventos programados e rotina de tratamento de interrupções. 

Focando-se na análise exercício por exercício, os relatórios de \ac{AUTETESE} podem servir para descobrir tendências do aprendizado e ajudar o responsável pelas turmas a conduzir aulas focadas nas dificuldades dos alunos. Pois a partir dos erros mais comuns dentre os alunos da turma é possível gerar discussões saudáveis sobre a área de sistemas operacionais de modo a facilitar o aprendizado e desenvolvimento dos próximos exercícios práticos. 

Como os experimentos possuem um escopo fechado, o relatório da execução dos testes pode ser utilizado para verificar se os requisitos dos exercícios foram atendidos. Deve-se considerar que o ambiente auxilia a identificação de padrões dos erros e dificuldades na hora de implementar um \textit{software} embarcado, mas que o discernimento do responsável pela turma é imprescindível.

Como planos futuros consideramos utilizar \ac{AUTETESE} de maneira integrada ao serviço de controle de versão (\textit{svn}, \textit{git}, \textit{mercurial}, etc). Através disso os alunos podem receber relatórios que vão auxiliá-los durante o aprendizado e que servem também para a geração de \textit{feedback} para o ambiente. Inclusive, este tipo de insumo é importante na melhoria de \ac{AUTETESE}, pois os resultados gerados podem servir como base para realizar os ajustes necessários para manipular a execução dos testes e a depuração.

\chapter{Análise qualitativa das ferramentas de teste e depuração de \textit{software}}
\label{cap:analiseQualitativa}
No Capítulo \ref{cap:trabalhosRelacionados} foram discutidos vários estudos focados em apresentar soluções práticas para os diversos pontos de melhoria na área de teste e depuração de sistemas. Os mesmos trabalhos serão agora o foco de um estudo qualitativo de suas características.

Existem vários trabalhos~\cite{bertolino:07, 6065143, 6200204, 6228988} que apontam o passado, estado atual e futuro do teste e depuração de \textit{software}. Pesquisadores como Bertolino~\cite{bertolino:07} apontaram metas e desafios relevantes para o avanço da área, enquanto trabalhos mais recentes procuram definir o estado da arte, verificando quais desafios estão sendo atendidos e apontando as dívidas técnicas remanescentes.

Assim como o presente trabalho, grande parte da produção de conhecimento e pesquisa na área (mais de 46\%) está focada em desenvolver ferramentas e ambientes que automatizam uma ou várias atividades de teste de \textit{software}~\cite{6065143}. 

Para que os trabalhos possam ser efetivamente comparados, a definição das características que serão analisadas qualitativamente foram inspiradas na pesquisa de Bertolino~\cite{bertolino:07}, no qual são explicitados os conceitos mais relevantes para a área de testes de \textit{software}, separados em realizações passadas e em metas ainda não atingidas.

Dentre as metas para a pesquisa em teste de \textit{software}, as relacionadas diretamente com o escopo deste trabalho são os testes 100\% automatizados. Esta meta é composta por um conjunto de desafios essenciais para o avanço do estado da arte.

\section{Testes 100\% automáticos}
A automação total dos testes depende de um ambiente poderoso. Ele deve ser capaz de automaticamente: (i) providenciar a instrumentação do \textit{software}, (ii) gerar o suporte necessário para desempenhar as atividades (ex. \textit{drivers}, \textit{stubs}, simuladores, emuladores), (iii) gerar os casos de teste mais adequados para o modelo, (iv) executar os casos de testes e, finalmente, (v) emitir um relatório sobre o ensaio executado.

Ainda existem muitos desafios a serem solucionados para atingir este nível de automação, contudo, os que mais se relacionam com o presente trabalho são:

\begin{description}
\item{\textbf{Desafio da geração de dados de entrada}}. A geração de dados de entrada para os testes sempre foi um tópico de pesquisa muito ativo, em meio à academia. Entretanto, este esforço produz um impacto limitado na indústria, onde a atividade de geração de teste permanece em grande parte manual.
Os resultados mais promissores são as abordagem baseada em modelo e a geração aleatória acrescida de alguma técnica com inteligência. Desta forma, ainda se faz necessária uma técnica que possa ser utilizada de maneira mais abrangente.

\item{\textbf{Desafio das abordagens de teste específicas para o domínio}}. O atual estado da arte aponta para a necessidade de executar a fase de testes com abordagens específicas de domínio. 
O intuito deste desafio é encontrar métodos e ferramentas específicas de domínio e poder aprimorar a automação de teste. Neste sentido, alguns trabalhos já conseguiram demonstrar a extração automática de requisitos para o teste a partir de um modelo escrito em uma linguagem fortemente tipada e específica de um domínio.

\item{\textbf{Desafio dos testes \textit{online}}}. O desafio de testes \textit{online} foca na ideia de monitorar o comportamento de um sistema em pleno  funcionamento, com o \textit{software} executando e recebendo todas as interferências reais.
É um desafio importante e complexo, pois nem sempre é possível realizar este tipo de teste, especialmente para aplicações embarcadas implantadas em um ambiente de recursos limitados, onde a sobrecarga exigida pela instrumentação de teste não poderia ser viável.
\end{description}

\section{Análise do ambiente proposto}
A partir da contextualização das características importantes para a evolução do estado da arte, agora já é possível realizar uma análise qualitativa de \ac{AUTETESE}, ressaltando as contribuições e melhorias do ambiente. As ferramentas e técnicas propostas nos trabalhos relacionados também serão analisadas sob o mesmo ponto de vista.

A meta dos testes completamente automatizados permite um controle de qualidade no desenvolvimento e manutenção de um \textit{software}. Esta meta é de difícil implantação, porém uma vez que a automação esteja em pleno funcionamento é possível manter a confiabilidade do sistema de maneira rápida e eficiente.

A Tabela~\ref{ref:tabQuestao3TAP} mostra a relação de \ac{AUTETESE} com a meta de automação completa da execução de testes de \textit{software}.

\begin{table*}[h]
\begin{center}
\caption{Análise de $AUTETESE$ para a meta de automação dos testes}
\begin{tabular}{|p{.18\textwidth}|p{.82\textwidth}|} 

\hline
\textbf{Geração de dados de entrada} & O suporte à geração de dados é \textbf{parcial}, pois a informação inicial dos dados de entrada é fornecida pelo usuário do ambiente. No caso do teste não ser determinado pelo usuário, o ambiente inclui dados iniciais aleatórios, sem adição de inteligência.\\ 

\hline
\textbf{Abordagens específicas para o domínio} &  \ac{AUTETESE} consegue importar configurações específicas de domínio para realizar a execução de testes, contemplando de forma \textbf{parcial} a extração de dados relacionados ao domínio da aplicação.\\ 

\hline
\textbf{Testes \textit{online}} & Este diferencial é apresentado por \ac{AUTETESE} através do ambiente integrado de teste e depuração, com suporte \textbf{total} aos testes durante à execução da aplicação. Todavia, quando a execução da aplicação ocorre em ambiente emulado o suporte à testes \textit{online} é considerado parcial.\\ 

\hline
\end{tabular}
\label{ref:tabQuestao3TAP} 
\end{center}
\end{table*}

Para que \ac{AUTETESE} seja considerado um ambiente 100\% automático, ainda deve-se investir em técnicas para receber a entrada dos dados, seja a partir da extração de valores do modelo ou através de algum tipo de inteligência artificial. Ao integrar \ac{AUTETESE} à ferramentas como, por exemplo, a \textit{ADESDTool}~\cite{Cancian:PHD:2011} pode-se fornecer melhores configurações para o próprio sistema embarcado.

A geração de dados de entrada é uma característica que muitas vezes é preterida por ferramentas que não realizam a geração dos casos de teste. Técnicas como a Partição de \textit{software} e Captura e Reprodução empregam o modelo do \textit{software} como dado de entrada para o teste e depuração, contudo, não são capazes incluir mutabilidade coerente ao modelo.

Conjuntamente, a abordagem específica de domínio também exerce bastante influência para uma automação total dos testes. Existem trabalhos totalmente focados para uma abordagem específica de domínio, como é o caso da Execução de testes por sistema alvo. Nesta técnica considera-se que cada sistema embarcado possui propriedades que o definem e, portanto, devem ser levadas em conta na escolha do modelo de testes a ser utilizado.

A Tabela~\ref{ref:tabQuestao3Ferramenta} apresenta uma análise comparativa entre as ferramentas e técnicas correlatas para os desafios propostos através de testes 100\% automatizados.

\begin{table*}[h]
\begin{center}
\caption{Comparativo qualitativo do suporte para a meta de testes 100\% automáticos}
\begin{tabular}{|p{.3\textwidth}|p{.19\textwidth}|p{.22\textwidth}|p{.15\textwidth}|} 
\hline
 & \multicolumn{3}{c}{\textbf{Suporte para o desafio}} \\

\cline{2-4}
\textbf{Trabalhos} & \textbf{Geração de entradas} & \textbf{Abordagem específica para domínio} & \textbf{teste \textit{online}}\\ 

\hline
\ac{AUTETESE} & Parcial & Parcial & Sim \\ 

\hline
$Justitia$ & Sim & Não & Sim \\ 

\hline
$ATEMES$ & Sim & Não & Sim \\ 

\hline
Execução de testes por sistema alvo & Parcial & Sim & Sim \\ 

\hline
Partição do \textit{software} & Não & Parcial & Sim \\ 

\hline
Depuração estatística & Não & Parcial & Não \\ 

\hline
Depuração por delta & Parcial  & Parcial & Não \\ 

\hline
Captura e reprodução & Não & Não & Sim\\ 

\hline
\end{tabular}
\label{ref:tabQuestao3Ferramenta} 
\end{center}
\end{table*}

Esta meta deixa em evidência que o ambiente integrado de \ac{AUTETESE}, mesmo com o foco voltado para a simplicidade de uso,  fornece bastante suporte à execução automática, tanto do teste, quanto da depuração \textit{online}. Além de \ac{AUTETESE}, apenas o trabalho \textit{Execução de testes por sistema alvo} apresenta um resultado positivo em todos os desafios propostos.

\chapter{Conclusões}
\label{cap:conclusao}
Nesta dissertação é discutida a automatização da execução de testes e depuração de \textit{software} embarcado. Isto é, realização das operações de teste e depuração focando em interferir minimamente no funcionamento do \ac{SUT}, levando-se e conta as limitações dos sistemas embarcados.

Para a execução automática de ambas as atividades é proposto um ambiente capaz de aplicar os casos de teste no \textit{software}, executar a depuração e sintetizar os resultados em um relatório. A abordagem de executar os casos de testes através de trocas de configuração das propriedades do sistema permite a reutilização dos casos de teste para aplicações que compartilhem uma determinada propriedade.

O ambiente de execução de testes e depuração fornece independência em relação à plataforma física de destino. Desta forma, os desenvolvedores não precisam gastar tempo compreendendo uma nova plataforma de desenvolvimento sempre que alguma característica do sistema embarcado for atualizada. Este é um passo importante, pois alguns sistemas embarcados podem não ser capazes de armazenar os dados adicionais necessários para apoiar a depuração. 

Os experimentos realizados apontaram valores quantitativos do tempo consumido para realizar os testes e da efetividade dos ensaios. No primeiro experimento, o ambiente \ac{AUTETESE} foi utilizado para executar casos de teste e depuração de uma aplicação real. Para este experimento houve um aumento de mais de 500\% no tamanho do código da aplicação e a uma sobrecarga de 60\% para o tempo de execução do teste. Ainda, foi confirmada que a eficácia do algoritmo de troca de parâmetros está intimamente ligada à efetividade da configuração dos valores e da granularidade configurados em \ac{AUTETESE}. 

O segundo experimento comprovou a efetividade da utilização do ambiente como ferramenta auxiliar para disciplinas de desenvolvimento de \textit{software} embarcado. Com alguns ajustes é possível extrair relatórios que podem facilitar tanto os professores em suas avaliações, quanto os alunos em discussões e no seu aprendizado. Entretanto, em trabalhos futuros, se faz necessária a integração da ferramenta com serviços de versionamento e o envio automático dos relatórios para os alunos.

Adicionalmente, apresentou-se uma análise qualitativa de \ac{AUTETESE}, levando-se em conta os desafios ainda presentes no teste de \textit{software} e metas que necessitam ser atingidas. \ac{AUTETESE} expressa comprometimento com o avanço do estado da arte, visto que proporciona soluções para grande parte dos desafios apontados. 

\section{Perspectivas futuras}
Ao final do desenvolvimento deste trabalho, foram identificados pontos que poderão ser otimizados em trabalhos futuros:

\begin{itemize}
	\item Analisar a possibilidade de reduzir o custo de memória e de tempo utilizado para a execução de testes. 
	\item Verificar formas de melhorar a configuração de \ac{AUTETESE} e dos dados de entrada do algoritmo de troca de parâmetros.
	\item Realizar um maior número de experimentos para verificar o desempenho de \ac{AUTETESE} em ambientes com restrições e que apresentem testes mais complexos.
	\item Aprimorar o ambiente para conseguir atingir mais desafios identificados na área de teste e depuração de \textit{software}.
	\item Criar interface de integração de \ac{AUTETESE} com serviços de versionamento de \textit{software}.
	\item Criar mecanismo de envio de e-mail automático dos relatórios.
	\item Propor a inclusão do ambiente em disciplinas de desenvolvimento de \textit{software} embarcado.
\end{itemize}

O desenvolvimento deste trabalho resultou em artigos publicados em eventos~\cite{soldi2013AEP, soldi2013tap}, e que contribuirão para o estado da arte nas áreas de verificação de \textit{software} e de construção de sistemas embarcados. Como perspectiva futura também encontra-se a produção de mais artigos que corroborem a efetividade do ambiente e que abranjam as novas funcionalidades.

\bibliographystyle{ufscThesis/ufsc-alf}
\bibliography{bibliografia}
\end{document}