%\documentclass[12pt, a4paper]{article}
\documentclass[conference]{IEEEtran}

\usepackage[utf8]{inputenc}
\usepackage{graphicx}

\newcommand{\fig}[4][ht]{
  \begin{figure}[#1] {\centering\scalebox{#2}{\includegraphics{fig/#3}}\par}
    \caption{#4\label{fig:#3}}
  \end{figure}
}


\begin{document}

\title{Optimizing Motion Estimation for Real-time HDTV Encoding}

\author{
\IEEEauthorblockN{Ronaldo Husemann}
\IEEEauthorblockA{University Center UNIVATES\\
Av. Avelino Tallini, 171\\
Lajeado, RS, Brazil\\
Email: husemann@univates.br}
\and
\IEEEauthorblockN{Antônio Augusto Fröhlich}
\IEEEauthorblockA{Federal University of Santa Catarina -- UFSC\\
Laboratory for Software and Hardware Integration -- LISHA\\
PO Box 476 - 88049-900 - Florianópolis, SC, Brazil\\
Email: guto@lisha.ufsc.br}
\and
\IEEEauthorblockN{Mateus Krepsky Ludwich}
\IEEEauthorblockA{Federal University of Santa Catarina -- UFSC\\
Laboratory for Software and Hardware Integration -- LISHA\\
PO Box 476 - 88049-900 - Florianópolis, SC, Brazil\\
Email: mateus@lisha.ufsc.br}
\and
\IEEEauthorblockN{Valter Roesler}
\IEEEauthorblockA{Federal University of Rio Grande do Sul -- UFRGS\\
Av. Bento Gonçalves, 9500\\
Porto Alegre, RS, Brazil\\
Email: roesler@inf.ufrgs.br}
}

\date{}

\maketitle

\begin{abstract}

Over 60\% of the total encoding time of raw video into H.264 is spent in
block-matching, a stage of Motion Estimation. In this paper we introduce
two block-matching optimizations that yield significant performance
improvements on Motion Estimation: truncation of the two less
significant bits per sample and 4:1 subsampling by macroblock.  When
applied to the JM Reference Encoder, our strategy showed an average
speedup of 2.64 times in total encoding time with a small loss of
quality (less than 0.5 dB). We also discuss the implementation of the
proposed strategy in hardware, evaluating it in terms of memory
consumption, clock cycles, frequency of operation, and chip area. In
comparison to available data from related work, this implementation is
the fastest one and demands about 72,000 logic gates.

\end{abstract}

%-----------------------------------------------------------------------------
\section{Introduction}\label{sec:intro}

Motion Estimation (ME) is a technique used to explore temporal
redundancy in multimedia data during compression. Temporal redundancy
arises from the fact that neighbor frames in a video stream very often
share similar pixel blocks. Therefore the goal of Motion Estimation is
to \emph{estimate} the shifting of such similar regions across neighbor
frames, thus enabling them to be differentially encoded.  Standards like
ISO MPEG series and ITU-T H26x are examples of encoders that use ME to
improve compression ratios in output video
streams~\cite{citeulike:1269699}.

In this article, we explore optimization opportunities in block-based
Motion Estimation algorithms. More precisely, we focus on
\emph{block-matching} algorithms, since our experiments with the JM H.264
Reference Encoder~\cite{site:jm} demonstrated that this stage accounts
for over 60\% of the total encoding time. Furthermore, a previous
implementation of a high definition scalable MPEG-2 hardware encoding
engine by one of the authors showed that ME takes up 53\% of the chip
area~\cite{Husemann:2006}. Block-based algorithms were preferred over
pixel and region-based ones mostly because we intended our optimizations
to be efficiently implemented in hardware and the regular nature of
macroblocks (i.e. fixed-size, square regions) is very convenient in that
scenario. For the same reason, Sum of Absolute Differences (SAD) was
taken as matching criterion in detriment of other traditional criteria
such as Sum of Squared Errors (SSE), Mean Squared Error (MSE), and Mean
Absolute Difference (MAD).

In order to cope with the computational cost of block-matching, two
major alternatives to Full-Search have been considered in this work:
sparse search and multi-resolution. Sparse search algorithms avoid
scanning the whole search window by deploying heuristics to identify the
most probable regions in which similar macroblocks can be
found. PMVFast~\cite{Tourapis:2001}, Three-Step search
(TSS)~\cite{Koga:1981}, and Four-Step Search (4SS)~\cite{bb17234} are
examples of sparse search algorithms. An alternative proposal, known as
Diamond Search (DS), was presented by Zhu~\cite{821744}, and was then
improved by Sung to support different H.264 modes~\cite{bb18476}.  The
second approach, multi-resolution, has the goal of simplifying ME
through the reduction of image resolution. The computational complexity
is reduced because the motion information computed in lower resolution
can be used to speed up the motion detection in higher resolutions. The
Wavelet Method~\cite{bb6698}, the Two-Stage Mechanism~\cite{Kim:1998},
and the Pyramidal Method~\cite{bb17203,Zan:2003} fall into this category
of block-matching algorithms. From these options, PMVFast has the best
performance and shows a Peak Signal to Noise Ratio (PSNR) similar to
that of Full-Search and thus was taken as a promising algorithm for the
optimizations we had in mind.

Our proposal for an optimized Motion Estimation engine builds on the
PMVFast block-matching algorithm and the SAD matching criterion to
explore two major strategies: truncation of the two least significant
bits of each sample and 4:1 subsampling by macroblock. The engine was
implemented in the JM Reference Encoder for quality assessment purposes
and subsequently on a FPGA for performance and size evaluation.

The next sections of this article are organized as follow:
section~\ref{sec:related} discusses the related work that inspired our
proposal and also work on hardware optimization;
section~\ref{sec:proposal} presents the proposed ME optimizations;
sections~\ref{sec:JM} and \ref{sec:hardware} describe the implementation
of the proposed optimization respectively for the JM H.264 Reference
Encoder and as dedicated hardware. Section~\ref{sec:results} is
dedicated to evaluate the proposal and its implementation while
comparing it to related work. The final considerations are presented in
section~\ref{sec:conclusion}.


%-----------------------------------------------------------------------------
\section{Related Work}\label{sec:related}

Liu~\cite{bb17292} performed a series of experiments with 1:1
subsampling (which is the canonical form and does not imply in losses),
2:1, and 4:1 using two different video sequences: \emph{Tennis}, and
\emph{Football}. Liu used 8x8 blocks, and the pixel choice to subsample
was based on an algorithm that chooses up to four different groups
asymmetrically positioned in the block.
Zhong-Li~\cite{DBLP:journals/tcsv/HeTCL00} performed a series of
experiments with 8-bit samples and adaptive truncation of 0, 1, 2, 3, 4,
5, 6, and 7 bits. His experiments were performed using five video
sequences with different animation characteristics: \emph{Miss America},
\emph{Salesman}, \emph{Clarie}, \emph{Carphone}, and \emph{Foreman}.
These works were the major inspiration to the approach presented in
section~\ref{sec:proposal}.

Lee~\cite{bb15910} describes hardware optimizations to ME based on
resolution reduction to minimize the computational cost and chip
area. The Successive Elimination Algorithm (SEA)~\cite{Brunig:2001} is
another approach used to speed up the Sum of Absolute Differences (SAD)
calculation proposed by Brunig. SEA avoids unnecessary SAD calculations,
comparing the minimum SAD to the absolute difference of the sum of all
current macroblock pixels and the sum of all candidate macroblock pixels
in the reference image. The total execution time is decreased, because
when the sum of a candidate macroblock surpasses the minimum, it can be
eliminated. The Global Elimination Algorithm (GEA) is an improvement of
SEA in hardware~\cite{bb17548}. GEA modifies SEA to achieve a regular
flow, with a constant number of clock cycles, and without the necessity
of an initial estimation. We compare our hardware implementation to GEA
in section~\ref{sec:hardware}.


%-----------------------------------------------------------------------------
\section{Proposed Optimizations} \label{sec:proposal}

We envisioned two major opportunities to optimize matching operations
within block-based Motion Estimation algorithms: subsampling and
truncation. Both can be applied to the computation of matching criteria
to speedup the whole process of motion estimation.

 
% The approach is applied during the Sum of Absolute Differences (SAD) computation.

\subsection{Subsampling}

Subsampling aims at reducing the time needed to compute a matching
criterion, like SAD, by applying the corresponding operation only to a
subset of the pixels in a macroblock. The optimization relies on the
assumption that neighbor pixels in a macroblock should have similar
values and thus computing the matching criterion from a regular subset
should yield comparable results. This assumption is confirmed by the
experiments presented in section~\ref{sec:JM}.  Figure
\ref{fig:subsampling} illustrates  the process of 4:1 subsampling in a 16x16
macroblock. The dark pixels are the pixels taken into
consideration. Instead of comparing 256 pixels (16x16), the algorithm
compares only 64 pixels (8x8), speeding up the process.
% The choice of 4:1 subsampling was based on practical experiments and also on data reported by Liu~\cite{bb17292}.

\fig{.5}{subsampling}{Illustration of 4:1 subsampling.}

\subsection{Truncation}

Similarly to subsampling, truncation also aims at reducing the
computational cost of block matching by eliminating redundant data. As
basis for this optimization we took the assumption that variations in
the least significant bits of pixels in a macroblock are more likely to
originate from texture nuances than from motion. Edges and shapes
usually considered in motion detection are more likely to cause
variations in most significant bits of pixels. 

Truncating bits of pixels has a more direct impact for Motion Estimation
implemented in hardware, since the optimization drives registers and
memory words to shrink, reducing the overall chip area and boosting
memory access. Nonetheless, our experiments with the JM H.264 Reference
Encoder demonstrate that truncation can also improve software-implemented 
algorithms, because the Full-Search block-matching algorithm in JM 
features an early termination mechanism. The motion cost in H.264 is calculated
using a Lagrangian rate-distortion cost function~\cite{1101854}. The Full-Search block-matching
algorithm in JM first computes the rate component of the motion cost for a
specific position in the search window. If the rate component value is grater or
equal than the previous minimum motion cost (cost of a previous compared block), the
distortion component of motion cost (given by the SAD function) is not computed,
and the motion cost computation is abbreviated for that block. Truncating bits,
numbers that were slightly different from each other, become equal. Because of
that, the number of occurrences that the previous minimum cost and the
rate component of the current cost are equal increase, and the SAD do not need to
be computed. The speedup is then caused by the reduction in the frequency of 
calls to the SAD function.

%-----------------------------------------------------------------------------
\section{Evaluation in the JM H.264 Reference Encoder}\label{sec:JM}

We implemented the optimizations described in the previous section in
the JM H.264 Reference Encoder~\cite{site:jm} to assess their impact on
the final video quality.  Being the reference software implementation
for the H.264 standard, JM includes both encoding and decoding
functionality. Our experiments were carried out with JM version 14.2
and consisted in encoding a set of reference video sequences with the
original encoder and subsequently with the modified one, therefore
enabling us to measure variations in the Peak Signal to Noise Ratio
(PSNR). Modifications to JM were implemented so that subsampling and
truncation could be evaluated both in separation and together, thus
rendering more detailed results.

For inter macroblock modes in H.264 (i.e. modes related to the Motion Estimation),
the motion cost for chrominance components derives from the motion cost for 
luminance components~\cite{1101854}. Consequently the PSNR for chrominance components derives 
from the PSNR for luminance components. For this reason, in this paper we focus 
on the PSNR variation of the luminance component.

The video sequences chosen for the experiments encompass different
resolutions and frame rates: Foreman, Mobile \& Calendar, City 4CIF, and
Stockholm 720p. These are all raw video sequences in YUV format
available at \emph{http://media.xiph.org/video/derf/}. Their key
features are summarized in table~\ref{tab:sequences}.


\begin{table}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Sequence} & \textbf{Resolution} & \textbf{Frame rate (fps)} & \textbf{Duration (s)} \\
\hline
Foreman & QCIF (176 x 144) & 30 & 10 \\
\hline
Mobile & CIF (352 x 288) & 30 & 10 \\
\hline
City & 4CIF (704 x 576) & 60 & 10 \\
\hline
Stockholm & 720p (1280 x 720) & 60 & 10 \\
\hline
\end{tabular}
\caption{Video sequences used in the experiments.}
\label{tab:sequences}
\end{center}
\end{table}

Subsampling was implemented in JM as proposed in
section~\ref{sec:proposal}. Samples are taken from macroblocs following
a regular, symmetrical pattern. For instance, a 4:1 subsample is
obtained by having the iteration loop to skip columns and rows as show
in Figure~\ref{fig:subsampling}. Truncation was done in JM by assigning
zero to the least significant bits of each sample, since actual register
and memory truncation are meaningless to software implementations.  Both
techniques, subsampling and truncation, were implemented during the SAD
computation. The block-matching algorithm used was Full-Search.

Our results show that subsampling and LSB truncation both improve video
encoding performance while degrading quality, either deployed separately
or in combination. Both parameters, performance and quality, vary
proportionally to the extend of the applied optimizations as can be seen
in figures~\ref{fig:jm_sub_perf}
to~\ref{fig:jm_tru_qual}. Figure~\ref{fig:jm_sub_perf} shows the
performance improvement for the chosen video sequences as a function of
a growing subsampling factor, while Figure~\ref{fig:jm_sub_qual} shows
the corresponding quality loss measured as a decrease in
PSNR. Figures~\ref{fig:jm_tru_perf} and~\ref{fig:jm_tru_qual}
illustrate the facts for truncation.

\fig{0.6}{jm_sub_perf}{Impact of subsampling on JM performance.}
\fig{0.6}{jm_sub_qual}{Impact of subsampling on JM quality.}
\fig{0.6}{jm_tru_perf}{Impact of truncation on JM performance.}
\fig{0.6}{jm_tru_qual}{Impact of truncation on JM quality.}

From the obtained data, we noticed that a combination of 4:1 subsampling
and 2 LSB truncation would result in considerable speedups at a
relatively small quality loss. Indeed, we looked into the obtained data
for a combination that would cause no more than 0.5db decrease in PSNR
and yet had good potential for a hardware implementation.  Truncation of
2 LSB incurs in an average PSNR reduction of less than 0.1dB, while 4:1
subsampling has a higher toll on quality, about 0.4db. Subsampling,
however, is the major speedup source for JM, since $3/4$ of the data in
each macroblock is simply ignored\footnote{As explained in
section~\ref{sec:proposal}, speedups from truncation on software ME
implementations arise from early termination mechanisms.}. This leads to
speedups of up to 3.18 times (for the Stockholm sequence).

The combination of 4:1 subsampling and 2 LSB truncation was further
evaluated, yielding the charts in Figures~\ref{fig:jm_perf}
and~\ref{fig:jm_qual}.  The average speedup for the combined approach
was of 2.64 times, with an average impact on PSNR of less than
0.5db. Notice that higher resolution sequences suffer less quality loss
from subsampling. This is because neighbor pixels of high resolution
sequences tend to be more similar than those in lower resolution
sequences.

\fig{0.6}{jm_perf}{Optimizations speedup on JM.}

\fig{0.6}{jm_qual}{PSNR degradation due to optimization on JM.}


%-----------------------------------------------------------------------------
\section{Implementation in Hardware}\label{sec:hardware}

Implementing the Motion Estimation optimizations proposed in
section~\ref{sec:proposal} in hardware brings about additional benefits
as regards performance.  A motion estimation engine implemented in
hardware can easily exploit data and process parallelism to achieve
considerable speedups relative to sequential implementations. This
has already been demonstrated by other groups~\cite{Nousias:2002,
bb15910, bb17548}.

As explained earlier in this article, the optimizations proposed here
were implemented around the SAD computation within the PMVFast
block-matching algorithm. They focus on reducing the comparison time
between target and reference image macroblock.  Basically, the
implemented algorithm gets the target macroblock and walks through the
reference image in a diamond topology, searching for the minimum SAD. In
the first loop, the algorithm compares the central macroblock of the
reference image, the central macroblock shifted one pixel up, shifted
one pixel right, one pixel down, and one pixel left. 

The first optimization, subsampling, when applied in the 4:1 proportion,
immediately yields a 75\% reduction in the number of operations needed
to calculate the SAD. Additionally, providing alignment of pixel data to
the width of the main memory bus further reduces the number of accesses
to the memory holding the reference macroblock. With 4:1 subsampling,
for example, when each line in the macroblock contains 8 pixels, the
PMVFast algorithm needs 10 pixels (8 pixels of central macroblock and
two more pixels of the adjacent macroblocks) to shift the main
macroblock in each one of four directions (up, down, left and right).
The memory cost varies according to the number of bits per
pixel. Considering 10 8-bit pixels, the bus width would be 80 bits. The
hardware implementation used a 64-bit memory bus, so the ideal sample
size is 6 bits, allowing a complete line to be read in only one memory
access.

An important issue in our hardware design was to limit contention on the
system's main memory. Therefore, after a frame is fetched from system
memory and subjected to truncation and subsampling, it is locally stored
in two fast access memories, called supplying memories, as illustrated
in Figure~\ref{fig:v_and_h_memory-orig}.  
% XXX Guto XXX reescrevi mudando o sentido! Deve ser verificado! A figura não ilustra o efeito das otimizações, ficando um pouco ``boba''. Acho que mais detalhes ajudariam.
The first memory, called H-Memory, stores pixel sequences arranged in
horizontal lines, which simplifies up and down shifts. The second one,
called V-Memory, contains pixel sequences arranged in vertical columns,
facilitating lateral pixel manipulation. Each supplying memory holds a
copy of the whole reference frame after truncation and subsampling,
allowing for parallel access within the iteration loops in the
algorithm.

\fig{.45}{v_and_h_memory-orig}{H and V supplying memories.}

Diamond searching is implemented through a dedicated matrix of registers
to increase parallelism. According to the PMVFast algorithm, the
register matrix is initially filled with pixel data which represent the
central macroblock and their neighboring pixels.  After that, each clock
cycle triggers a sequence of operations to compare current SAD values
with those for the central macroblock and then shift the macroblocks by
one pixel to all directions (above, below, right, left, and
diagonal). The result of the nine SAD computations determine the next
step of the algorithm as follow:

\begin{itemize}

\item If the central SAD has the lowest result of all, then the value for
the motion vector is determined;

\item If the top or bottom SAD has the lowest result of all, then the
register matrix is taken from the corresponding line of H-Memory (top or
bottom memory shift);

\item If the right or left SAD has the lowest result of all, then the
register matrix is taken from the corresponding column of V-Memory
(lateral memory shift);

\item If the SAD of some diagonal has the lowest result of all, then the
register matrix is taken from the corresponding line of H-Memory and by
the corresponding column of V-Memory.

\end{itemize}

Data from the supplying memory is continually inserted into the register
matrix to define the current searching matrix. For example, if a top
line is inserted into the searching matrix, pixel data is shifted from
top to bottom. The last inferior line is then discarded. This procedure
(i.e. data shifting into the register matrix) is executed in only one
clock cycle.


%-----------------------------------------------------------------------------
\section{Result Analysis}\label{sec:results}

The optimized Motion Estimated engine described above was implemented in
VHDL and simulated with Modelsim.  The implementation was evaluated
according to the following metrics:

\begin{itemize}
\item Number of clock cycles spent in the motion estimation module;

\item Chip area, measured in gate number;

\item Minimum frequency operation;

\item Demanded memory.

\end{itemize}

These metrics are the same used in Huang's work~\cite{bb17548} about the
Global Elimination Algorithm~(GEA) mentioned in
section~\ref{sec:related}. Since the encoder for which the Motion
Estimation engine was designed is still under development, it was not
possible to measure PSNR directly during the evaluation
% XXX Guto XXX: se fosse possível, porque precisamos da JM???
essays. Nevertheless, we can presume the impact of the proposed
optimization on PSNR for the hardware module to be the same as that of
the JM implementation, since all other hardware optimizations targeted
performance improvements that did not incur in algorithm modifications.
PMVFast was performed with small diamond search, taking 64 values in
each macroblock (obtained through 4:1 sub-sampling). The search area was
defined as the whole image. The half-pixel resolution algorithm does not
use subsampling.

The performance analysis was carried out with the Foreman sequence at
CIF resolution. The original table~\ref{tab:comparison_me_hw} was
extracted from Huang's work~\cite{bb17548}. Huang implemented all
referred architectures besides his own (GEA). The architectures signaled
with ``*'' where no fully implemented by Huang (they miss some
shift-registers), therefore the number of gates shown in the table for
those implementations must be arbitrarily extrapolated for comparison
purposes. The evaluation of the engine proposed here is appended to the
table.

\begin{table*}
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\textbf{Architecture} & \textbf{Description} & \textbf{Memory (bits)} & \textbf{Clock cycles} & \textbf{Min. freq (MHz)} & \textbf{Gate number (k)}  \\
\hline
Yang & 1-D semi-systolic & 24 & 8,192 & 97.32 & 44.7 \\
\hline
AB1 & 1-D systolic & 256 & 24,064 & 285.88 & 14.4 \\
\hline
AB2 & 2-D systolic & 128 & 1,504 & 17.87 & 98.2 \\
\hline
Hsieh* & 2-D systolic & 8 & 2,209 & 26.24 & 100.1 \\
\hline
Tree & Tree based & 4,096 & 1,024 & 12.17 & 58.7 \\
\hline
Yeo & 2-D semi-systolic & 24 & 256 & 3.04 & 436.6 \\
\hline
Lai & 1-D semi-systolic & 24 & 256 & 3.04 & 384.8 \\
\hline
SA* & 2-D systolic & 16 & 1,024 & 12.17 & 127.0 \\
\hline
SSA* & 2-D semi-systolic & 16 & 1,024 & 12.17 & 110.6 \\
\hline
GEA & GEA based & 256 & 1,635 & 19.42 & 23.1 \\
\hline
Proposed Architecture & 4:1 Subsampling and 2 LSB truncation & 64 & 130 & 1.55 & 71.8 \\
\hline
\end{tabular}
\caption{Comparison of different Motion Estimation optimized architectures}
\label{tab:comparison_me_hw}
\end{center}
\end{table*}

All Huang's implementations use Full-Search limited to a search window
with p = 16 or a total area of 32 pixels. This is too small for a CIF
image. Our implementation uses PMVFast without a search window, that is
searching the whole image (352 x 288 pixels for the used CIF
sequence). So, the proposed approach uses a faster navigation algorithm
and a bigger search window.

In the following subsections we compare our approach with the others
presented in table~\ref{tab:comparison_me_hw}.

\subsubsection{Memory Utilization}

From all presented architectures, the one that demands more memory is
the Tree architecture, which needs 4096 bits. The one that demands less
memory needs only 8 bits. The implemented approach needs an intermediary
memory of 64 bits. The GEA architecture demands 256 bits, which is 4
times more than our proposal.
    
\subsubsection {Clock Cycles}

Our approach demands 130 clock cycles for type B
(bidirectional) motion vectors, segmented according to the following
composition:

\begin{itemize}
\item 27 cycles to previous reference image analysis;

\item 27 cycles to subsequent reference image analysis

\item 76 cycles to half-pixel motion estimation calculation.

\end{itemize}

From all the architectures in table~\ref{tab:comparison_me_hw}, the
fastest is the one proposed here. This is particularly important for the
project in which the Motion Estimation engine will be used, since it
targets real-time, 30 fps, HD encoding.

\subsubsection{Minimum Operating Frequency}

As a direct consequence of the small number of clock cycles demanded,
the proposed implementation has the smallest operation frequency of all
algorithms. With a clock of only 1.55 MHz, it is possible to encode, in
real time (30 fps), all motion estimation of the Foreman video sequence
in CIF resolution. This puts our algorithm as the best one in this
metric.

Considering the results (1.55 MHz to CIF resolution), it is possible to
estimate the engine's performance for high definition images. Taking a
1920 x 1080 pixel image, we can conclude that it would only be necessary
to have a typical board frequency of 30 MHz to accomplish a 30 frame per
second motion estimation.

\subsubsection{Chip Area}

Concerning the chip area in hardware, normally there is a direct
relation between performance increase and the involved parallelism,
i.e., increasing the parallelism and performance increases the chip
area. This can also be seen in table~\ref{tab:comparison_me_hw}, where
the proposed implementation shows the best performance, and also a high
gate number (about 72,000) when implemented in a Xilinx Virtex II PRO
FPGA. A careful design, however, helped to limit the impact on area,
keeping our proposal in pair with others considered. Actually, only
Yang, AB1, Tree, and GEA implementations are smaller than our proposal,
at the onus of a worse performance.

\subsubsection{Reasoning about the Comparisons}

The processing capacity of 1-D based architectures is not big enough to
perform motion estimation in larger search areas, reaching unacceptable
frequency requirements. The processing capacity of 2-D based
architectures is higher than 1-D, however, the required gate number
increases, demanding larger memory use.

GEA architecture uses less gates, however it demands more memory, has a
higher minimal operating frequency, and needs more clock cycles per
operation.


%-----------------------------------------------------------------------------
\section{Conclusions}\label{sec:conclusion}

In this article, we introduce two complementary optimizations for
block-based Motion Estimation in multimedia encoding: truncation of the
two less significant bits per sample and 4:1 subsampling by
macroblock. The proposed optimizations were developed around the PMVFast
block-matching algorithm and the SAD matching criterion, yielding a
block-matching engine that was implemented in the JM Reference Encoder
and also on a FPGA.

The evaluation of the proposed optimizations in the JM H.264 Reference
Encoder showed a quality loss of less than 0.5dB for all considered
sequences, and an average speedup of 2.64. The main purpose of this
evaluation was to assess ME quality, since good performance gains were
foreseeable for the parallel hardware implementation of the proposed
optimizations. The positive results encouraged us to advanced with the
hardware implementation.

The hardware implementation was compared to different strategies and it
was clear by the presented results that the proposed method is the most
efficient one, requiring only 130 clock cycles at 1.55 MHz, and
requiring only 64 bits of memory. The chip area, with about 72,000
gates, is also very competitive.

Therefore, we conclude that there is a real advantage in using the
approach proposed in this paper, mainly for encoders targeted at
high-resolution, real-time streaming.


\bibliographystyle{IEEEtran}
\nocite{*}
\bibliography{h264}

\end{document}


