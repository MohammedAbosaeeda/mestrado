/* THIS FILE IS AUTO GENERATED BY KESO! DON'T EDIT */

#include <keso_support.h>
#include <keso_types.h>
#include "global.h"
#include "domains.h"


#include <utility/tracer_c.h>

#define KESO_IRR_NOGCRESOURCE ~0
#define KESO_IRR_MAXSLOTS 128
static unsigned int bitmap[(KESO_IRR_MAXSLOTS+7)/(sizeof(unsigned int)*8)];
#define KESO_IRRGCT_MODE_LAZY 1
#include "irr.h"

#define MANAGEDDOMAINS 1
#define DOMAINID() (domains[curdom])
#define GCRESOURCE() KESO_IRR_NOGCRESOURCE
static unsigned char domains[1] = { 0 };
#define KESO_EOFML ((irr_listel_t*) -1)
#define KESO_IRR_BYTES2SLOTS(__bytesize__, __slotsize__) ((__slotsize__-1+__bytesize__)/__slotsize__)

void keso_irr_listwalker(domain_t*, int (*) (irr_listel_t*, unsigned char, irr_listel_t volatile **, domain_t*, void*), void *);
int keso_irr_alloccbfkt(irr_listel_t *, unsigned char, irr_listel_t volatile **, domain_t*, void **);

#ifdef KESO_IRRGCT_MODE_LAZY
static unsigned char gctpaused=1;
#endif

/*
 * Allocator function of the IRR Heap.
 *
 * This will return a pointer to a free memory area on the
 * heap of the current domain with @size bytes.
 *
 * This will throw an error if no suitable memory block can
 * be found.
 */
object_t* TRICORE_PSPR_SECTION keso_irr_alloc(class_id_t class_id, obj_size_t size, obj_size_t roff) {
	/* Used to pass the required size to the allocator in the one direction,
	 * and to pass back the pointer to the allocated memory in the other
	 * direction. If no memory block is found, KESO_EOFML is passed back.
	 */
	void *retval;
	object_t* obj;
	domain_t *domain;

	domain = &DOMAINDESC(KESO_CURRENT_DOMAIN);
	size = KESO_IRR_BYTES2SLOTS(size,domain->heap.irr.slotSize);
	retval = (void *) (unsigned int) size; /* Pass size to call back function */

	keso_irr_listwalker(domain, (int (*) (irr_listel_t*, unsigned char, irr_listel_t volatile **, domain_t*, void*)) keso_irr_alloccbfkt, (void *) &retval);

	/* TODO maybe we should return NULL here (and in all other heap specific
	 * allocs, too) and throw the out of memory error in allocObject */
#ifndef NO_WRITE
    if (retval == KESO_EOFML) keso_throw_error("out_of_memory\n");
#else
    if (retval == KESO_EOFML) keso_throw_error((char *) 0);
#endif
    
	DisableAllInterrupts();
	domain->heap.irr.sasls += size;
	domain->heap.irr.freeslots -= size;
	EnableAllInterrupts();

	/* Clear allocated memory, except first word with colorbit! */
	size *= domain->heap.irr.slotSize/sizeof(int);
	while(--size>0) ((int*)retval)[size]=0;

#ifdef KESO_IRRGCT_MODE_LAZY
	if(0<gctpaused) {
		gctpaused=0;
		ActivateTask(keso_irr_gc);
	}
#endif
	{
		obj = (object_t*)retval;
		unsigned char colorbit=*(unsigned char*)obj;
		*(int*) obj = 0;
		obj = (object_t*) ((object_t**)obj+roff);
		obj->class_id=class_id;
		obj->gcinfo = colorbit;

	}

	return obj;
}

/*
 * Allocator callback function for the listwalker.
 * The parameter *retval initially contains the required size in slots
 * for the memory block that needs to be found.
 * When a block large enough for the request is found, it is either split
 * or removed from the list, and a pointer to the allocated area is returned
 * in *retval.
 */
int TRICORE_PSPR_SECTION keso_irr_alloccbfkt(irr_listel_t *element, unsigned char mode,
		irr_listel_t volatile **prevNextPointer, domain_t *domain, void **retval) {
	unsigned short size;

	if(element == KESO_EOFML) { *retval=KESO_EOFML; return 0; }

	size = (unsigned short) (unsigned int) *retval;

	/***************************************************************************
	SPLITTING AN ELEMENT
	If the element's size is larger than the requested size, we split it by
	simply reducing its size.
	This subtraction must, however, happen atomically. There are the following
	scenarios that might happen:
	 (1) The interrupt might happen before writing back the substracted value,
	  but after having read the original value of size. In this case, we would
	  overwrite the modified value and hand out the same piece of memory twice.
	 (2) The interrupt might happen after writing back the modified value. In
	  this case, we would notice that somebody else performed a split, too, and
	  would not have a problem.
	Atomic substraction  would be possible with an compare-and-swap instruction,
	but the tricore architecture does not offer any instruction like this. We
	therefore have to disable the interrupts for a short period here.
	***************************************************************************/
	if(element->size > size) {
		unsigned short localsize;

		DisableAllInterrupts();
		localsize = element->size;
		if(localsize>size) {       /* enough memory to satisfy the request */
			localsize -= size;
			element->size = localsize;
			EnableAllInterrupts();
			/* Allocated memory is directly at element+new size */
			*retval =  (char*)element + (localsize * domain->heap.irr.slotSize);
			/* Pass allocation color in first byte of allocated memory.
			 * Always set bit0 so the object header looks differnt than
			 * references
			 */
			*((unsigned char*) *retval) = element->alloccolor | 1;
			return 0;
		}
		/* interrupting alloc stole our memory */
		EnableAllInterrupts();
	}

	/***************************************************************************
	REMOVAL OF AN EXACTLY FITTING ELEMENT
	If we encounter an element that exactly fits, we have to remove it. In order
	to do this, we have to do the following:
	 (1) Check that the element had no locks at all set by an interrupted alloc
	 (2) Remove the element in a critical section.
	While setting the next pointer of the previous element to the next pointer
	of this element, the next element may be removed. The next pointer of the
	prevElement would point to an object not in the queue anymore, and yet another
	interrupting alloc could run over the inconsistent queue before we can fix that.
	Therefore we need to disable Interrupts here, again.
	***************************************************************************/
	if(element->size==size && mode==0) {
		DisableAllInterrupts();
		if(element->size==size) {   /* make sure the element wasn't split */
			*prevNextPointer = element->next;
			EnableAllInterrupts();
			*retval = element;
			*((unsigned char*) *retval) = element->alloccolor | 1;
			return 0;
		}
		EnableAllInterrupts();
	}
	return 1;
}

/* Interrupt-friendly walk over the memory list, applying
 * a callback function to each list-element.
 *
 * The callback function is called for each element. When reaching
 * the end of the queue, it is called with KESO_EOFML as element.
 * The listwalker will terminate after that.
 *
 * The callback function shall return 1 if it wishes further processing
 * of the queue, or 0 if it is done and the run should be aborted.
 * An optional pointer parameter is passed through to the callback function and
 * may be used for parameters to the callback or return values from the
 * callback. When aborting, the mode of the previous element will be restored,
 * and so will the mode of the current element unless the next pointer of the
 * previous Element was changed by the callback or curElement was KESO_EOFML.
 * If the callback function updates *prevNextPointer but curElement remains
 * within the queue, the callback function must restore the original mode,
 * passed to the callback function as the mode parameter. After restoring the
 * original mode, the callback function cannot assume that the element will not
 * be removed and therefore should not perform any actions on the element after
 * restoring the original mode.
 *
 * Interrupts are enabled when calling the callback, however locking bits will
 * prevent the previous and the current Element from being removed by interrupting
 * instances.
 *
 * The callback function may remove the current element if the old mode does not
 * forbid it, but must ensure that there are no problems by overlapping listwalkers,
 * i.e. the removal must happen atomically.
 * If the callback function removes an element and requests
 * further walking over the queue, it will be called with the same prevElement
 * and the new element it is pointing to as curElement.
 *
 * A callback function may insert new elements, but only between the prevElement
 * and the curElement. The next pointer must always point to an element at a
 * higher address to keep the list sorted. When the
 * callback function inserts one or more elements and requests further queue
 * walking, it will be called with the same prevElement and the new element that
 * it now points to the next time and from thereon proceed normally, i.e. all
 * inserted elements will be passed to the callback function unless removed by
 * interrupting allocators in the meantime.
 *
 * In any case, if the callback function changes the next pointer of the
 * previous element, it MUST RESTORE THE ORIGINAL MODE OF CURELEMENT IF
 * CURELEMENT IS STILL A MEMBER OF THE LIST, since this will not be done
 * by the listwalker in order to not modify the memory of removed elements.
 * I.e. when inserting a new element and having prevNextPointer pointing to
 * the new element, the mode of curElement must be restored in the callback
 * function or the element will stay locked forever.
 */
void TRICORE_PSPR_SECTION keso_irr_listwalker(domain_t *domain,
		int (*elementCallback) (irr_listel_t*, unsigned char, irr_listel_t volatile **, domain_t*, void*),
		void *cbparam) {
	irr_listel_t volatile **prevNextPointer;
	irr_listel_t *curElement, *prevElement;
	irr_listel_t scratch;
	unsigned char pmode=0, mode;
	int cbreturn=1;

	prevNextPointer = &domain->heap.irr.freemem;
	prevElement = &scratch;           /* hack to make mode restoration work on first element */

	while(cbreturn) {
		/*****************************************************************************
		ADVANCING TO THE NEXT ELEMENT
		At this point, we have prevNextPointer pointing to the memory slot with the pointer
		to the next element in the list, which may also be KESO_EOFML.
		The memory slot of prevNextElement itself is protected and cannot be removed.
		Initially, this is due to the fact that the head pointer of the list is in the
		domain_desc and cannot be removed. The task is now to advance to the next
		element, and aquire a simple lock on it.
		The locking mode is saved in the mode member of the listelement. The object
		could be removed while aquiring the lock and writing mode to the already
		handed out object could cause inconsistencies. To avoid this, we must disable
		Interrupts here for a short portion of code.
		*****************************************************************************/
		DisableAllInterrupts();
		curElement = (irr_listel_t *) *prevNextPointer;
		if(curElement == KESO_EOFML) {
			EnableAllInterrupts();
			elementCallback(curElement, 1, prevNextPointer, domain, cbparam);
			break;
		}
		mode = curElement->mode;        /* save mode for later restoration */
		curElement->mode = mode | 1;    /* aquire a lock */
		EnableAllInterrupts();

		/* curElement is now locked and can be processed */
		cbreturn = elementCallback(curElement, mode, prevNextPointer, domain, cbparam);

		/* If prevElement still points to curElement
		 *   move previousElement to the current element and unlock previousElement
		 * else
		 *   keep prevElement and recall callback with the new element it points to
		 *
		 * curElement and prevElement are protected from being removed, however, the
		 * callback function is allowed to remove curElement if the previous mode
		 * allows it. The callback function is also allowed to insert new elements
		 * between prevElement and curElement. Both cases change the content of
		 * prevElement->next, and we call the callback again with the same
		 * prevElement and the new element that it points to.
		 * We do not need to care about restoring the mode of curElement here, since
		 * this is the task of the callback function.
		 */
		if(*prevNextPointer==curElement) {
			prevElement->mode = pmode;      /* restore original mode of previous element */
			prevElement = curElement;       /* advance one element */
			prevNextPointer = &curElement->next;
			pmode = mode;                   /* backup mode of new previous element */
		}
		/* in case we keep prevElement we also need to keep it locked */
	}
	/* If processing is aborted without changed prevNextPointer, prevElement
	 * was updated with curElement and we restore its mode here. */
	prevElement->mode = pmode;
}
#include <keso_support.h> 
#ifdef KESO_OBJECTARRAYCLASS_AVAILABLE
#include "object_array.h"
#endif

#define REFONHEAP(__obj__) (__obj__>=heap_begin && __obj__<heap_end)
#define HEAPDESC DOMAINDESC(DOMAINID()).heap.irr
#define OBJCOLORED(__obj__) ((__obj__->gcinfo) == coloredobj)
#define COLOROBJ(__obj__) (__obj__->gcinfo = coloredobj)

#define IRRGC_DEBUG

#ifdef DEBUG
#define GC_LOCK_STACK() DisableAllInterrupts();GetResource(GCRESOURCE())
#define GC_UNLOCK_STACK() ReleaseResource(GCRESOURCE());EnableAllInterrupts()
#else
#define GC_LOCK_STACK() GetResource(GCRESOURCE())
#define GC_UNLOCK_STACK() ReleaseResource(GCRESOURCE())
#endif

/*
 * NOTES
 *
 * Calculating slot sizes is expensive (division). slotsizes that are power of 2
 * could be handled much more efficiently.
 *
 * Maybe we should automatically adjust the heapsize to have a slotcount that is
 * a multiple of 32, so we only have full ints in the bitmaps, none that are
 * only partially occupied
 * 
 * chooseDomain might loop until it finds a domain with a need instead of
 * collecting the domain with the lowest need value. This will reduce the
 * interrupt jitter and the time spans with enabled write barriers.
 * As an alternative, the garbage collector could wait for an event when it
 * finds there are no domains that need garbage collection. The Event could be
 * sent by the irr-alloc function. This would additionally decrease the energy
 * consumption on certain microcontrollers.
 */

/* Parameter structure for the sweep callback function */
struct sweepparam {
	irr_listel_t *lastel;     /* Pointer to the last processed element, NULL on first element */
	unsigned short bmpos;     /* Current bitposition in the bitmap, initially 0, later the first
	                             bit behind the last scanned element. */
};

/* Indicator on which domain irr-gc is currently
 * active, intended for use by barriers.
 * INVALID_DOMAIN means gc not active
 */
unsigned char irr_gc_active_domain=INVALID_DOMAIN;

/* Contains the full gcbyte for an already colored object.
 * This allows checking/setting an objects color without
 * masking any bits. The initial value does not matter because
 * the GC will correctly initialize it every time before enabling
 * writebarriers. Currently, this value be always either 1 or 3.
 */
static unsigned char coloredobj=3;

/* Index of the next free slot on the stack (one above
 * the top element of the stack) */
static short keso_irr_sp=0;

/* very simple estimation of the maximum number of objects that might be allocated */
static object_t *keso_irr_stack[KESO_IRR_MAXSLOTS];

/* internal variables */
static object_t *heap_begin, *heap_end;

/* index in domain[] to currently processed domainid,
 * this is NOT the domainid itself */
static unsigned char curdom = 0;

/* internal functions */
static void markObj(object_t*);
static void markSlots(unsigned short, unsigned short);
static unsigned short findFreeBlock(unsigned short *, unsigned short);
static unsigned short findBit(unsigned short, unsigned short, unsigned char);
static char searchInt(unsigned short, unsigned char, unsigned char, unsigned char);
static int sweep(irr_listel_t *, unsigned char, irr_listel_t volatile **, domain_t *, struct sweepparam *);
static int  fspmarker(irr_listel_t *, unsigned char, irr_listel_t volatile **, domain_t*, void *);
static __inline__ int refCnt(object_t *obj);
static void scanStack();
static void chooseDomain();
#ifdef KESO_NEED_FINALIZE
static void callFinalize(unsigned short bstart, unsigned short size);
#endif


#if KESO_WRBR_INLINE_LEVEL==0
/* Write barrier function for IRR Heaptype. This will
 * also assign the new value. addr is a pointer to the
 * reference field, and value is new value that will be
 * assigned to the field.
 */
void keso_irr_writebarrier(object_t** addr, object_t* value) {
	if (irr_gc_active_domain==KESO_CURRENT_DOMAIN)
		keso_irr_pushObject(*addr);
	*addr = value;
}
#endif

/* This is the main GC function of the IRR Heap.
 * The task is chaining itself and run periodically,
 * collection garbage on the heap of one domain at
 * each run.
 */
TASK(keso_irr_gc) {
    tracer_trace("\nGC TASK Running!\n");

	struct sweepparam sp;
	unsigned short i;

	while (1) {

		/* choose the domain that will be scanned */
		chooseDomain();

#ifdef KESO_NEED_FINALIZE
		/* Migrate Task to Target Domain */
		/* only needed if java method called ! */
		KESO_CURRENT_TASK->_e_domain_id = DOMAINID(); 
#endif

		/* heap begin and end are e.g. used to determine if an object reference is
		 * on the domain heap or not */
		heap_begin = (object_t*) HEAPDESC.heap_top;
		heap_end = (object_t*) ((char*) heap_begin + HEAPDESC.heap_size);

		/* ENABLE WRITEBARRIERS
		 * 
		 * Writebarriers are active on
		 *  - static fields (PutStatic)
		 *  - object array's fields
		 *  - reference fields in objects (PutField)
		 *
		 * We don't need writebarriers on
		 *  - immortal (constant) objects
		 */
		keso_irr_sp=0;
		coloredobj = HEAPDESC.colorbit | 1;
		irr_gc_active_domain = DOMAINID();

		/* Mark all slots of current freemem list */
		keso_irr_listwalker(&DOMAINDESC(DOMAINID()), fspmarker, NULL);

		/* record new allocations from this point */
		DOMAINDESC(DOMAINID()).heap.irr.sasls = 0;

		/* scan stacks and tasks */
		i=KESO_MAX_TASK-1;
		while(i-->0) { /* skips INVALID_TASK */
			KESO_TASKCLASSTYPE* task;
			keso_stack_t* stack;

			task = keso_task_index[i];

			if (task!=NULL && task->_domain_id==DOMAINID()) 
				keso_irr_pushObject((object_t*)task);

			if (GCRESOURCE() != KESO_IRR_NOGCRESOURCE) {
				stack = keso_stack_index[i];

				if(stack!=NULL) {
					GC_LOCK_STACK();
					for ( ; stack!=NULL; stack=stack->next) {
						if (stack->domain_id!=DOMAINID()) continue;
						KESO_STACK_WALK(stack, keso_irr_pushObject);
					}
					GC_UNLOCK_STACK();
				}
			}
		}

		/* if Alarm/Resource-objects contained references, we
		 * would need to scan them, too
		 */

		/* Scan statics */
#if KESO_NUM_STATIC_REFS>0
		for(i=0; i<KESO_NUM_STATIC_REFS; i++)
			keso_irr_pushObject(STATICREF(irr_gc_active_domain)[i]);
#endif


		/* The root set is on the working stack now, scan and mark
		 * until the stack is empty */
		scanStack();

		/* scanning complete, disable write barriers */
		irr_gc_active_domain = INVALID_DOMAIN;

		/* Recover unused memory */
		sp.lastel = NULL;
		sp.bmpos  = 0;
		keso_irr_listwalker(&DOMAINDESC(DOMAINID()), (int (*) (irr_listel_t *, unsigned char, irr_listel_t volatile **, domain_t*, void *))sweep, &sp);

		/* Clear the bitmap for next gc cycle */
		i = HEAPDESC.heap_size / HEAPDESC.slotSize;
		i = (i-1) / (sizeof(unsigned int)*8);
		do { bitmap[i] = 0; } while(i--> 0);

		HEAPDESC.colorbit ^= 2;

	}
}

/* Choose the domain that needs garbage collection
 * the most of all managed domains, determined by
 * a metric called the "need" here.
 *
 * The need of a domain for a GC is determined by
 * the slots allocated since the last garbage collection (sasls)
 * compared to the remaining space. The remaining
 * space should not be less than the sasls plus a grace
 * area of 25% of the heap size. Thus, the "need" is calculated
 * freeslots - sasls - (heapsize>>2).
 * The function goes RoundRobin through the domain array and chooses
 * the first found domain that violates the above criterium (need<0). By
 * doing this instead of simply choosing the domain with the most severe
 * need, we can assure that all domains that have a need actually receive
 * a garbage collection (fairness to some degree).
 * If every domain has need>=0, the domain with the lowest need is taken.
 */

static void chooseDomain() {
#ifdef KESO_IRRGCT_MODE_WORKAHOLIC
	signed int minneed=65535;
	unsigned char mindom=curdom;
#endif
	signed int need;
	domain_t *dom;
	unsigned char i=curdom;
	
	while(1) {
		if(++i >= MANAGEDDOMAINS) i=0;

		dom = &DOMAINDESC(domains[i]);
		need = dom->heap.irr.freeslots-
			dom->heap.irr.sasls-
			((dom->heap.irr.heap_size/dom->heap.irr.slotSize)>>2);

		/* if need<0, this domain receives a GC treatment */
		if(need<0) { curdom=i; return; }
#ifdef KESO_IRRGCT_MODE_WORKAHOLIC
		if(need<minneed) { mindom=i; minneed=need; }
#endif

		/* The need of every domain is >0, choose
		 * the one with the lowest need */
		if(i == curdom) {
#ifdef KESO_IRRGCT_MODE_LAZY
			gctpaused=1;
			TerminateTask();
#endif
#ifdef KESO_IRRGCT_MODE_WORKAHOLIC
			curdom=mindom;
			return;
#endif
		}
	}
}

/* This function processes the working stack, initially containing
 * the root object set when run. Elements are popped and marked when
 * completly processed, and may be pushed upon discovery of white
 * inner references or concurrently by write barriers.
 *
 * The same reference will never be pushed onto the stack twice, which
 * ensures that the stack is always large enough. This is accomplished
 * by marking already scanned elements by setting their colorbit before
 * pushing them to the stack. Already colored elements are not pushed
 * again.
 *
 * The bits of an object in the bitmap are not marked until popping the
 * element from the stack. This enables writebarriers to push objects
 * without the need to mark them on the stack (and do it in the low
 * priority gc task). On the other hand, it introduces the need for
 * the color bit in the objects header.
 */
static void scanStack() {
	object_t *curobj;
	signed int curref;

	while(keso_irr_sp>0) {
		DisableAllInterrupts();
		curobj = keso_irr_stack[--keso_irr_sp];
		EnableAllInterrupts();

		curref = refCnt(curobj);
		if(curref < 0) { /* regular object w/ reference fields */
			while(curref<0) keso_irr_pushObject( ((object_t**)curobj)[curref++] );
#ifdef KESO_OBJECTARRAYCLASS_AVAILABLE
		} else if(curref > 0) { /* object array */
			while(curref-->0) keso_irr_pushObject( ((object_array_t*)curobj)->data[curref] );
#endif
		}

		/* Finally, mark the objects bits in the bitmap */
		markObj(curobj);
	}
}

/* Marks the bits of the slots occupied by @obj.
 * If obj is an object outside the domain heap,
 * nothing is done (immortal objects).
 */
static void markObj(object_t *obj) {
	if(REFONHEAP(obj)) {
		unsigned short firstslot, size;

		/* calculate size in slots (i.e. bits used in the bitmap) */;
		size = keso_objSize(HEAPDESC.slotSize,obj);
		
		firstslot= (unsigned short)(((unsigned int) ((object_t**)obj-CLS_ROFF(obj->class_id))
			-(unsigned int)heap_begin) / HEAPDESC.slotSize);
		markSlots(firstslot, size);
	}
}

/* Return reference count of an object
 *
 *  x: obj is object array with length x
 *  0: obj has no internal references
 * -x: obj is regular object w/ internal references
 */
static __inline__ int refCnt(object_t *obj) {
#ifdef KESO_OBJECTARRAYCLASS_AVAILABLE
	if(keso_isObjectArrayClass(obj->class_id))
		return ((array_t*)obj)->size;
#endif
	return -(signed int) CLS_ROFF(obj->class_id);
}

/* Callback function to the listwalker, that recovers unused memory by
 * freeing unreachable objects. The memory slots that can be freed are
 * marked by having their corresponding bit in the bitmap still set to 0.
 *
 * If possible, the recovered memory is merged with preceding or succeeding
 * blocks to fight fragmentation.
 */
static int sweep(irr_listel_t *element, unsigned char mode,
		irr_listel_t volatile **prevNextPointer, domain_t *domain,
		struct sweepparam *sp) {
	/*
	 * Previous element: sp->lastel (NULL before first element)
	 * First bit for next processing: sp->bmpos (initially 0)
	 * Current element:  element    (KESO_EOFML behind last element)
	 */

	/* Pointer to the head of the partial list */
	irr_listel_t *start;

	/* Pointer to the next pointer linking to the tail
	 * of the partial list. Initially, this points to
	 * the start pointer, but after finding 2 or more
	 * elements it will point to the next pointer of
	 * the element before the tail element in the
	 * partial list */
	irr_listel_t **pnext;

	/* Return value to the listwalker */
	unsigned int retval;

	/* Maximum position in the bitmap to scan, i.e.
	 * this will contain the first slot of element */
	unsigned short maxbmpos;

	/* Size of the currently reclaimed block */
	unsigned short size;

	/* Record the number of reclaimed slot, and add them
	 * in a single critical section before leaving the
	 * sweep function */
	unsigned short addedslots;

	if(element == KESO_EOFML) {
		/* maxbmpos = last slot of bitmap */
		maxbmpos = domain->heap.irr.heap_size / domain->heap.irr.slotSize;
		/* done after this run */
		retval=0;
	} else {
		/* maxbmpos = first slot of element */
		maxbmpos=(unsigned short)(
				((unsigned int) element - (unsigned int) heap_begin) /
				(unsigned int)domain->heap.irr.slotSize);
		/* demand further invocations */
		retval=1;
	}

	if(sp->bmpos > maxbmpos) {
		/* recalled with inserted elements from prior calls */
		sp->lastel = element;
		return retval;
	}

#ifdef IRRGC_DEBUG
	if((sp->lastel!=NULL) && (prevNextPointer != &(sp->lastel->next))) {
		write(1, "SWEEP ERR pNP!=sp->lastel\n", 22);
	}
#endif

	start = NULL;
	pnext = &start;
	addedslots=0;

	/** (1) Build partial list not connected to the list (uncritical) **/
	size = findFreeBlock(&sp->bmpos, maxbmpos-sp->bmpos);
	if(size>0){
	while(1) {
		*pnext = (irr_listel_t*) ((char*)heap_begin + sp->bmpos*domain->heap.irr.slotSize);

		/* Call destructors of objects that will be freed */
#ifdef KESO_NEED_FINALIZE
		callFinalize(sp->bmpos, size);
#endif

		/* Init new element, except the next pointer */
		(*pnext)->size = size;
		(*pnext)->mode = 0;
		(*pnext)->alloccolor = domain->heap.irr.colorbit;

		/* Skip scanning of this block */
		sp->bmpos += size;
		addedslots+= size;

		size = findFreeBlock(&sp->bmpos, maxbmpos-sp->bmpos);
		if(size>0) pnext = (irr_listel_t **) &((*pnext)->next);
		else break;
	}}

	/* Next Pointer of the tail element of the partial list is not initialized */

#ifdef IRRGC_DEBUG
	if(sp->bmpos != maxbmpos)
		write(1, "SWEEP ERR BP!=MBP\n", 15);
#endif

	if(element != KESO_EOFML)   /* Skip element on next search */
		sp->bmpos += element->size;

	if(0==addedslots) { /* No free space found */
		sp->lastel = element;
		return retval;
	}

	/* (2) Insert the partial list into the free memory list
	 *
	 * *pnext: Next Pointer of last element of partial list, possibly start->next
	 * start : first element of partial list, not NULL
	 * element: element following the scanned heap area, possibly KESO_EOFML
	 * *prevNextPointer: next pointer of the previous element
	 * sp->lastel: previous element
	 */
	if((char*)element == ((char*)*pnext + (*pnext)->size*domain->heap.irr.slotSize)) {
		/* Tail mergable (this can't be changed by interruptions) */
		DisableAllInterrupts();
		(*pnext)->next  = element->next;
		(*pnext)->size += element->size;
		if(sp->lastel!=NULL &&
				(char*)start == ((char*)sp->lastel + sp->lastel->size*domain->heap.irr.slotSize)) {
			/* Head and Tail mergable */
			sp->lastel->size += start->size;
			sp->lastel->next  = start->next;
			EnableAllInterrupts();
		} else {
			/* head not mergeable */
			*prevNextPointer = start;
			EnableAllInterrupts();
		}
		/* No need to unlock element on this path */
	} else {
		/* Tail not mergable */

		/* Set Next Pointer of last element */
		(*pnext)->next = element;

		if(sp->lastel!=NULL &&
				(char*)start == ((char*)sp->lastel + sp->lastel->size*domain->heap.irr.slotSize)) {
			/* Local Copy of the next pointer enables us to put
			 * the lastel->next = start->next outside the critical section */
			register irr_listel_t *headnext = (irr_listel_t *) start->next;

			DisableAllInterrupts();
			if((char*)start == ((char*)sp->lastel + sp->lastel->size*domain->heap.irr.slotSize)) {
				sp->lastel->size += start->size;
				EnableAllInterrupts();
				sp->lastel->next = headnext;

				if((sp->lastel->next != element) && (element != KESO_EOFML))
					element->mode = 0;
				else sp->lastel = element;
			} else {
				EnableAllInterrupts();
				*prevNextPointer = start;
				if(element != KESO_EOFML) element->mode = 0;
			}
		} else {
			/* Neither head nor tail mergable (atomic insertion possible) */
			*prevNextPointer = start;
			/* unlock element */
			if(element != KESO_EOFML) element->mode = 0;
			/* sp->lastel will be recovered in subsequent listwalker invocations */
		}
	}

	/* Update freeslots in the domain descriptor */
	DisableAllInterrupts();
	domain->heap.irr.freeslots += addedslots;
	EnableAllInterrupts();

	return retval;
}

/* Scan a block that is about to be freed for objects and
 * call the finalize() method of the found objects
 */
#ifdef KESO_NEED_FINALIZE
static void callFinalize(unsigned short bstart, unsigned short size) {
	object_t **blockbegin = (object_t**) ((char*)heap_begin + 
			( (unsigned int) bstart * (unsigned int) HEAPDESC.slotSize));
	unsigned short refsize = (unsigned short)
		(((unsigned int) size * (unsigned int) HEAPDESC.slotSize)/(unsigned int) sizeof(object_t*));
	unsigned short index = 0;
	unsigned short objbegin=index;
	object_t *obj;

	while(index < refsize) {
		if( (((unsigned int) blockbegin[index])&1) == 0 ) { index++; continue; }
	
		obj = (object_t*)(blockbegin+index);

#ifdef IRRGC_DEBUG
		if( CLS_ROFF(obj->class_id) != (index-objbegin) ) {
			write(1, "ERROR callFinalize (1)\n", 23);
			return;
		}
#endif

		/* call finalizer
		 *
		 * ISSUES
		 *  - getTaskID will return the task object of the irr-gc Task. This object
		 *    does not contain any fields modifyable from the Java application.
		 *  - WaitEvent should not be allowed in a finalize() method. If it is used,
		 *    it will fail anyway.
		 */
		KESO_INVOKE_FINALIZE(obj)(obj);

		/* position behind found object */
		objbegin += keso_objSize(HEAPDESC.slotSize,obj) * (HEAPDESC.slotSize/sizeof(object_t*));
		index = objbegin;
	}

#ifdef IRRGC_DEBUG
	if(index != refsize || index != objbegin) {
		write(1, "ERROR callFinalize (3)\n", 23);
	}
#endif
}
#endif

/* Find a block of consecutive unset bits in the bitmap,
 * beginning at position *bmbegin. From bmbegin a maximum
 * of maxsize slots are searched.
 * The function returns the size of the found free block,
 * that may be 0. If there are multiple free blocks in the
 * specified are, the size of the first block is returned.
 * *bmbegin is adjusted to contain the first slot of the
 * free block whose size is returned. If no free block is
 * found, 0 will be returned and *bmbegin will contains its
 * original value + maxsize.
 */
static unsigned short findFreeBlock(unsigned short *bmbegin, unsigned short maxsize) {
	unsigned short blockstart, blockend;
	if(maxsize==0) return 0;
	blockstart = findBit(*bmbegin, maxsize, 0); /* Find a free slot */
	maxsize -= (blockstart-*bmbegin);
	*bmbegin = blockstart;
	if(maxsize==0) return 0;                    /* No free block in the specified range */
	blockend = findBit(blockstart, maxsize, 1); /* Find end of free block */
	return blockend-blockstart;                 /* return size of free block */
}

/* Find the first bit that is set to value in the bitmap range starting at
 * bmbegin (counting from 0) with a length of maxsize.
 * The function returns the bitposition, or bmbegin+maxsize if
 * an occurance could not be found in the specified range.
 */
static unsigned short findBit(unsigned short bmbegin, unsigned short maxsize, unsigned char value) {
	unsigned int negvalue = ((value==0)?0xffffffff:0);
	unsigned short bmbit, bmint;
	char pos;

	bmint = bmbegin / (unsigned short) (sizeof(unsigned int)*8);
	bmbit = bmbegin % (unsigned short) (sizeof(unsigned int)*8);

	/* the begin is not on an integer boundary */
	if(bmbit>0) {
		pos = searchInt(bmint, (unsigned char) bmbit, value, maxsize);
		if(pos>=0) return (bmbegin+(unsigned short) pos);
		pos = -pos;
		bmbegin += (unsigned short) pos;
		maxsize -= (unsigned short) pos;
		bmint++;
	}

	/* process full ints */
	while(maxsize >= (unsigned short) (sizeof(unsigned int)*8)) {
		if(bitmap[bmint]!=negvalue) break; /* This int contains the searched bit */
		bmint++;
		maxsize -= (unsigned short) (sizeof(unsigned int)*8);
		bmbegin += (unsigned short) (sizeof(unsigned int)*8);
	}

	if(maxsize > 0) {
		pos = searchInt(bmint, 0, value, maxsize);
		if(pos>=0) return (bmbegin+(unsigned short) pos);
		pos = -pos;
		bmbegin += (unsigned short) pos;
	}
	return bmbegin;
}

/* Search an int for the first bit with the specified value, starting with
 * bitposition bmbit. Maxsize may limit how much bits are searched, but it
 * is never searched more than to the end of the int.
 *
 * If a bit is found, its offset from bmbit is returned. If not, the negative value
 * of the number of bits searched is returned.
 */
static char searchInt(unsigned short bmint, unsigned char bmbit, unsigned char value, unsigned char maxsize) {
	unsigned char i;
	char bitssearched=0;

	/* bmbit used as shiftindex,
	 * i contains number of bits to process */
	bmbit = i = (sizeof(unsigned int)*8)-bmbit;
	if(i>maxsize) i=maxsize;

	while(i-->0) {
		if( ((bitmap[bmint]>>--bmbit)&1) == value) return bitssearched;
		else bitssearched++;
	}
	return -bitssearched; /* Nothing found */
}

/* Callback function that marks the slots of provided free memory blocks.
 * For each element it is called with, it marks the corresponding bits in
 * the bitmap. This makes objects allocated during the gccycle survive the
 * this cycle.
 *
 * It furthermore changes the allocation color of the element, so that all
 * objects allocated beyond that are allocated colored and will never be pushed
 * on the stack during this GC cycle.
 */
static int fspmarker(irr_listel_t *element, unsigned char mode, irr_listel_t volatile **prevNextPointer, domain_t *domain, void *param) {
	unsigned short firstslot;
	unsigned short size;
	if(element == KESO_EOFML) return 0;

	firstslot= (unsigned short) ((unsigned int)element - (unsigned int)heap_begin) / (unsigned short) domain->heap.irr.slotSize;

	/* Atomically saving current size of the element and changing
	 * its allocation color to black assures that all elements are
	 * allocated black from the area marked for survival.
	 * The critical section here would not be strictly necessary,
	 * but can save the work of marking the slots twice in case
	 * an object is allocated white before changing the allocation
	 * color but after reading the size of the element.
	 *
	 * ADDITIONAL NOTE
	 * In order to reduce the expected jitter the interrupt blockade
	 * has been removed, at the expense of the (very unlikely) case that
	 * the slots of an object will be marked twice.
	 */
	/*DisableAllInterrupts();*/
	size = element->size;
	element->alloccolor = domain->heap.irr.colorbit;
	/*EnableAllInterrupts();*/

	markSlots(firstslot, size);
	return 1;
}

/* Mark a sequence of slots beginning at <firstslot> (start counting with 0)
 * with a length of <slotcount> slots in the bitmap.
 */
static void markSlots(unsigned short firstslot, unsigned short slotcount) {
	unsigned short bmint=firstslot / (sizeof(unsigned int)*8);
	unsigned char bmbit =firstslot % (sizeof(unsigned int)*8);
	unsigned char i;

	/* until next full int */
	if(bmbit>0) {
		i = bmbit = sizeof(unsigned int)*8-bmbit;
		if(i>slotcount) i=slotcount;
		slotcount-=i;

		while(i-->0) {
			unsigned int mask = 1<<--bmbit;
			bitmap[bmint] |= mask;
		}
		bmint++;
	}

	/* full ints */
	while(slotcount >= sizeof(unsigned int)*8) {
		slotcount-=sizeof(unsigned int)*8;
		bitmap[bmint++] = ~0;
	}

	/* beginning of last partially filled int */
	for(i=sizeof(unsigned int)*8; slotcount>0; slotcount--) {
		bitmap[bmint] |= 1<<--i;
	}
}

/* Push a white object on the stack.
 * If the object is not white, this function
 * will do nothing.
 *
 * THIS IS THE ONLY PLACE WHERE OBJECTS ARE PUSHED
 * ON THE STACK.
 */
void TRICORE_PSPR_SECTION keso_irr_pushObject(object_t *obj) {
	register short sp;
	if(obj==NULL) return;

	DisableAllInterrupts();
	if(!OBJCOLORED(obj)) {
		COLOROBJ(obj);
		/* We could enable and immediately disable interrupts
		 * here if this is desirable, because this is actually
		 * two consecutive critical sections.
		 */
		sp = keso_irr_sp;
		keso_irr_sp = sp+1;
		EnableAllInterrupts();
		/* Uncritical, because a "popObject" will never overlap
		 * a pushObject (only the other way round)
		 */
		keso_irr_stack[sp] = obj;
		return;
	}
	EnableAllInterrupts();
}



