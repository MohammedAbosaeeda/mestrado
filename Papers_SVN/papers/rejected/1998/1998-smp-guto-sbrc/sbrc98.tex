\documentstyle[epsf,11pt]{article}

\newcommand{\tild}{\raisebox{-.75ex}{\~{}}}
\newcommand{\rb}[1]{\raisebox{1.5ex}[-1.5ex]{#1}}

\newcommand{\putfig}[5]{
	\begin{figure}[htb]
	\begin{center}
	\leavevmode
	\epsfxsize=#2
	\epsfysize=#3
	\epsfbox{#1}
	\end{center}
	\vskip -1.5em
	\caption{\label{#4}#5}
	\end{figure}
} 

% A4 size paper (210mm x 297mm);
% Left and right margins = 30 mm
% Top margin = 35mm bottom margin = 20mm
\oddsidemargin 4.6mm \evensidemargin 4.6mm
\topmargin -20.4mm \headheight 1cm \headsep 2cm \footskip 30pt 
\textheight = 242mm \textwidth 150mm

\setlength{\parskip}{1.5ex}
\setlength{\parindent}{3em}
\pagestyle{empty}

\sloppy

\begin{document}

\title{High Performance Computing on Clusters of SMPs}

\author
{
Ant\^onio Augusto Fr\"ohlich\, \, Wolfgang Schr\"oder-Preikschat\\*[3mm]
German National Research Center for Information Technology\\
Research Institute for Computer Architecture and Software Technology\\*[3mm]
{\normalsize {\tt Rodower Chaussee 5}}\\*[-1mm]	
{\normalsize {\tt 12489 - Berlin - Germany}}\\*[-1mm]	
{\normalsize {\tt Tel.:+49 30 6392-1837\, \, Fax:+49 30 6392-1805}}\\*[-1mm]
{\normalsize {\tt E-mail: \{guto|wosh\}@first.gmd.de}}\\*[-1mm]
}	

\date{}

\maketitle

\thispagestyle{empty}

\begin{abstract}

As commodity microprocessors and networks reach performance levels comparable to those used in massively parallel processors (MPPs), clusters of symmetric multiprocessors (SMPs) are starting to be called the new supercomputers. It is already a fact that they provide a better cost/performance ration if compared to traditional MPPs, and maybe in a short time both technologies will converge into a single one.

	This paper discusses several aspects regarding the adoption of clusters of SMPs to support high performance computing, including software and hardware restrains to deliver processing power to parallel application, as well as the most innovative alternatives to overcome these restrains. Besides discussing the current state of cluster computing, the authors try to identify which should be the next steps to enable these low-cost machines to replace expensive MPPs.

\end{abstract}


\section{Introduction}

	The current state of microprocessor and interconnection technologies is enabling the conception of low-cost high-performance parallel machines based on commodity components. As the microprocessors technology benefits from large scale production (and sales) to support technological improvement, the custom processors formerly used on supercomputers are losing their space. With high performance microprocessors available, most supercomputers are now being made of off-the-shelf microprocessors. In parallel, the microcomputer industry is using the same technology to make available symmetric multiprocessors that shown many architectural features of yesterdays supercomputers, and thus represent a meeting point for both technologies.
	
	Together with powerful processors, powerful interconnection networks, sometimes very similar to those used in massively parallel processors, are commercially available. Some commodity networks deliver more bandwidth then many computer busses, frequently at very low latencies. Many network adapters also include enough circuitry to implement communication strategies without impacting main processor performance. This has encouraged several experiments on clustering SMPs to accomplish cost-effective supercomputers.
	
	The availability of powerful hardware, however, count for just a part of the accomplishment of high performance computing based on clusters of SMPs. Appropriate software must be provided to deliver the computational power to applications. Developing low overhead software to support parallel computing in cluster environments constitutes the big challenge, specially when traditional programming interfaces have to be kept.

	In this paper we identify and discuss key points on this emerging technology that promises to be the supercomputers of tomorrow. We start by discussing the main characteristics of current clusters of SMPs, including processors and interconnection networks. Then we discuss the software commonly adopted on such architectures, analyzing processing and communication strategies. We finish this paper with a balance of what can already be taken for grant and what is still missing to bring cluster computing into reality.  
	

\section{\label{hard}Hardware Aspects}

	As the purpose of this survey is to analyze the possibilities of using clusters of SMPs to support high performance computing and not the clusters  them selves, we will concentrate on hardware aspects that can highly influence the software design, either by impacting performance or by imposing design restrictions. Even if these characteristics can not be modified by software, knowing them during the operating system design may help to envision alternatives to reduce their effects on applications.

	The next sections discuss the fundamental components of a cluster of SMPs: SMPs and interconnection networks. Often, high performance applications, besides processing and communication, will also demand high performance file systems. However, we will not approach the theme here. 
	

\subsection{\label{hard_proc}Symmetric Multiprocessors}

	A symmetric multiprocessor (SMP) is characterized by a small set of microprocessors sharing a single memory, as shown in figure \ref{smp}. The expression symmetric multiprocessor is derived from the fact that there are no specific purpose processors and that  all processors have the same privilege when accessing the memory. Actually, all processors are usually identical. This symmetry among the processors yields to very simple process management strategies, since any ready to run process (thread) can be dispatched to any processor. In this way, porting traditional single processor operating systems is also made easy. As a matter of fact, however, achieving good performance in  SMPs will imply in making the operating system itself a concurrent program, what is far more complex.

\putfig{fig/smp.eps}{70mm}{37mm}{smp}{The basic structure of a typical SMP.}
	

\subsubsection{Processors}

	Most traditional microprocessors on the market can be found in SMP configurations. Pentium, PowerPC, Sparc, Alpha, MIPS and PA-RISC are very often gathered in sets of 2 to 16 units sharing hundreds of Megabytes to a couple of Gigabytes of main memory through caches that vary in size from some hundreds of Kilobytes to some Megabytes. The table \ref{processors} presents clock frequency, primary and secondary cache sizes and SPEC 95 benchmark results for some top-line processors. These numbers must change very quickly and are presented just for illustration.
	
\begin{table*}[htb]
\footnotesize
\begin{center}
\begin{tabular}{|l|c|c|c|r|r|}
\hline
 & Clock & \multicolumn{2}{c|}{Cache} & \multicolumn{2}{c|}{SPEC 95}\\
\cline{3-4} \cline{5-6}
\multicolumn{1}{|c|}{\rb{Processor}} & (MHz) & Pri. (K) & Sec. & \multicolumn{1}{|c|}{int} & \multicolumn{1}{|c|}{fp}\\
\hline
PowerPC 604e	& 200 &  32 I + 32 D & 256 K &  9.3 &  8.9\\
Pentium II	& 300 &  16 I + 16 D & 512 K & 11.9 &  8.6\\
MIPS R10000	& 195 &	 32 I + 32 D &   4 M &  9.5 & 19.0\\
UltraSparc II	& 300 &  16 I + 16 D &   2 M & 12.1 & 15.5\\
PA-RISC 8200	& 236 &	 2M I + 2M D &     - & 17.3 & 25.4\\
Alpha 21164	& 600 &   8 I +  8 D &   8 M & 18.8 & 29.2\\
\hline
\end{tabular}
\caption{Performance characteristics of some top-line processors.}
\label{processors}
\end{center}
\end{table*}
 

\subsubsection{\label{hard_proc_mem}Memory}

	If compared to other parallel machines, SMPs present very high memory latencies, mainly due to its simple organization. However, this weakness is frequently compensate with the adopting large private caches for each processor. To analyze the memory contention problem on SMPs, let us divide them in two groups: those made of RISC processors and those made of CISC processors. The memory bottleneck appears in quite a different fashion on these two groups. For the RISC  microprocessors, that usually include large register banks, small instruction opcodes and simple memory operations, the adoption of large caches have been showing to be effective to reduce memory contention. Nevertheless, the same can not be said about the CISC microprocessors, basically represented in the SMP scenery by the Intel P6 family.

	The P6 family of microprocessors, that currently includes Pentium Pro and Pentium II, is widely used on high-end microcomputers, but reports on SMPs  have been quite disappointing. The P6 family presents a highly optimized internal architecture that benefits from a complex pipeline with multiple parallel functional units and is able of out-of-order, speculative execution. Despite of this, the Intel macro-architecture, i.e., ix86, seems to suffocate the microprocessor, that is forced to deliver its integer and floating-point high performance units through an interface with only 8 general use registers.

	Many groups are building or buying clusters of Pentium Pro and reporting poor results, even when large second level caches are used. The figure \ref{pentium} is based on the measurements of a Pentium Pro SMP running Solaris presented in \cite{Tanaka:98}. It shows that the memory copy bandwidth available to each processor in the SMP decreases drastically with the number of processors, moreover, the same paper reports that, for some data intensive applications running in a four processors configuration, two processors spend most of the time waiting for the memory subsystem. The reason for this poor results is not only the processor macro-architecture, but specially the obsolete PC boards design that restrain the memory copy bandwidth to about 74 MB/s.

\putfig{gnu/pentium.eps}{82mm}{60.4mm}{pentium}{Per processor memory copy bandwidth in an ordinary Pentium Pro SMP.}
	

\subsubsection{\label{hard_proc_io}I/O}

	It may look like strange to discuss I/O when the focus is high performance computing in clusters of SMPs, but none of the general purpose microprocessors usually configured as SMPs directly supports communication. Thus, in order to cluster SMPs, one need to plug some sort of network interface card (NIC) into the SMP I/O bus. Independently of the selected bus, remote communication will always go through it and, some times, will be limited by it. Usually, the operating system designers can count on a DMA controller or on some sort of burst transfer in the I/O bus, nevertheless, these mechanisms are often insufficient to exploit the available network bandwidth.
	
	A typical case where the bus restrains the network is Myrinet on a PC PCI bus. Myrinet will be described in more details latter, for while it is enough to know its raw bandwidth: 1.28 Gbits/s. There are two basic ways to operate a Myrinet NIC: programmed I/O or DMA. The decision of which technic to use should consider that, even if DMA is usually quite faster than programmed I/O, it also requires the physical address where the data is to be transfered to/from to be know in advance. Besides that, DMA requires the data to be contiguously stored. Thus, using DMA implies in moving the data to a well-known area or re-programming the DMA controller with buffer physical addresses translated by hand. Both alternatives can put DMA performance back to programmed I/O levels. In a standard PC PCI bus at 33 MHz, the peak throughput for programmed I/O is 355 Mbits/s and the DMA peak throughput, disregarding copies, re-programming and address translations, is 1066 Mbits/s. Even the theoretical DMA bandwidth is not enough to support Myrinet and other high speed networks bandwidth requirements. The scenery for SBus is even worst, restricting the NIC to a bandwidth around 432 Mbits/s \cite{Pakin:95}.


\subsection{\label{hard_comm}Communication}

	The scenery of interconnection hardware to cluster SMPs is quite a large one, including, among others, Fast and Giga Ethernet, FDDI, ATM, Fiber-channel, Myrinet and SCI. In order to discuss it, we selected two very distinct representatives: Myrinet and SCI. Both are high speed networks well suited and often used to cluster SMPs. The first aims to support message passing, while the second is designed to support a distributed shared memory scheme in hardware.
	

\subsubsection{\label{hard_comm_myrinet}Myrinet}

	Myrinet \cite{Felderman:94, Boden:95} is a high speed network commercially produced by Myricon Inc. It has its roots on the Caltech Mosaic massive multiprocessor project \cite{Seitz:92}, from which it inherited, among other characteristics, the worm-hole routing strategy. Myrinet's hardware and software interfaces, as well as protocols, are published and open, what encouraged several research projects to adopt it.
		
	Myrinet NICs are currently based on the LANai 4.0 chip, which includes a 37.5 MHz embedded RISC processor, two DMA controllers for pushing and pulling bit streams into/from the network and one DMA controller to transfer messages from/to the host memory.  Besides the LANai, a Myrinet NIC also includes a full duplex link interface, a host bus interface (currently SBus and PCI) and from 256 Kbytes to 1 Mbytes of static RAM that can be used to store the program for the LANai processor and also to store  message buffers. These NICs are able of transferring arbitrary length messages, to which them automatically generate/verify a checksum, at a raw bandwidth of 1.28 Gbits/s. The layout of a Myrinet NIC is presented in figure \ref{myrinet}.

\putfig{fig/myrinet.eps}{74mm}{60mm}{myrinet}{The layout of a Myrinet NIC.}
 	
	A Myrinet switch is basically a 8x8 mesh, with a per-hop latency of around 0.5 $\mu s$, where worm-hole routing takes place. Switches can be arbitrarily connected to form virtually any topology, some of which are shown in figure \ref{myrinet_topology}. However, although arbitrary, the network topology must be realized in order to enable source routing. Other important characteristics of Myrinet switches are packet ordering preservation and flow control. Packets sent through the same path are delivered by the switch in the same order they were sent. Flow control is handle in hardware by blocking a packet when a selected output port is busy. Packets can be blocked for up to 50 $ms$, when they are dropped. This hardware flow control propagates until the source NIC stops pushing packets into the network.

\putfig{fig/myrinet_topology.eps}{140mm}{30mm}{myrinet_topology}{Two possible Myrinet topologies: a fat-tree (a) and a cube (b).}

	The Myrinet NIC organization is specially interesting for operating system designers, because communication protocols and strategies are implemented in software and can be easily modified. Therefore, several communication packages are available for Myrinet, including emulations for standard networks, like Ethernet and ATM, standard protocols like TCP/IP and many experimental protocols. This flexibility is probably the reason why Myrinet has been widely used to cluster SMPs. Research projects adopting Myrinet include, among others: NOW from the University of California at Berkeley \cite{Culler:97}; Fast Messages from the University of Illinois \cite{Pakin:97}; COMPaS from the Real World Computing Partnership \cite{Tanaka:98}; PARNASS from the University of Bonn \cite{Griebel:97}.


\subsubsection{\label{hard_comm_sci}Scalable Coherence Interface}

	The Scalable Coherence Interface (SCI), as the name indicates, is not a network, but an interface specified in the IEEE standard 1596 from 1992. The standard defines hardware and protocols to tightly connect up to 64 K nodes. The most remarkable difference from a traditional network to a SCI compliant one is that in SCI data is exchanged among nodes using implicit communication, via remote memory access, in stead of using explicit message passing.

	SCI defines a global 64 bits address space shared by all nodes in the cluster and accessible through bus-like transactions for reading, writing, moving and locking memory locations. These transactions are atomic, dead-lock free and preserve memory and cache coherence along the cluster. Up to 64 transactions may be outstanding on each node, what allows for better bandwidth utilization. In this context, nodes can export segments of its local physical address space by mapping them into the SCI global address space. In brief, SCI specifies a hardware based distributed shared memory.	
	
	The scalability question is addressed in the standard by only employing unidirectional, point-to-point links to connect nodes. By doing so, the number of nodes in a SCI cluster in not limited by the length or by the speed of the network, as it would be in a SMP bus. Nodes can be gathered in rings or through switches, although the ring is usually adopted. Larger clusters can be build of rings of rings or of rings interconnected by switches. Some examples of SCI topologies are shown in figure \ref{sci_topology}.

\putfig{fig/sci_topology.eps}{80mm}{30mm}{sci_topology}{Two possible SCI topologies: a ring of rings (a) and a tree of rings (b).}

	Data integrity is maintained in hardware by means of checksums that are generated and verified for each transfered packet. If a bad packet is received, it is simply discarded. The associate transaction will eventually time-out, causing the packet to be retransmitted. This combination of checksum and time-out ensures reliable transactions. Nevertheless, SCI protocols do not ensure in-order delivering, that, when requested, must be implemented in a software layer on top of SCI.
	
	Current implementations of SCI NICs deliver raw bandwidth of 1 Gbits/s and are available for SBus and PCI buses. Usually, only a subset of the SCI standard is implemented. Cache coherence implementation, for instance, is infeasible in NICs connected to the I/O bus and is only implemented in custom architectures. Lock transaction are often missing too.
	
	NICs connected to the I/O bus implement the hardware distributed shared memory scheme in two phases: at first a contiguous segment in the I/O range of the physical address space designated to the NIC is allocated and mapped into the SCI global address space in the area reserved for the respective node; then, the segment is mapped into the address space of some local processes. Every time the segment is referenced, the NIC generates the appropriate SCI transaction. This translation process is depicted in figure \ref{sci_dsm}.
	
\putfig{fig/sci_dsm.eps}{140mm}{70mm}{sci_dsm}{Address translation in SCI.}

	The big appeal of SCI is that most of the effort to support application communication is done in hardware. Moreover, passing the SCI distributed shared memory abstraction to applications is a tempting solution to support software developed for shared memory machines, although good performing message passing layers have been developed on top of SCI. Research projects adopting SCI include, among others: SMiLE from the Technical University of Munich \cite{Hellwagner:95, Eberl:97}; HPVM from the University of Illinois \cite{Chien:97}; Scintilla from the University of California at Santa Barbara \cite{Ibel:96, Ibel:97}.
	

\section{\label{soft}Software Aspects}

	Knowing the characteristics and limitations of the underlying hardware is fundamental for a good software design, specially because, as a matter of fact, the hardware in a computer system is far more evolved than the respective software, which is responsible for the major limitations imposed on applications. Therefore, software design aspects must be carefully studied when aiming to provide flexible, high performance solutions.

	In the next sections we discuss the different approaches to support processing and communication in clusters of SMPs.
	

\subsection{\label{soft_proc}Processes}

	The final goal of any computing system is to deliver computing power to applications. Whatever abstraction chosen to achieve this should aggregate a bare minimum overhead, specially if the system aims to support high performance computing. Besides, to bring a cluster of SMPs to the parallel machines universe, the process abstraction delivered to application programmers should preserve many centralized characteristics of shared memory machines, yet implemented in a distributed fashion. That is, the process abstraction should make several points in the distributed nature of a cluster transparent to applications. Reaching this transparency implies in the implementation of some sort of global scheduling, synchronization and communication mechanisms. What extent to implement each transparency mechanism is a compromise to the (high performance) application requirements. In which level to implement them, whether in kernel or user level, is a compromise between scalability and performance in one side and generality in the other.
	
	 However, independently of where to place the user/kernel barrier, if any, adopting multi-threading is a consensus in cluster computing. Besides the advantages of adopting it to hide latency in the client/server scenery, clusters of SMPs benefit from multi-threading as a programming paradigm that allows for implicit communication on each SMP. Parallel programs can thus be implemented as collections of multi-threaded processes where each process executes in a different SMP. Inter-process (inter-node) communication is then handled using message passing, while intra-process (intra-node, inter-processor, inter-thread) communication is handled via shared memory.
	 
	To discuss these questions, we divided the processes, according to the level where the basic elements are defined, in two groups: kernel and user level.
	
	
\subsubsection{\label{soft_proc_kern}Processes at Kernel Level}

	At first sight, it may seem strange to adopt Unix-like operating systems in clusters of SMPs, because they are highly centralized and do not scale to distributed architectures. However, many Unix implementations are available in multi-threaded SMP versions, and, more important, they fit perfectly in the idea of building up parallel machines out of commodity components, yet with a high overhead to applications.
	
	The overhead imposed by Unix does not reside in the process implementation it self, that has been highly optimized over the years, but in the side-effects of running Unix. For many applications, having an operating system able of managing several peripheral devices, virtual memory, file system, TPC/IP network, etc, is nothing but overhead. Besides that, the absence of adequate cluster-wide scheduling, synchronization and communication mechanisms, asks for additional layers of software, like MPI or PVM. Even so, Unix is the most frequently used operating system in the cluster computing universe. The second most frequent choice, Windows NT, shares the same deficiencies.
	
	Another, more sophisticate, approach to implement the process abstraction at kernel level are the micro-kernels. The most modern ones got rid of most Unix overhead by pushing device drivers, file systems, virtual memory, etc, to user level. Unquestionably, many micro-kernels are more adequate to equip clusters of SMPs than monolithic Unix-like operating systems, however, the absence of a traditional user interface, like Posix and MPI, seems to constitute an in-traversable obstacle. Therefore, several micro-kernels supply server or library based Posix interfaces. The fact that these interfaces often perform worst than Unix is the reason why micro-kernels are rarely saw in clusters of SMPs.


\subsubsection{\label{soft_proc_user}Processes at User Level}

	Implementing the process abstraction at user level is an approach motivated by the realization that different applications have distinct requirements regarding processes. When processes are implemented at user level, its basic properties, like scheduling, grouping and synchronization, may be customized according to the application requirements.
	
	The implementation of a user level operating system usually counts on function libraries with several distinct implementations for each component. Applications programmers can then select the best choice to satisfy their needs. Library operating systems, as this approach is known, are extremely time consuming to develop and face two big challenges: customizing an operating system is a task out of the scope of most application programmers, what demands for automatic tools still to be implemented; the same interface restrain faced by micro-kernels verifies here.
	
	An example of library operating system is MIT's Exo-Kernel \cite{Engler:95}. An exo-kernel based operating system is composed by an exo-kernel, that acts as a secure interface to the underlying hardware, and by a library that implements the operating system abstractions, including processes. Although very flexible, the adoption of a Posix library makes Exo-Kernel to perform not significantly better than Unix.
	
	A more effective approach is to use object orientation to specialize the operating system components. The implementation is also based in libraries, but, in the place of functions, one can find classes. Inheritance can significantly reduce the efforts to supply broader sets of options, and the applications them selves can still specialize most components. This strategy, adopted in the Peace project from GMD-FIRST is one of the most adequate to support high performance computing, because the applications get exactly the operating system they need. Supplying a traditional API, like MPI or PVM, is also made easier in this approach, since the operating system can be specialized only till the point where it satisfies the API requirements, avoiding implementing a complete Unix layer.


\subsection{\label{soft_comm}Communication}

	The distributed nature of a cluster claims for effective inter-process communication. Fortunately, network technologies have been evolving in such a fashion to, sometimes, even challenge processors. In this context, developing efficient software and protocols became the real challenge.
	
	Several studies based in the LogP\footnote{LogP is a theoretical model of a distributed memory multiprocessor that specifies the performance characteristics of the interconnection network through four parameters: "{\em L}" is the latency or delay incurred in sending a small message from its origin to its destination; "{\em o}" is the overhead or the time spent by the processor to transmit or receive a message; "{\em g}" is the gap or interval between consecutive message transmissions or receptions at a processor; "{\em P}" is the number of processors.} model \cite{Culler:96} have shown that network bandwidth and latency are no longer the bottleneck of inter-node communication, while the overhead to carry out a message exchange assumes the ungrateful role. Breakdowns like those presented in \cite{Martin:97, Lumetta:97, Tanaka:98} confirm that the impact of increasing overhead is more destructive to most applications than the one caused by proportionally reducing bandwidth or increasing latency. In order to compare the overhead present in distinct communication implementations, one should distinguish between the overhead incurred when preparing the message (marshaling, encapsulating, fragmentation, ordering, etc) and that incurred in the path between the application and the communication handler. The first is inherent to the chosen communication strategy, while the second is usually eliminated when the communication is implemented at user level.
	
	The demand for low overhead communications has led to several user level implementations. However, there are still cases in which the current user level implementations are not adequate. Following we discuss both possibilities.


\subsubsection{\label{soft_comm_kern}Communication at Kernel Level}

	Implementing communication at kernel level is the natural approach for traditional operating systems, like Unix and Windows NT, because they consider the network adapter as just another peripheral device. Interfacing the network to applications is normally done using the socket abstraction, an end-point for n-to-1 communication channels defined in the scope of a protocol, very likely TCP/IP. In this context, accessing the network implies in a system call that is dispatched by the operating system, trough the TCP/IP stack, to the proper device driver. Moreover, the socket interface, in most of the cases, is not adequate to express communication in parallel programs, what leads for additional layers of software, like PVM or MPI, on top of it. Unnecessary to say the overhead on this scheme is much to high. In fact, traditional Unix and Windows NT communication is seldom adopted when the purpose of the cluster is to support high performance computing.
	
	Micro-kernels, in the oder hand, export the network to applications through some particular view of links, ports, mailboxes or remote procedure calls. The basic abstraction is implemented inside the kernel, while the network specific code is sometimes implemented by an external (highly coupled) server, what counts for better software engendering. Besides, TCP/IP is usually replaced by a lighter and more adequate protocol. Although significantly more adequate to parallel environments than monolithic, implementing communication inside a micro-kernel, due to the simple fact of crossing a system call barrier, still incurs in considerable overhead. The path from the user process to the network driver/server demands for around 10 $\mu s$ in fast platforms like L4 \cite{Liedtke:95}. This is approximately the round-trip time for some networks. Together with the user interface problem discussed previously, the high overhead restrain the use of micro-kernel based communication in clusters of SMPs	
	

\subsubsection{\label{soft_comm_user}Communication at User Level}

	Defining process at user level means to forget traditional operating systems and therefore is not a frequent approach in clusters of SMPs. Defining communication at user level, however, is far less traumatic. It is normally accomplished by implementing a device driver that initializes and then exports the network to applications. The implementation of a communication abstraction, including protocols, is entirely up to applications, that can then define it according to their needs. This scheme can be adopted in traditional operating systems as well as in micro-kernels and library operating systems.
	
	The major restriction to implement communication at user level appears in environments where the cluster is shared by mutually untrusted applications, what demands for secure multiplexing of the network. In this cases, some sort of kernel intervention is usually required. An interesting exception is the previously described SCI, that benefits from the MMU's protection capabilities to ensure secure multiplexing of the network.

	Two frequently used strategies to support user level communication are:
	
\begin{itemize}

\item {\bf Active Messages} are a communication model widely used in the parallel programming community due to its simplicity and efficiency. Active messages belong to the so called one-way communication group, because only the send part of a message exchange is explicitly expressed. Each sent message, besides data, carries along a reference to a handler, which is invoked when the message reaches its destination. The sent data is then treated by the specified handler in the scope of the receiving process. Explicit receives are not possible.

	Interesting implementations of active messages came from the University of California at Berkeley and Santa Barbara. Berkeley implemented active messages on a cluster of Sun Enterprise 5000 servers interconnected by Myrinet, while Santa Barbara implemented active messages for a cluster of Sun UltraSPARC workstations interconnected by SCI . The table \ref{am} summarizes the results reported by these two implementations in \cite{Lumetta:97} and \cite{Ibel:96}, respectively. Since the hardware platforms are not the same, the results can not be used for comparisons, however they give a notion of the current status of active messages implementations.

\begin{table*}[htb]
\footnotesize
\begin{center}
\begin{tabular}{|r|c|c|}
\hline
 & \makebox[1.5cm]{Myrinet} & \makebox[1.5cm]{SCI} \\
\hline
Latency ($\mu s$) & 13.8 & 9.3 \\
Send overhead ($\mu s$) & 5.6 & 3.1 \\
Receive overhead ($\mu s$) & 8.1 & 3.1 \\
Gap ($\mu s$) & 17.6 & 13.5 \\
Bandwidth (Mbits/s) & 31.7 & 26 \\
\hline
\end{tabular}
\caption{LogP parameters, bandwidth and round-trip times for active messages implementation for Myrinet by the University of California at Berkeley and for SCI by the University of California at Santa Barbara.}
\label{am}
\end{center}
\end{table*}
 
\item {\bf Asynchronous Remote Copy} is also a one-way communication strategy, where asynchronous remote memory access is supported. The memory mapped in the address space of a process can be read or written by another, possibly remote, processes. Remote memory access can suppress buffer handling and unnecessary memory copies, nevertheless, a handshake to arrange for locations where to read data from or write data to in other processes' address spaces, as well as mechanisms to notify the receiver that a new message is available at the destination should be provided. 

	An example of asynchronous remote copy are the Fast Messages from the University of Illinois. The Fast Messages implementation on a cluster of Sun Sparc Station workstations interconnected by Myrinet achieved bandwidth around 20 Mbits/s and latency of around 10 $\mu s$ \cite{Pakin:97}. Another example is PM from the Real World Computing Partnership, which implementation for a cluster of Pentium Pro interconnected by Myrinet is reported to achieve 82.5 Mbits/s of bandwidth with on overhead of 16 $\mu s$ \cite{Tanaka:98}.
	
\end{itemize}


\section{\label{disc}Discussion}

Much has been done to enable clusters of SMPs to support high performance computing. But, in the current development stage, what is satisfactory and what is not? In this section we try to evaluate some aspects of this complex question, again, splitting the subject into hardware and software.


\subsection{\label{disc_hard}Hardware Aspects}

	Many currently available SMPs, for instance SGI Origin and Sun Enterprise, present price/performance ratios much better than those from traditional parallel machines, like Cray and NEC. The good performance of these  SMPs is mainly due to the large registers banks of its RISC processors, to its large caches and to its high speed memory buses. Quite far from this reality are the Pentium based SMPs, that usually adopt the old memory subsystem from ordinary PCs and thus impose very high penalty on memory access.
	
	These high performance SMPs can now be clustered by "intelligent" high-speed networks that are getting closer to the custom interconnection systems of MPPs. Actually, many networks provide bandwidths that can not be successfully used by current SMPs due to restrictions on the I/O buses to where the communication adapters are to be plugged. In most clusters of SMPs, these I/O buses appear as the bottleneck in the communication system. The reason why so sophisticate machines adopt slow I/O buses is the fact that most of them have been used as servers, where disk I/O used to be the operation that consume bandwidth the most and was promptly afforded by these buses. The arrival of new high speed devices, like network adapters, is leading to development of faster buses that shall relieve the path between CPU/memory and network. As an example, a 64 bits PCI at 66 MHz is now on the market.
	
	In general, we can say that a careful selection of components can conduct to very high performing machines at relatively low costs.
	

\subsection{Software Aspects}

	The big challenge to support high performance computing in clusters of SMPs reside in the software, since its improvement do not step along with the hardware's. The main reason for this is the poor software design that do not promote encapsulation and thus make system software updates a painful operation that usually propagates until the application software. Therefore, most cluster users prefer traditional system software, to the detriment of performance. The penalty of adopting a standard all-purpose operating system can be tolerated, in many cases, by distributed computing, but seldom by parallel computing.
	
	Several important advances on removing software overhead from the way from applications to hardware have been achieved, specially with user level communication. However, we believe that a generic operating system can only represent a compromise to make resources available. It will not be able to deliver appropriate performance to specific classes of applications. In stead of a generic operating system, we propose the adoption of object orientation to make available, through inheritance, a set of system software components that can be instantiated according to the requirement of applications.

	We also believe that any attempt to remove software overhead, either by "striping" conventional systems or by developing new alternatives, should be encapsulate into a standard user interface, allowing for system software revisions without impacting application software. A reasonable user interface could be composed by Posix file operations, standard mathematics functions and MPI. Behind such interface, only the minimal components required by the application. A software package designed under this philosophy could drastically increase performance.
	
		
\section{Conclusion}

	In this paper we analyzed several aspects regarding the adoption of currently available clusters of SMPs to support high performance computing. Many of the considered SMPs and interconnection systems seems to deliver satisfactory performance. However, most of the software packages analyzed proved to be inadequate, specially due to high overhead or to the lack of an standard user interface.
	
	The authors proposal of a scalable object oriented set of system software components encapsulated by a standard user interface seems to fill the gap between massively parallel processors and clusters of SMPs. Such a proposal is now under development at GMD-FIRST.

\bibliographystyle{plain}
\bibliography{cluster_computing,operating_systems,communication,processors,parallel_machines}

\end{document}


